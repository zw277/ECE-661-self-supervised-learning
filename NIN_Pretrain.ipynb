{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jSGmccarHkRC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader,ConcatDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#credit to 'https://github.com/gidariss/FeatureLearningRotNet/blob/master/architectures/NetworkInNetwork.py'\n",
        "import math\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.layers = nn.Sequential()\n",
        "        self.layers.add_module('Conv', nn.Conv2d(in_planes, out_planes,\n",
        "                                                 kernel_size=kernel_size, stride=1, padding=padding, bias=False))\n",
        "        self.layers.add_module('BatchNorm', nn.BatchNorm2d(out_planes))\n",
        "        self.layers.add_module('ReLU', nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class GlobalAveragePooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GlobalAveragePooling, self).__init__()\n",
        "\n",
        "    def forward(self, feat):\n",
        "        num_channels = feat.size(1)\n",
        "        return F.avg_pool2d(feat, (feat.size(2), feat.size(3))).view(-1, num_channels)\n",
        "\n",
        "class NetworkInNetwork(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(NetworkInNetwork, self).__init__()\n",
        "\n",
        "        num_classes = opt['num_classes']\n",
        "        num_inchannels = opt['num_inchannels'] if ('num_inchannels' in opt) else 3\n",
        "        num_stages = opt['num_stages'] if ('num_stages' in opt) else 3\n",
        "        use_avg_on_conv3 = opt['use_avg_on_conv3'] if ('use_avg_on_conv3' in opt) else True\n",
        "\n",
        "        assert(num_stages >= 3)\n",
        "        nChannels = 192\n",
        "        nChannels2 = 160\n",
        "        nChannels3 = 96\n",
        "\n",
        "        blocks = [nn.Sequential() for _ in range(num_stages)]\n",
        "        # 1st block\n",
        "        blocks[0].add_module('Block1_ConvB1', BasicBlock(num_inchannels, nChannels, 5))\n",
        "        blocks[0].add_module('Block1_ConvB2', BasicBlock(nChannels, nChannels2, 1))\n",
        "        blocks[0].add_module('Block1_ConvB3', BasicBlock(nChannels2, nChannels3, 1))\n",
        "        blocks[0].add_module('Block1_MaxPool', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "        # 2nd block\n",
        "        blocks[1].add_module('Block2_ConvB1', BasicBlock(nChannels3, nChannels, 5))\n",
        "        blocks[1].add_module('Block2_ConvB2', BasicBlock(nChannels, nChannels, 1))\n",
        "        blocks[1].add_module('Block2_ConvB3', BasicBlock(nChannels, nChannels, 1))\n",
        "        blocks[1].add_module('Block2_AvgPool', nn.AvgPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "        # 3rd block\n",
        "        blocks[2].add_module('Block3_ConvB1', BasicBlock(nChannels, nChannels, 3))\n",
        "        blocks[2].add_module('Block3_ConvB2', BasicBlock(nChannels, nChannels, 1))\n",
        "        blocks[2].add_module('Block3_ConvB3', BasicBlock(nChannels, nChannels, 1))\n",
        "\n",
        "        if num_stages > 3 and use_avg_on_conv3:\n",
        "            blocks[2].add_module('Block3_AvgPool', nn.AvgPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "        for s in range(3, num_stages):\n",
        "            blocks[s].add_module('Block'+str(s+1)+'_ConvB1', BasicBlock(nChannels, nChannels, 3))\n",
        "            blocks[s].add_module('Block'+str(s+1)+'_ConvB2', BasicBlock(nChannels, nChannels, 1))\n",
        "            blocks[s].add_module('Block'+str(s+1)+'_ConvB3', BasicBlock(nChannels, nChannels, 1))\n",
        "\n",
        "        # global average pooling and classifier\n",
        "        blocks.append(nn.Sequential())\n",
        "        blocks[-1].add_module('GlobalAveragePooling', GlobalAveragePooling())\n",
        "        blocks[-1].add_module('Classifier', nn.Linear(nChannels, num_classes))\n",
        "\n",
        "        self._feature_blocks = nn.ModuleList(blocks)\n",
        "        self.all_feat_names = ['conv'+str(s+1) for s in range(num_stages)] + ['classifier',]\n",
        "        assert(len(self.all_feat_names) == len(self._feature_blocks))\n",
        "\n",
        "    def _parse_out_keys_arg(self, out_feat_keys):\n",
        "        # By default return the features of the last layer / module.\n",
        "        out_feat_keys = [self.all_feat_names[-1],] if out_feat_keys is None else out_feat_keys\n",
        "\n",
        "        if len(out_feat_keys) == 0:\n",
        "            raise ValueError('Empty list of output feature keys.')\n",
        "\n",
        "        for f, key in enumerate(out_feat_keys):\n",
        "            if key not in self.all_feat_names:\n",
        "                raise ValueError('Feature with name {0} does not exist. Existing features: {1}.'.format(key, self.all_feat_names))\n",
        "            elif key in out_feat_keys[:f]:\n",
        "                raise ValueError('Duplicate output feature key: {0}.'.format(key))\n",
        "\n",
        "        # Find the highest output feature in `out_feat_keys\n",
        "        max_out_feat = max([self.all_feat_names.index(key) for key in out_feat_keys])\n",
        "\n",
        "        return out_feat_keys, max_out_feat\n",
        "\n",
        "    def forward(self, x, out_feat_keys=None):\n",
        "        \"\"\"Forward an image `x` through the network and return the asked output features.\n",
        "\n",
        "        Args:\n",
        "          x: input image.\n",
        "          out_feat_keys: a list/tuple with the feature names of the features\n",
        "                that the function should return. By default the last feature of\n",
        "                the network is returned.\n",
        "\n",
        "        Return:\n",
        "            out_feats: If multiple output features were asked then `out_feats`\n",
        "                is a list with the asked output features placed in the same\n",
        "                order as in `out_feat_keys`. If a single output feature was\n",
        "                asked then `out_feats` is that output feature (and not a list).\n",
        "        \"\"\"\n",
        "        out_feat_keys, max_out_feat = self._parse_out_keys_arg(out_feat_keys)\n",
        "        out_feats = [None] * len(out_feat_keys)\n",
        "\n",
        "        feat = x\n",
        "        for f in range(max_out_feat + 1):\n",
        "            feat = self._feature_blocks[f](feat)\n",
        "            key = self.all_feat_names[f]\n",
        "            if key in out_feat_keys:\n",
        "                out_feats[out_feat_keys.index(key)] = feat\n",
        "\n",
        "        out_feats = out_feats[0] if len(out_feats) == 1 else out_feats\n",
        "        return out_feats\n",
        "\n",
        "    def weight_initialization(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m.weight.requires_grad:\n",
        "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                if m.weight.requires_grad:\n",
        "                    m.weight.data.fill_(1)\n",
        "                if m.bias.requires_grad:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                if m.bias.requires_grad:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "def create_model(opt):\n",
        "    return NetworkInNetwork(opt)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    size = 32\n",
        "    opt = {'num_classes': 4, 'num_stages': 4}\n",
        "\n",
        "    net = create_model(opt)\n",
        "    x = torch.autograd.Variable(torch.FloatTensor(1, 3, size, size).uniform_(-1, 1))\n",
        "\n",
        "    out = net(x, out_feat_keys=net.all_feat_names)\n",
        "    for f in range(len(out)):\n",
        "        print('Output feature {0} - size {1}'.format(\n",
        "            net.all_feat_names[f], out[f].size()))\n",
        "\n",
        "    out = net(x)\n",
        "    print('Final output: {0}'.format(out.size()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khAhXwfeu_60",
        "outputId": "599efa0f-5002-42d6-a5ad-8ac7e11ed1d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output feature conv1 - size torch.Size([1, 96, 16, 16])\n",
            "Output feature conv2 - size torch.Size([1, 192, 8, 8])\n",
            "Output feature conv3 - size torch.Size([1, 192, 4, 4])\n",
            "Output feature conv4 - size torch.Size([1, 192, 4, 4])\n",
            "Output feature classifier - size torch.Size([1, 4])\n",
            "Final output: torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_img(img, rot):\n",
        "    if rot == 0: # 0 degrees rotation\n",
        "        return img\n",
        "    elif rot == 90: # 90 degrees rotation\n",
        "        return np.flipud(np.transpose(img, (1,0,2)))\n",
        "    elif rot == 180: # 90 degrees rotation\n",
        "        return np.fliplr(np.flipud(img))\n",
        "    elif rot == 270: # 270 degrees rotation / or -90\n",
        "        return np.transpose(np.flipud(img), (1,0,2))\n",
        "    else:\n",
        "        raise ValueError('rotation should be 0, 90, 180, or 270 degrees')"
      ],
      "metadata": {
        "id": "EqQ3r02HHtJB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10Rotate(CIFAR10):\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "        super(CIFAR10Rotate, self).__init__(root, train=train, transform=transform, target_transform=target_transform, download=download)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index // 4], self.targets[index // 4]\n",
        "\n",
        "        # Use PIL for rotation\n",
        "        pil_img = Image.fromarray(img)\n",
        "        rotated_img = pil_img.rotate(90 * (index % 4 ))\n",
        "\n",
        "        # Convert back to NumPy array\n",
        "        #img = np.array(rotated_img).permute(2, 0, 1)\n",
        "        img = torch.tensor(np.transpose(rotated_img, (2, 0, 1)))\n",
        "\n",
        "        label = index % 4\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of images in the new dataset\n",
        "        return 4 * len(self.data)  # Each image is rotated four times\n"
      ],
      "metadata": {
        "id": "D2j8FtKmH0NV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the loss function\n",
        "class RotationPredictionLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the rotation prediction loss as described in the paper.\n",
        "    It assumes that the model's output are logits for each of the K classes (geometric transformations).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(RotationPredictionLoss, self).__init__()\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        Calculate the loss given the logits and the target classes.\n",
        "\n",
        "        :param logits: Predicted logits from the model for each of the K classes. Shape (batch_size, K)\n",
        "        :param targets: Actual labels of the geometric transformations applied. Shape (batch_size,)\n",
        "        :return: The mean rotation prediction loss for the batch.\n",
        "        \"\"\"\n",
        "        # Calculate the log probabilities\n",
        "        log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "        # Gather the log probabilities by the target class labels\n",
        "        target_log_probs = log_probs.gather(dim=1, index = targets.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Calculate the negative mean of these log probabilities\n",
        "        #loss = -target_log_probs.mean()\n",
        "        loss = -target_log_probs.sum()/4/128\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Example usage:\n",
        "# Assuming `outputs` are the raw logits from the model's forward pass, and `labels` are the correct class labels\n",
        "# outputs = model(input_data)\n",
        "# labels = torch.tensor([correct_class_labels])  # This should be provided as per the dataset\n",
        "# loss_fn = RotationPredictionLoss()\n",
        "# loss = loss_fn(outputs, labels)\n",
        "# loss.backward()  # Backpropagate the loss\n",
        "\n",
        "# Note that in an actual implementation, you would also include optimizer steps and update the model parameters."
      ],
      "metadata": {
        "id": "yc9GuNR-H5n4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameter setting\n",
        "import torch.optim as optim\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "criterian = RotationPredictionLoss()\n",
        "opt = {'num_classes': 4, 'num_stages': 4,'use_avg_on_conv3': False}\n",
        "NIN_net_4block = create_model(opt)\n",
        "optimizer = optim.SGD(NIN_net_4block.parameters(), lr = INITIAL_LR,\n",
        "                      momentum = MOMENTUM,\n",
        "                      weight_decay=REG)"
      ],
      "metadata": {
        "id": "MQ8L8513NyUh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3x9FB92HawOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kbds2iYHOrOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_dataset_train = CIFAR10Rotate(root='./data', train=True, download=True)\n",
        "cifar_dataset_train_loader = DataLoader(cifar_dataset_train, batch_size=128*4, shuffle=False)\n",
        "cifar_dataset_val = CIFAR10Rotate(root='./data', train=False, download=True)\n",
        "cifar_dataset_val_loader = DataLoader(cifar_dataset_val, batch_size=128*4, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0eP5VlYOc5a",
        "outputId": "13a5cfcc-50e1-49b8-bde2-20cce8582e96"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:11<00:00, 15132348.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# some hyperparameters\n",
        "# total number of training epochs\n",
        "EPOCHS = 100\n",
        "\n",
        "# the folder where the trained model is saved\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "# start the training/validation process\n",
        "# the process should take about 5 minutes on a GTX 1070-Ti\n",
        "# if the code is written efficiently.\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY = 0.2\n",
        "valid_acc = []\n",
        "losslist = []\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    if i  == 35 or i == 70 or i == 85 or i==100 :\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    NIN_net_4block.train()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    # this help you compute the training accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0 # track training loss if you want\n",
        "\n",
        "    # Train the model for 1 epoch.\n",
        "    for batch_idx, (inputs, targets) in enumerate(cifar_dataset_train_loader):\n",
        "        ####################################\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        inputs = inputs.float().to(device)\n",
        "        targets = targets.to(device)\n",
        "        NIN_net_4block = NIN_net_4block.to(device)\n",
        "        # compute the output and loss\n",
        "        outputs = NIN_net_4block(inputs)\n",
        "        loss = RotationPredictionLoss().forward(outputs, targets)\n",
        "\n",
        "        train_loss += loss\n",
        "        # zero the gradient\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        # apply gradient and update the weights\n",
        "        optimizer.step()\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "        _, predicted = outputs.max(1)  #make prediction based on the highest value\n",
        "        total_examples += targets.size(0) # in this case,128 for each batch\n",
        "        correct_examples += predicted.eq(targets).sum().item()\n",
        "        ####################################\n",
        "\n",
        "    avg_loss = train_loss / len(cifar_dataset_train_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "\n",
        "    NIN_net_4block.eval()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    # this help you compute the validation accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 # again, track the validation loss if you want\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(cifar_dataset_val_loader):\n",
        "            ####################################\n",
        "            # your code here\n",
        "            # copy inputs to device\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            inputs = inputs.float().to(device)\n",
        "            targets = targets.to(device)\n",
        "            NIN_net_4block = NIN_net_4block.to(device)\n",
        "\n",
        "\n",
        "            # compute the output and loss\n",
        "\n",
        "            outputs = NIN_net_4block(inputs)\n",
        "            loss = RotationPredictionLoss().forward(outputs, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # count the number of correctly predicted samples in the current batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += predicted.eq(targets).sum().item()\n",
        "            ####################################\n",
        "\n",
        "    avg_loss = val_loss / len(cifar_dataset_val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    valid_acc.append(avg_acc)\n",
        "    losslist.append(avg_loss.item())\n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        #if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "        #    os.makedirs(CHECKPOINT_FOLDER)\n",
        "        #print(\"Saving ...\")\n",
        "\n",
        "        torch.save(NIN_net_4block.state_dict(), 'NIN_net_4block.pth')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tB7gu4wNw3Y",
        "outputId": "43d45fdd-c8fa-423a-88c8-df5bfcdfc491"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 0.8377, Training accuracy: 0.6543\n",
            "Validation loss: 0.7875, Validation accuracy: 0.6776\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 0.6197, Training accuracy: 0.7563\n",
            "Validation loss: 0.7003, Validation accuracy: 0.7172\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 0.5315, Training accuracy: 0.7938\n",
            "Validation loss: 0.6382, Validation accuracy: 0.7469\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 0.4747, Training accuracy: 0.8186\n",
            "Validation loss: 0.6825, Validation accuracy: 0.7288\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 0.4352, Training accuracy: 0.8340\n",
            "Validation loss: 0.6647, Validation accuracy: 0.7414\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 0.4061, Training accuracy: 0.8462\n",
            "Validation loss: 0.8968, Validation accuracy: 0.6772\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 0.3884, Training accuracy: 0.8534\n",
            "Validation loss: 0.7538, Validation accuracy: 0.7167\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 0.3721, Training accuracy: 0.8596\n",
            "Validation loss: 0.6770, Validation accuracy: 0.7506\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 0.3599, Training accuracy: 0.8646\n",
            "Validation loss: 0.6688, Validation accuracy: 0.7474\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 0.3497, Training accuracy: 0.8689\n",
            "Validation loss: 0.7270, Validation accuracy: 0.7348\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 0.3372, Training accuracy: 0.8735\n",
            "Validation loss: 0.6975, Validation accuracy: 0.7332\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 0.3282, Training accuracy: 0.8769\n",
            "Validation loss: 0.6064, Validation accuracy: 0.7744\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 0.3223, Training accuracy: 0.8797\n",
            "Validation loss: 0.6569, Validation accuracy: 0.7646\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 0.3181, Training accuracy: 0.8804\n",
            "Validation loss: 0.6177, Validation accuracy: 0.7663\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 0.3066, Training accuracy: 0.8851\n",
            "Validation loss: 1.0117, Validation accuracy: 0.6697\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 0.3030, Training accuracy: 0.8870\n",
            "Validation loss: 0.7944, Validation accuracy: 0.7130\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 0.2980, Training accuracy: 0.8887\n",
            "Validation loss: 0.5714, Validation accuracy: 0.7805\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 0.2933, Training accuracy: 0.8908\n",
            "Validation loss: 0.5964, Validation accuracy: 0.7783\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 0.2916, Training accuracy: 0.8913\n",
            "Validation loss: 0.7698, Validation accuracy: 0.7398\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 0.2867, Training accuracy: 0.8930\n",
            "Validation loss: 1.2251, Validation accuracy: 0.6328\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 0.2823, Training accuracy: 0.8946\n",
            "Validation loss: 0.6677, Validation accuracy: 0.7639\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 0.2813, Training accuracy: 0.8945\n",
            "Validation loss: 0.8311, Validation accuracy: 0.7215\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 0.2763, Training accuracy: 0.8970\n",
            "Validation loss: 0.6112, Validation accuracy: 0.7697\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 0.2747, Training accuracy: 0.8971\n",
            "Validation loss: 0.6640, Validation accuracy: 0.7573\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 0.2720, Training accuracy: 0.8991\n",
            "Validation loss: 1.0235, Validation accuracy: 0.6844\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 0.2679, Training accuracy: 0.8999\n",
            "Validation loss: 0.6927, Validation accuracy: 0.7552\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 0.2699, Training accuracy: 0.8996\n",
            "Validation loss: 0.6130, Validation accuracy: 0.7787\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 0.2633, Training accuracy: 0.9020\n",
            "Validation loss: 0.7212, Validation accuracy: 0.7487\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 0.2645, Training accuracy: 0.9016\n",
            "Validation loss: 0.6143, Validation accuracy: 0.7853\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 0.2625, Training accuracy: 0.9021\n",
            "Validation loss: 0.5481, Validation accuracy: 0.8031\n",
            "\n",
            "Epoch 30:\n",
            "Training loss: 0.2590, Training accuracy: 0.9038\n",
            "Validation loss: 0.6789, Validation accuracy: 0.7601\n",
            "\n",
            "Epoch 31:\n",
            "Training loss: 0.2597, Training accuracy: 0.9025\n",
            "Validation loss: 0.8311, Validation accuracy: 0.7199\n",
            "\n",
            "Epoch 32:\n",
            "Training loss: 0.2562, Training accuracy: 0.9049\n",
            "Validation loss: 0.5877, Validation accuracy: 0.7925\n",
            "\n",
            "Epoch 33:\n",
            "Training loss: 0.2560, Training accuracy: 0.9043\n",
            "Validation loss: 0.5990, Validation accuracy: 0.7853\n",
            "\n",
            "Epoch 34:\n",
            "Training loss: 0.2519, Training accuracy: 0.9060\n",
            "Validation loss: 0.7136, Validation accuracy: 0.7444\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 35:\n",
            "Training loss: 0.1393, Training accuracy: 0.9512\n",
            "Validation loss: 0.3251, Validation accuracy: 0.8881\n",
            "\n",
            "Epoch 36:\n",
            "Training loss: 0.0832, Training accuracy: 0.9734\n",
            "Validation loss: 0.3652, Validation accuracy: 0.8845\n",
            "\n",
            "Epoch 37:\n",
            "Training loss: 0.0513, Training accuracy: 0.9859\n",
            "Validation loss: 0.4544, Validation accuracy: 0.8693\n",
            "\n",
            "Epoch 38:\n",
            "Training loss: 0.0364, Training accuracy: 0.9907\n",
            "Validation loss: 0.4604, Validation accuracy: 0.8736\n",
            "\n",
            "Epoch 39:\n",
            "Training loss: 0.0356, Training accuracy: 0.9894\n",
            "Validation loss: 0.4693, Validation accuracy: 0.8770\n",
            "\n",
            "Epoch 40:\n",
            "Training loss: 0.0437, Training accuracy: 0.9853\n",
            "Validation loss: 0.4897, Validation accuracy: 0.8716\n",
            "\n",
            "Epoch 41:\n",
            "Training loss: 0.0526, Training accuracy: 0.9817\n",
            "Validation loss: 0.5022, Validation accuracy: 0.8674\n",
            "\n",
            "Epoch 42:\n",
            "Training loss: 0.0516, Training accuracy: 0.9820\n",
            "Validation loss: 0.4829, Validation accuracy: 0.8709\n",
            "\n",
            "Epoch 43:\n",
            "Training loss: 0.0494, Training accuracy: 0.9829\n",
            "Validation loss: 0.4986, Validation accuracy: 0.8673\n",
            "\n",
            "Epoch 44:\n",
            "Training loss: 0.0511, Training accuracy: 0.9822\n",
            "Validation loss: 0.5607, Validation accuracy: 0.8555\n",
            "\n",
            "Epoch 45:\n",
            "Training loss: 0.0525, Training accuracy: 0.9814\n",
            "Validation loss: 0.5109, Validation accuracy: 0.8639\n",
            "\n",
            "Epoch 46:\n",
            "Training loss: 0.0535, Training accuracy: 0.9814\n",
            "Validation loss: 0.4867, Validation accuracy: 0.8690\n",
            "\n",
            "Epoch 47:\n",
            "Training loss: 0.0561, Training accuracy: 0.9802\n",
            "Validation loss: 0.5749, Validation accuracy: 0.8511\n",
            "\n",
            "Epoch 48:\n",
            "Training loss: 0.0552, Training accuracy: 0.9809\n",
            "Validation loss: 0.5114, Validation accuracy: 0.8626\n",
            "\n",
            "Epoch 49:\n",
            "Training loss: 0.0567, Training accuracy: 0.9798\n",
            "Validation loss: 0.6152, Validation accuracy: 0.8360\n",
            "\n",
            "Epoch 50:\n",
            "Training loss: 0.0572, Training accuracy: 0.9796\n",
            "Validation loss: 0.5779, Validation accuracy: 0.8474\n",
            "\n",
            "Epoch 51:\n",
            "Training loss: 0.0564, Training accuracy: 0.9805\n",
            "Validation loss: 0.6332, Validation accuracy: 0.8372\n",
            "\n",
            "Epoch 52:\n",
            "Training loss: 0.0579, Training accuracy: 0.9795\n",
            "Validation loss: 0.4743, Validation accuracy: 0.8686\n",
            "\n",
            "Epoch 53:\n",
            "Training loss: 0.0578, Training accuracy: 0.9797\n",
            "Validation loss: 0.5645, Validation accuracy: 0.8474\n",
            "\n",
            "Epoch 54:\n",
            "Training loss: 0.0598, Training accuracy: 0.9793\n",
            "Validation loss: 0.5599, Validation accuracy: 0.8534\n",
            "\n",
            "Epoch 55:\n",
            "Training loss: 0.0564, Training accuracy: 0.9804\n",
            "Validation loss: 0.5004, Validation accuracy: 0.8628\n",
            "\n",
            "Epoch 56:\n",
            "Training loss: 0.0517, Training accuracy: 0.9820\n",
            "Validation loss: 0.5994, Validation accuracy: 0.8410\n",
            "\n",
            "Epoch 57:\n",
            "Training loss: 0.0595, Training accuracy: 0.9787\n",
            "Validation loss: 0.5557, Validation accuracy: 0.8494\n",
            "\n",
            "Epoch 58:\n",
            "Training loss: 0.0552, Training accuracy: 0.9809\n",
            "Validation loss: 0.5198, Validation accuracy: 0.8586\n",
            "\n",
            "Epoch 59:\n",
            "Training loss: 0.0566, Training accuracy: 0.9800\n",
            "Validation loss: 0.5457, Validation accuracy: 0.8515\n",
            "\n",
            "Epoch 60:\n",
            "Training loss: 0.0584, Training accuracy: 0.9797\n",
            "Validation loss: 0.5574, Validation accuracy: 0.8492\n",
            "\n",
            "Epoch 61:\n",
            "Training loss: 0.0545, Training accuracy: 0.9807\n",
            "Validation loss: 0.5478, Validation accuracy: 0.8548\n",
            "\n",
            "Epoch 62:\n",
            "Training loss: 0.0546, Training accuracy: 0.9810\n",
            "Validation loss: 0.5234, Validation accuracy: 0.8589\n",
            "\n",
            "Epoch 63:\n",
            "Training loss: 0.0538, Training accuracy: 0.9816\n",
            "Validation loss: 0.5860, Validation accuracy: 0.8445\n",
            "\n",
            "Epoch 64:\n",
            "Training loss: 0.0584, Training accuracy: 0.9798\n",
            "Validation loss: 0.6147, Validation accuracy: 0.8415\n",
            "\n",
            "Epoch 65:\n",
            "Training loss: 0.0558, Training accuracy: 0.9804\n",
            "Validation loss: 0.5545, Validation accuracy: 0.8478\n",
            "\n",
            "Epoch 66:\n",
            "Training loss: 0.0518, Training accuracy: 0.9817\n",
            "Validation loss: 0.7582, Validation accuracy: 0.8105\n",
            "\n",
            "Epoch 67:\n",
            "Training loss: 0.0544, Training accuracy: 0.9814\n",
            "Validation loss: 0.5182, Validation accuracy: 0.8619\n",
            "\n",
            "Epoch 68:\n",
            "Training loss: 0.0559, Training accuracy: 0.9804\n",
            "Validation loss: 0.5827, Validation accuracy: 0.8473\n",
            "\n",
            "Epoch 69:\n",
            "Training loss: 0.0539, Training accuracy: 0.9814\n",
            "Validation loss: 0.5879, Validation accuracy: 0.8400\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 70:\n",
            "Training loss: 0.0200, Training accuracy: 0.9944\n",
            "Validation loss: 0.3639, Validation accuracy: 0.9006\n",
            "\n",
            "Epoch 71:\n",
            "Training loss: 0.0052, Training accuracy: 0.9998\n",
            "Validation loss: 0.3632, Validation accuracy: 0.9027\n",
            "\n",
            "Epoch 72:\n",
            "Training loss: 0.0032, Training accuracy: 1.0000\n",
            "Validation loss: 0.3651, Validation accuracy: 0.9031\n",
            "\n",
            "Epoch 73:\n",
            "Training loss: 0.0026, Training accuracy: 1.0000\n",
            "Validation loss: 0.3664, Validation accuracy: 0.9028\n",
            "\n",
            "Epoch 74:\n",
            "Training loss: 0.0023, Training accuracy: 1.0000\n",
            "Validation loss: 0.3670, Validation accuracy: 0.9030\n",
            "\n",
            "Epoch 75:\n",
            "Training loss: 0.0021, Training accuracy: 1.0000\n",
            "Validation loss: 0.3673, Validation accuracy: 0.9031\n",
            "\n",
            "Epoch 76:\n",
            "Training loss: 0.0020, Training accuracy: 1.0000\n",
            "Validation loss: 0.3672, Validation accuracy: 0.9033\n",
            "\n",
            "Epoch 77:\n",
            "Training loss: 0.0019, Training accuracy: 1.0000\n",
            "Validation loss: 0.3669, Validation accuracy: 0.9035\n",
            "\n",
            "Epoch 78:\n",
            "Training loss: 0.0018, Training accuracy: 1.0000\n",
            "Validation loss: 0.3664, Validation accuracy: 0.9037\n",
            "\n",
            "Epoch 79:\n",
            "Training loss: 0.0017, Training accuracy: 1.0000\n",
            "Validation loss: 0.3658, Validation accuracy: 0.9037\n",
            "\n",
            "Epoch 80:\n",
            "Training loss: 0.0017, Training accuracy: 1.0000\n",
            "Validation loss: 0.3651, Validation accuracy: 0.9037\n",
            "\n",
            "Epoch 81:\n",
            "Training loss: 0.0016, Training accuracy: 1.0000\n",
            "Validation loss: 0.3644, Validation accuracy: 0.9036\n",
            "\n",
            "Epoch 82:\n",
            "Training loss: 0.0016, Training accuracy: 1.0000\n",
            "Validation loss: 0.3635, Validation accuracy: 0.9039\n",
            "\n",
            "Epoch 83:\n",
            "Training loss: 0.0016, Training accuracy: 1.0000\n",
            "Validation loss: 0.3627, Validation accuracy: 0.9040\n",
            "\n",
            "Epoch 84:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3619, Validation accuracy: 0.9041\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 85:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3619, Validation accuracy: 0.9036\n",
            "\n",
            "Epoch 86:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3618, Validation accuracy: 0.9037\n",
            "\n",
            "Epoch 87:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3616, Validation accuracy: 0.9036\n",
            "\n",
            "Epoch 88:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3615, Validation accuracy: 0.9037\n",
            "\n",
            "Epoch 89:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3613, Validation accuracy: 0.9037\n",
            "\n",
            "Epoch 90:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3611, Validation accuracy: 0.9037\n",
            "\n",
            "Epoch 91:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3609, Validation accuracy: 0.9035\n",
            "\n",
            "Epoch 92:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3608, Validation accuracy: 0.9034\n",
            "\n",
            "Epoch 93:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3606, Validation accuracy: 0.9035\n",
            "\n",
            "Epoch 94:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3604, Validation accuracy: 0.9035\n",
            "\n",
            "Epoch 95:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3602, Validation accuracy: 0.9036\n",
            "\n",
            "Epoch 96:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3600, Validation accuracy: 0.9036\n",
            "\n",
            "Epoch 97:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3598, Validation accuracy: 0.9035\n",
            "\n",
            "Epoch 98:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3596, Validation accuracy: 0.9036\n",
            "\n",
            "Epoch 99:\n",
            "Training loss: 0.0015, Training accuracy: 1.0000\n",
            "Validation loss: 0.3595, Validation accuracy: 0.9036\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.9041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LyJfBTkkOx1o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}