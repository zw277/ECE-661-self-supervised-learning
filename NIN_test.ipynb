{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jSGmccarHkRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4322aab5-f467-49fe-f3ad-44c4e8a62d98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader,ConcatDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#credit to 'https://github.com/gidariss/FeatureLearningRotNet/blob/master/architectures/NetworkInNetwork.py'\n",
        "#NIN model\n",
        "import math\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.layers = nn.Sequential()\n",
        "        self.layers.add_module('Conv', nn.Conv2d(in_planes, out_planes,\n",
        "                                                 kernel_size=kernel_size, stride=1, padding=padding, bias=False))\n",
        "        self.layers.add_module('BatchNorm', nn.BatchNorm2d(out_planes))\n",
        "        self.layers.add_module('ReLU', nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class GlobalAveragePooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GlobalAveragePooling, self).__init__()\n",
        "\n",
        "    def forward(self, feat):\n",
        "        num_channels = feat.size(1)\n",
        "        return F.avg_pool2d(feat, (feat.size(2), feat.size(3))).view(-1, num_channels)\n",
        "\n",
        "class NetworkInNetwork(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(NetworkInNetwork, self).__init__()\n",
        "\n",
        "        num_classes = opt['num_classes']\n",
        "        num_inchannels = opt['num_inchannels'] if ('num_inchannels' in opt) else 3\n",
        "        num_stages = opt['num_stages'] if ('num_stages' in opt) else 3\n",
        "        use_avg_on_conv3 = opt['use_avg_on_conv3'] if ('use_avg_on_conv3' in opt) else True\n",
        "\n",
        "        assert(num_stages >= 3)\n",
        "        nChannels = 192\n",
        "        nChannels2 = 160\n",
        "        nChannels3 = 96\n",
        "\n",
        "        blocks = [nn.Sequential() for _ in range(num_stages)]\n",
        "        # 1st block\n",
        "        blocks[0].add_module('Block1_ConvB1', BasicBlock(num_inchannels, nChannels, 5))\n",
        "        blocks[0].add_module('Block1_ConvB2', BasicBlock(nChannels, nChannels2, 1))\n",
        "        blocks[0].add_module('Block1_ConvB3', BasicBlock(nChannels2, nChannels3, 1))\n",
        "        blocks[0].add_module('Block1_MaxPool', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "        # 2nd block\n",
        "        blocks[1].add_module('Block2_ConvB1', BasicBlock(nChannels3, nChannels, 5))\n",
        "        blocks[1].add_module('Block2_ConvB2', BasicBlock(nChannels, nChannels, 1))\n",
        "        blocks[1].add_module('Block2_ConvB3', BasicBlock(nChannels, nChannels, 1))\n",
        "        blocks[1].add_module('Block2_AvgPool', nn.AvgPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "        # 3rd block\n",
        "        blocks[2].add_module('Block3_ConvB1', BasicBlock(nChannels, nChannels, 3))\n",
        "        blocks[2].add_module('Block3_ConvB2', BasicBlock(nChannels, nChannels, 1))\n",
        "        blocks[2].add_module('Block3_ConvB3', BasicBlock(nChannels, nChannels, 1))\n",
        "\n",
        "        if num_stages > 3 and use_avg_on_conv3:\n",
        "            blocks[2].add_module('Block3_AvgPool', nn.AvgPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "        for s in range(3, num_stages):\n",
        "            blocks[s].add_module('Block'+str(s+1)+'_ConvB1', BasicBlock(nChannels, nChannels, 3))\n",
        "            blocks[s].add_module('Block'+str(s+1)+'_ConvB2', BasicBlock(nChannels, nChannels, 1))\n",
        "            blocks[s].add_module('Block'+str(s+1)+'_ConvB3', BasicBlock(nChannels, nChannels, 1))\n",
        "\n",
        "        # global average pooling and classifier\n",
        "        blocks.append(nn.Sequential())\n",
        "        blocks[-1].add_module('GlobalAveragePooling', GlobalAveragePooling())\n",
        "        blocks[-1].add_module('Classifier', nn.Linear(nChannels, num_classes))\n",
        "\n",
        "        self._feature_blocks = nn.ModuleList(blocks)\n",
        "        self.all_feat_names = ['conv'+str(s+1) for s in range(num_stages)] + ['classifier',]\n",
        "        assert(len(self.all_feat_names) == len(self._feature_blocks))\n",
        "\n",
        "    def _parse_out_keys_arg(self, out_feat_keys):\n",
        "        # By default return the features of the last layer / module.\n",
        "        out_feat_keys = [self.all_feat_names[-1],] if out_feat_keys is None else out_feat_keys\n",
        "\n",
        "        if len(out_feat_keys) == 0:\n",
        "            raise ValueError('Empty list of output feature keys.')\n",
        "\n",
        "        for f, key in enumerate(out_feat_keys):\n",
        "            if key not in self.all_feat_names:\n",
        "                raise ValueError('Feature with name {0} does not exist. Existing features: {1}.'.format(key, self.all_feat_names))\n",
        "            elif key in out_feat_keys[:f]:\n",
        "                raise ValueError('Duplicate output feature key: {0}.'.format(key))\n",
        "\n",
        "        # Find the highest output feature in `out_feat_keys\n",
        "        max_out_feat = max([self.all_feat_names.index(key) for key in out_feat_keys])\n",
        "\n",
        "        return out_feat_keys, max_out_feat\n",
        "\n",
        "    def forward(self, x, out_feat_keys=None):\n",
        "        \"\"\"Forward an image `x` through the network and return the asked output features.\n",
        "\n",
        "        Args:\n",
        "          x: input image.\n",
        "          out_feat_keys: a list/tuple with the feature names of the features\n",
        "                that the function should return. By default the last feature of\n",
        "                the network is returned.\n",
        "\n",
        "        Return:\n",
        "            out_feats: If multiple output features were asked then `out_feats`\n",
        "                is a list with the asked output features placed in the same\n",
        "                order as in `out_feat_keys`. If a single output feature was\n",
        "                asked then `out_feats` is that output feature (and not a list).\n",
        "        \"\"\"\n",
        "        out_feat_keys, max_out_feat = self._parse_out_keys_arg(out_feat_keys)\n",
        "        out_feats = [None] * len(out_feat_keys)\n",
        "\n",
        "        feat = x\n",
        "        for f in range(max_out_feat + 1):\n",
        "            feat = self._feature_blocks[f](feat)\n",
        "            key = self.all_feat_names[f]\n",
        "            if key in out_feat_keys:\n",
        "                out_feats[out_feat_keys.index(key)] = feat\n",
        "\n",
        "        out_feats = out_feats[0] if len(out_feats) == 1 else out_feats\n",
        "        return out_feats\n",
        "\n",
        "    def weight_initialization(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m.weight.requires_grad:\n",
        "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                if m.weight.requires_grad:\n",
        "                    m.weight.data.fill_(1)\n",
        "                if m.bias.requires_grad:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                if m.bias.requires_grad:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "def create_model(opt):\n",
        "    return NetworkInNetwork(opt)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    size = 32\n",
        "    opt = {'num_classes': 4, 'num_stages': 4}\n",
        "\n",
        "    net = create_model(opt)\n",
        "    x = torch.autograd.Variable(torch.FloatTensor(1, 3, size, size).uniform_(-1, 1))\n",
        "\n",
        "    out = net(x, out_feat_keys=net.all_feat_names)\n",
        "    for f in range(len(out)):\n",
        "        print('Output feature {0} - size {1}'.format(\n",
        "            net.all_feat_names[f], out[f].size()))\n",
        "\n",
        "    out = net(x)\n",
        "    print('Final output: {0}'.format(out.size()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khAhXwfeu_60",
        "outputId": "8e75606f-2dc7-4c1d-e9a3-4757820d9101"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output feature conv1 - size torch.Size([1, 96, 16, 16])\n",
            "Output feature conv2 - size torch.Size([1, 192, 8, 8])\n",
            "Output feature conv3 - size torch.Size([1, 192, 4, 4])\n",
            "Output feature conv4 - size torch.Size([1, 192, 4, 4])\n",
            "Output feature classifier - size torch.Size([1, 4])\n",
            "Final output: torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#credit to 'https://github.com/gidariss/FeatureLearningRotNet/blob/master/architectures/LinearClassifier.py'\n",
        "#linear classifier\n",
        "class Flatten(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "\n",
        "    def forward(self, feat):\n",
        "        return feat.view(feat.size(0), -1)\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(Classifier, self).__init__()\n",
        "        nChannels = opt['nChannels']\n",
        "        num_classes = opt['num_classes']\n",
        "        pool_size = opt['pool_size']\n",
        "        pool_type = opt['pool_type'] if ('pool_type' in opt) else 'max'\n",
        "        nChannelsAll = nChannels * pool_size * pool_size\n",
        "\n",
        "        self.classifier = nn.Sequential()\n",
        "        if pool_type == 'max':\n",
        "            self.classifier.add_module('MaxPool', nn.AdaptiveMaxPool2d((pool_size, pool_size)))\n",
        "        elif pool_type == 'avg':\n",
        "            self.classifier.add_module('AvgPool', nn.AdaptiveAvgPool2d((pool_size, pool_size)))\n",
        "        self.classifier.add_module('BatchNorm', nn.BatchNorm2d(nChannels, affine=False))\n",
        "        self.classifier.add_module('Flatten', Flatten())\n",
        "        self.classifier.add_module('LinearClassifier', nn.Linear(nChannelsAll, num_classes))\n",
        "        self.initialize()\n",
        "\n",
        "    def forward(self, feat):\n",
        "        return self.classifier(feat)\n",
        "\n",
        "    def initialize(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                fin = m.in_features\n",
        "                fout = m.out_features\n",
        "                std_val = np.sqrt(2.0 / fout)\n",
        "                m.weight.data.normal_(0.0, std_val)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0.0)"
      ],
      "metadata": {
        "id": "1_UrbETWRPmX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#conv classifier\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.layers = nn.Sequential()\n",
        "        self.layers.add_module('Conv', nn.Conv2d(in_planes, out_planes,\n",
        "                                                 kernel_size=kernel_size, stride=stride, padding=padding, bias=False))\n",
        "        self.layers.add_module('BatchNorm', nn.BatchNorm2d(out_planes))\n",
        "        self.layers.add_module('ReLU', nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class GlobalAvgPool(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GlobalAvgPool, self).__init__()\n",
        "\n",
        "    def forward(self, feat):\n",
        "        assert(feat.size(2) == feat.size(3))\n",
        "        feat_avg = F.avg_pool2d(feat, feat.size(2)).view(-1, feat.size(1))\n",
        "        return feat_avg\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "\n",
        "    def forward(self, feat):\n",
        "        return feat.view(feat.size(0), -1)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(Classifier, self).__init__()\n",
        "        nChannels = opt['nChannels']\n",
        "        num_classes = opt['num_classes']\n",
        "        self.cls_type = opt['cls_type']\n",
        "\n",
        "        self.classifier = nn.Sequential()\n",
        "\n",
        "        if self.cls_type == 'MultLayer':\n",
        "            nFeats = min(num_classes*20, 2048)\n",
        "            self.classifier.add_module('Flatten', Flatten())\n",
        "            self.classifier.add_module('Linear_1', nn.Linear(nChannels, nFeats, bias=False))\n",
        "            self.classifier.add_module('BatchNorm_1', nn.BatchNorm2d(nFeats))\n",
        "            self.classifier.add_module('ReLU_1', nn.ReLU(inplace=True))\n",
        "            self.classifier.add_module('Linear_2', nn.Linear(nFeats, nFeats, bias=False))\n",
        "            self.classifier.add_module('BatchNorm2d', nn.BatchNorm2d(nFeats))\n",
        "            self.classifier.add_module('ReLU_2', nn.ReLU(inplace=True))\n",
        "            self.classifier.add_module('Linear_F', nn.Linear(nFeats, num_classes))\n",
        "        elif self.cls_type == 'NIN_ConvBlock3':\n",
        "            self.classifier.add_module('Block3_ConvB1', BasicBlock(nChannels, 192, 3))\n",
        "            self.classifier.add_module('Block3_ConvB2', BasicBlock(192, 192, 1))\n",
        "            self.classifier.add_module('Block3_ConvB3', BasicBlock(192, 192, 1))\n",
        "            self.classifier.add_module('GlobalAvgPool', GlobalAvgPool())\n",
        "            self.classifier.add_module('Linear_F', nn.Linear(192, num_classes))\n",
        "        elif self.cls_type == 'Alexnet_conv5' or self.cls_type == 'Alexnet_conv4':\n",
        "            if self.cls_type == 'Alexnet_conv4':\n",
        "                block5 = nn.Sequential(\n",
        "                    nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "                    nn.BatchNorm2d(256),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                )\n",
        "                self.classifier.add_module('ConvB5', block5)\n",
        "            self.classifier.add_module('Pool5', nn.MaxPool2d(kernel_size=3, stride=2))\n",
        "            self.classifier.add_module('Flatten', Flatten())\n",
        "            self.classifier.add_module('Linear1', nn.Linear(256*6*6, 4096, bias=False))\n",
        "            self.classifier.add_module('BatchNorm1', nn.BatchNorm1d(4096))\n",
        "            self.classifier.add_module('ReLU1', nn.ReLU(inplace=True))\n",
        "            self.classifier.add_module('Linear2', nn.Linear(4096, 4096, bias=False))\n",
        "            self.classifier.add_module('BatchNorm2', nn.BatchNorm1d(4096))\n",
        "            self.classifier.add_module('ReLU2', nn.ReLU(inplace=True))\n",
        "            self.classifier.add_module('LinearF', nn.Linear(4096, num_classes))\n",
        "        else:\n",
        "            raise ValueError('Not recognized classifier type: %s' % self.cls_type)\n",
        "\n",
        "        self.initialize()\n",
        "\n",
        "    def forward(self, feat):\n",
        "        return self.classifier(feat)\n",
        "\n",
        "    def initialize(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                fin = m.in_features\n",
        "                fout = m.out_features\n",
        "                std_val = np.sqrt(2.0/fout)\n",
        "                m.weight.data.normal_(0.0, std_val)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0.0)\n",
        "\n",
        "def create_model(opt):\n",
        "    return Classifier(opt)\n",
        "\n",
        "# Example usage:\n",
        "# size = 32\n",
        "# opt = {'num_classes': 10, 'nChannels': 3, 'cls_type': 'MultLayer'}\n",
        "# net = create_model(opt)\n",
        "# x = torch.autograd.Variable(torch.FloatTensor(1, 3, size, size).uniform_(-1, 1))\n",
        "# out = net(x)\n",
        "# print(out)\n",
        "\n",
        "\n",
        "# cls_net_optim_params = {'optim_type': 'sgd', 'lr': 0.1, 'momentum':0.9, 'weight_decay': 5e-4, 'nesterov': True, 'LUT_lr':[(35, 0.1),(70, 0.02),(85, 0.004),(100, 0.0008)]}\n",
        "# cls_net_opt = {'num_classes':10, 'nChannels':192, 'cls_type':'NIN_ConvBlock3'}"
      ],
      "metadata": {
        "id": "zTud8SIwSFfL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example of get the feature map generated by the pretrained model(2nd block)\n",
        "\n",
        "NIN_net_4block = NetworkInNetwork({'num_classes': 4, 'num_stages': 4, 'use_avg_on_conv3': False})\n",
        "\n",
        "# Load the state_dict using torch.load\n",
        "checkpoint_path = '/content/NIN_net_4block.pth'\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# Load the state_dict into the model\n",
        "NIN_net_4block.load_state_dict(checkpoint)\n",
        "\n",
        "out = NIN_net_4block(x, out_feat_keys=NIN_net_4block.all_feat_names)\n",
        "for f in range(len(out)):\n",
        "    print('Output feature {0} - size {1}'.format(\n",
        "        NIN_net_4block.all_feat_names[f], out[f]))"
      ],
      "metadata": {
        "id": "xHJ1uJOAViMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "out_feat_keys = ['conv2']  # Specify the desired feature key(s)\n",
        "\n",
        "out = NIN_net_4block(x, out_feat_keys=out_feat_keys)\n",
        "\n",
        "# Print the size of the desired feature(s)\n",
        "for f in range(len(out)):\n",
        "    print('Output feature {0} - size {1}'.format(\n",
        "        out_feat_keys[f], out[f].shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGDQHdaSOTch",
        "outputId": "97ad6860-b999-45af-a222-b3cab5c70ba0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output feature conv2 - size torch.Size([192, 8, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming NIN_net_4block and Classifier are already defined\n",
        "\n",
        "# Set up the model and move it to the device  # or 'cpu' depending on your availability\n",
        "\n",
        "NIN_net_4block.to(device)\n",
        "classifier = Classifier({'num_classes': 10, 'nChannels': 192, 'cls_type': 'NIN_ConvBlock3'}).to(device)\n",
        "\n",
        "\n",
        "# out_feat_keys = ['conv2']\n",
        "\n",
        "# feature_map = NIN_net_4block(x, out_feat_keys=out_feat_keys)\n",
        "\n",
        "# # Use the extracted feature map for classification\n",
        "# classification_result = classifier(feature_map)\n",
        "\n",
        "# # Print the classification result\n",
        "# print('Classification result:', classification_result)\n"
      ],
      "metadata": {
        "id": "8G_DgARlOVMg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature_map = NIN_net_4block(x, out_feat_keys=out_feat_keys)\n",
        "# feature_map.shape"
      ],
      "metadata": {
        "id": "IYfhco-bPqtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "#############################################\n",
        "# your code here\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(size = 32, padding=4),#random cropping\n",
        "    transforms.RandomHorizontalFlip() ,#random flip\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "# specify preprocessing function\n",
        "\n",
        "transform_train = transform\n",
        "\n",
        "transform_val = transform\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# a few arguments, do NOT change these\n",
        "DATA_ROOT = \"./data\"\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "VAL_BATCH_SIZE = 100\n",
        "\n",
        "#############################################\n",
        "# your code here\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "# your code here\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "train_set = CIFAR10(\n",
        "    root=DATA_ROOT,\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "\n",
        "val_set = CIFAR10(\n",
        "    root=DATA_ROOT,\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_val\n",
        ")\n",
        "\n",
        "# construct dataloader\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_set,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_set,\n",
        "    batch_size=VAL_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIWI2x6BXJBf",
        "outputId": "2412cdf3-b3a3-4623-d91c-d2349832f1b1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**random initialize + conv**\n"
      ],
      "metadata": {
        "id": "Lji2wVaveDZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameter setting\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "EPOCHS = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(classifier.parameters(), lr = INITIAL_LR,\n",
        "                      momentum = MOMENTUM,\n",
        "                      weight_decay=REG)"
      ],
      "metadata": {
        "id": "kKly5X1FZh2f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# the folder where the trained model is saved\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "# start the training/validation process\n",
        "# the process should take about 5 minutes on a GTX 1070-Ti\n",
        "# if the code is written efficiently.\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY = 0.2\n",
        "valid_acc = []\n",
        "losslist = []\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    if i  == 30 or i == 60 or i == 80:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    classifier.train()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    # this help you compute the training accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0 # track training loss if you want\n",
        "\n",
        "    # Train the model for 1 epoch.\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        ####################################\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # inputs = inputs.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "        feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "        targets = targets.to(device)\n",
        "        NIN_net_4block = NIN_net_4block.to(device)\n",
        "        # compute the output and loss\n",
        "        outputs = classifier(feature_map)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss\n",
        "        # zero the gradient\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        # apply gradient and update the weights\n",
        "        optimizer.step()\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "        _, predicted = outputs.max(1)  #make prediction based on the highest value\n",
        "        total_examples += targets.size(0) # in this case,128 for each batch\n",
        "        correct_examples += predicted.eq(targets).sum().item()\n",
        "        ####################################\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "\n",
        "    classifier.eval()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    # this help you compute the validation accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 # again, track the validation loss if you want\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            ####################################\n",
        "            # your code here\n",
        "            # copy inputs to device\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "            NIN_net_4block = NIN_net_4block.to(device)\n",
        "\n",
        "\n",
        "            # compute the output and loss\n",
        "\n",
        "            outputs = classifier(feature_map)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # count the number of correctly predicted samples in the current batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += predicted.eq(targets).sum().item()\n",
        "            ####################################\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    valid_acc.append(avg_acc)\n",
        "    losslist.append(avg_loss.item())\n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        #if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "        #    os.makedirs(CHECKPOINT_FOLDER)\n",
        "        #print(\"Saving ...\")\n",
        "\n",
        "        torch.save(classifier.state_dict(), 'classifier_conv3_NIN_block2.pth')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "476G-S_vRzCG",
        "outputId": "6376c5e4-af88-4ea5-dcaa-4bc6ec0a66e4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 1.5704, Training accuracy: 0.4399\n",
            "Validation loss: 1.3744, Validation accuracy: 0.5030\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 1.2935, Training accuracy: 0.5370\n",
            "Validation loss: 1.3538, Validation accuracy: 0.5139\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 1.2099, Training accuracy: 0.5671\n",
            "Validation loss: 1.3084, Validation accuracy: 0.5333\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 1.1651, Training accuracy: 0.5842\n",
            "Validation loss: 1.3662, Validation accuracy: 0.5196\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 1.1229, Training accuracy: 0.6000\n",
            "Validation loss: 1.1910, Validation accuracy: 0.5790\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 1.0945, Training accuracy: 0.6127\n",
            "Validation loss: 1.1419, Validation accuracy: 0.5938\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 1.0828, Training accuracy: 0.6166\n",
            "Validation loss: 1.2562, Validation accuracy: 0.5696\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 1.0550, Training accuracy: 0.6263\n",
            "Validation loss: 1.2309, Validation accuracy: 0.5752\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 1.0429, Training accuracy: 0.6303\n",
            "Validation loss: 1.1248, Validation accuracy: 0.6000\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 1.0371, Training accuracy: 0.6334\n",
            "Validation loss: 1.2660, Validation accuracy: 0.5552\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 1.0191, Training accuracy: 0.6403\n",
            "Validation loss: 1.1997, Validation accuracy: 0.5854\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 1.0104, Training accuracy: 0.6411\n",
            "Validation loss: 1.2119, Validation accuracy: 0.5750\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 1.0010, Training accuracy: 0.6489\n",
            "Validation loss: 1.2169, Validation accuracy: 0.5730\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 0.9963, Training accuracy: 0.6485\n",
            "Validation loss: 1.0524, Validation accuracy: 0.6269\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 0.9900, Training accuracy: 0.6511\n",
            "Validation loss: 1.4112, Validation accuracy: 0.5296\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 0.9769, Training accuracy: 0.6534\n",
            "Validation loss: 1.1798, Validation accuracy: 0.5993\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 0.9826, Training accuracy: 0.6516\n",
            "Validation loss: 1.1266, Validation accuracy: 0.6056\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 0.9831, Training accuracy: 0.6559\n",
            "Validation loss: 1.2626, Validation accuracy: 0.5729\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 0.9673, Training accuracy: 0.6604\n",
            "Validation loss: 1.3936, Validation accuracy: 0.5318\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 0.9656, Training accuracy: 0.6607\n",
            "Validation loss: 1.2769, Validation accuracy: 0.5637\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 0.9662, Training accuracy: 0.6593\n",
            "Validation loss: 1.2087, Validation accuracy: 0.5817\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 0.9565, Training accuracy: 0.6664\n",
            "Validation loss: 1.1690, Validation accuracy: 0.5921\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 0.9613, Training accuracy: 0.6616\n",
            "Validation loss: 1.1686, Validation accuracy: 0.5886\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 0.9545, Training accuracy: 0.6635\n",
            "Validation loss: 1.2974, Validation accuracy: 0.5715\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 0.9507, Training accuracy: 0.6653\n",
            "Validation loss: 1.1160, Validation accuracy: 0.6028\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 0.9546, Training accuracy: 0.6658\n",
            "Validation loss: 1.1041, Validation accuracy: 0.6122\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 0.9479, Training accuracy: 0.6664\n",
            "Validation loss: 1.1615, Validation accuracy: 0.5998\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 0.9439, Training accuracy: 0.6688\n",
            "Validation loss: 1.1743, Validation accuracy: 0.5991\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 0.9503, Training accuracy: 0.6672\n",
            "Validation loss: 1.1736, Validation accuracy: 0.5970\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 0.9455, Training accuracy: 0.6652\n",
            "Validation loss: 1.1259, Validation accuracy: 0.6101\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 30:\n",
            "Training loss: 0.7818, Training accuracy: 0.7281\n",
            "Validation loss: 0.8725, Validation accuracy: 0.6983\n",
            "\n",
            "Epoch 31:\n",
            "Training loss: 0.7264, Training accuracy: 0.7463\n",
            "Validation loss: 0.8931, Validation accuracy: 0.6987\n",
            "\n",
            "Epoch 32:\n",
            "Training loss: 0.7176, Training accuracy: 0.7472\n",
            "Validation loss: 0.8707, Validation accuracy: 0.6977\n",
            "\n",
            "Epoch 33:\n",
            "Training loss: 0.7022, Training accuracy: 0.7547\n",
            "Validation loss: 0.8936, Validation accuracy: 0.6874\n",
            "\n",
            "Epoch 34:\n",
            "Training loss: 0.7021, Training accuracy: 0.7565\n",
            "Validation loss: 0.9088, Validation accuracy: 0.6880\n",
            "\n",
            "Epoch 35:\n",
            "Training loss: 0.6921, Training accuracy: 0.7568\n",
            "Validation loss: 0.8998, Validation accuracy: 0.6920\n",
            "\n",
            "Epoch 36:\n",
            "Training loss: 0.6898, Training accuracy: 0.7576\n",
            "Validation loss: 0.8752, Validation accuracy: 0.6989\n",
            "\n",
            "Epoch 37:\n",
            "Training loss: 0.6843, Training accuracy: 0.7610\n",
            "Validation loss: 0.9056, Validation accuracy: 0.6923\n",
            "\n",
            "Epoch 38:\n",
            "Training loss: 0.6867, Training accuracy: 0.7589\n",
            "Validation loss: 0.9163, Validation accuracy: 0.6900\n",
            "\n",
            "Epoch 39:\n",
            "Training loss: 0.6797, Training accuracy: 0.7599\n",
            "Validation loss: 0.9131, Validation accuracy: 0.6876\n",
            "\n",
            "Epoch 40:\n",
            "Training loss: 0.6808, Training accuracy: 0.7626\n",
            "Validation loss: 0.9475, Validation accuracy: 0.6807\n",
            "\n",
            "Epoch 41:\n",
            "Training loss: 0.6765, Training accuracy: 0.7635\n",
            "Validation loss: 0.9676, Validation accuracy: 0.6759\n",
            "\n",
            "Epoch 42:\n",
            "Training loss: 0.6773, Training accuracy: 0.7637\n",
            "Validation loss: 0.9434, Validation accuracy: 0.6774\n",
            "\n",
            "Epoch 43:\n",
            "Training loss: 0.6700, Training accuracy: 0.7641\n",
            "Validation loss: 0.9745, Validation accuracy: 0.6664\n",
            "\n",
            "Epoch 44:\n",
            "Training loss: 0.6734, Training accuracy: 0.7654\n",
            "Validation loss: 0.9390, Validation accuracy: 0.6771\n",
            "\n",
            "Epoch 45:\n",
            "Training loss: 0.6746, Training accuracy: 0.7627\n",
            "Validation loss: 0.9170, Validation accuracy: 0.6876\n",
            "\n",
            "Epoch 46:\n",
            "Training loss: 0.6687, Training accuracy: 0.7650\n",
            "Validation loss: 0.9470, Validation accuracy: 0.6783\n",
            "\n",
            "Epoch 47:\n",
            "Training loss: 0.6655, Training accuracy: 0.7677\n",
            "Validation loss: 0.9342, Validation accuracy: 0.6817\n",
            "\n",
            "Epoch 48:\n",
            "Training loss: 0.6630, Training accuracy: 0.7662\n",
            "Validation loss: 0.9134, Validation accuracy: 0.6904\n",
            "\n",
            "Epoch 49:\n",
            "Training loss: 0.6639, Training accuracy: 0.7672\n",
            "Validation loss: 0.9506, Validation accuracy: 0.6774\n",
            "\n",
            "Epoch 50:\n",
            "Training loss: 0.6603, Training accuracy: 0.7713\n",
            "Validation loss: 0.9090, Validation accuracy: 0.6901\n",
            "\n",
            "Epoch 51:\n",
            "Training loss: 0.6613, Training accuracy: 0.7670\n",
            "Validation loss: 0.9458, Validation accuracy: 0.6844\n",
            "\n",
            "Epoch 52:\n",
            "Training loss: 0.6573, Training accuracy: 0.7702\n",
            "Validation loss: 0.9474, Validation accuracy: 0.6780\n",
            "\n",
            "Epoch 53:\n",
            "Training loss: 0.6567, Training accuracy: 0.7708\n",
            "Validation loss: 0.9440, Validation accuracy: 0.6825\n",
            "\n",
            "Epoch 54:\n",
            "Training loss: 0.6550, Training accuracy: 0.7710\n",
            "Validation loss: 0.9582, Validation accuracy: 0.6791\n",
            "\n",
            "Epoch 55:\n",
            "Training loss: 0.6469, Training accuracy: 0.7737\n",
            "Validation loss: 0.9117, Validation accuracy: 0.6924\n",
            "\n",
            "Epoch 56:\n",
            "Training loss: 0.6438, Training accuracy: 0.7732\n",
            "Validation loss: 0.9078, Validation accuracy: 0.6858\n",
            "\n",
            "Epoch 57:\n",
            "Training loss: 0.6430, Training accuracy: 0.7746\n",
            "Validation loss: 0.8971, Validation accuracy: 0.6901\n",
            "\n",
            "Epoch 58:\n",
            "Training loss: 0.6488, Training accuracy: 0.7725\n",
            "Validation loss: 0.9460, Validation accuracy: 0.6802\n",
            "\n",
            "Epoch 59:\n",
            "Training loss: 0.6401, Training accuracy: 0.7750\n",
            "Validation loss: 0.9457, Validation accuracy: 0.6882\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 60:\n",
            "Training loss: 0.5302, Training accuracy: 0.8179\n",
            "Validation loss: 0.7978, Validation accuracy: 0.7253\n",
            "\n",
            "Epoch 61:\n",
            "Training loss: 0.4941, Training accuracy: 0.8293\n",
            "Validation loss: 0.7925, Validation accuracy: 0.7331\n",
            "\n",
            "Epoch 62:\n",
            "Training loss: 0.4780, Training accuracy: 0.8386\n",
            "Validation loss: 0.7920, Validation accuracy: 0.7268\n",
            "\n",
            "Epoch 63:\n",
            "Training loss: 0.4692, Training accuracy: 0.8410\n",
            "Validation loss: 0.8162, Validation accuracy: 0.7244\n",
            "\n",
            "Epoch 64:\n",
            "Training loss: 0.4589, Training accuracy: 0.8430\n",
            "Validation loss: 0.8075, Validation accuracy: 0.7283\n",
            "\n",
            "Epoch 65:\n",
            "Training loss: 0.4532, Training accuracy: 0.8453\n",
            "Validation loss: 0.8015, Validation accuracy: 0.7317\n",
            "\n",
            "Epoch 66:\n",
            "Training loss: 0.4498, Training accuracy: 0.8464\n",
            "Validation loss: 0.8017, Validation accuracy: 0.7300\n",
            "\n",
            "Epoch 67:\n",
            "Training loss: 0.4425, Training accuracy: 0.8500\n",
            "Validation loss: 0.8217, Validation accuracy: 0.7214\n",
            "\n",
            "Epoch 68:\n",
            "Training loss: 0.4396, Training accuracy: 0.8486\n",
            "Validation loss: 0.8073, Validation accuracy: 0.7318\n",
            "\n",
            "Epoch 69:\n",
            "Training loss: 0.4386, Training accuracy: 0.8498\n",
            "Validation loss: 0.8370, Validation accuracy: 0.7236\n",
            "\n",
            "Epoch 70:\n",
            "Training loss: 0.4334, Training accuracy: 0.8529\n",
            "Validation loss: 0.8294, Validation accuracy: 0.7257\n",
            "\n",
            "Epoch 71:\n",
            "Training loss: 0.4303, Training accuracy: 0.8517\n",
            "Validation loss: 0.8524, Validation accuracy: 0.7161\n",
            "\n",
            "Epoch 72:\n",
            "Training loss: 0.4235, Training accuracy: 0.8549\n",
            "Validation loss: 0.8406, Validation accuracy: 0.7257\n",
            "\n",
            "Epoch 73:\n",
            "Training loss: 0.4216, Training accuracy: 0.8565\n",
            "Validation loss: 0.8465, Validation accuracy: 0.7215\n",
            "\n",
            "Epoch 74:\n",
            "Training loss: 0.4201, Training accuracy: 0.8558\n",
            "Validation loss: 0.8426, Validation accuracy: 0.7165\n",
            "\n",
            "Epoch 75:\n",
            "Training loss: 0.4216, Training accuracy: 0.8556\n",
            "Validation loss: 0.8386, Validation accuracy: 0.7191\n",
            "\n",
            "Epoch 76:\n",
            "Training loss: 0.4152, Training accuracy: 0.8574\n",
            "Validation loss: 0.8339, Validation accuracy: 0.7246\n",
            "\n",
            "Epoch 77:\n",
            "Training loss: 0.4142, Training accuracy: 0.8572\n",
            "Validation loss: 0.8692, Validation accuracy: 0.7176\n",
            "\n",
            "Epoch 78:\n",
            "Training loss: 0.4146, Training accuracy: 0.8555\n",
            "Validation loss: 0.8532, Validation accuracy: 0.7185\n",
            "\n",
            "Epoch 79:\n",
            "Training loss: 0.4080, Training accuracy: 0.8605\n",
            "Validation loss: 0.8661, Validation accuracy: 0.7221\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 80:\n",
            "Training loss: 0.3706, Training accuracy: 0.8755\n",
            "Validation loss: 0.8114, Validation accuracy: 0.7321\n",
            "\n",
            "Epoch 81:\n",
            "Training loss: 0.3565, Training accuracy: 0.8807\n",
            "Validation loss: 0.8110, Validation accuracy: 0.7308\n",
            "\n",
            "Epoch 82:\n",
            "Training loss: 0.3552, Training accuracy: 0.8840\n",
            "Validation loss: 0.8112, Validation accuracy: 0.7348\n",
            "\n",
            "Epoch 83:\n",
            "Training loss: 0.3486, Training accuracy: 0.8849\n",
            "Validation loss: 0.8121, Validation accuracy: 0.7322\n",
            "\n",
            "Epoch 84:\n",
            "Training loss: 0.3419, Training accuracy: 0.8866\n",
            "Validation loss: 0.8181, Validation accuracy: 0.7303\n",
            "\n",
            "Epoch 85:\n",
            "Training loss: 0.3441, Training accuracy: 0.8843\n",
            "Validation loss: 0.8130, Validation accuracy: 0.7337\n",
            "\n",
            "Epoch 86:\n",
            "Training loss: 0.3430, Training accuracy: 0.8862\n",
            "Validation loss: 0.8053, Validation accuracy: 0.7373\n",
            "\n",
            "Epoch 87:\n",
            "Training loss: 0.3407, Training accuracy: 0.8857\n",
            "Validation loss: 0.8303, Validation accuracy: 0.7316\n",
            "\n",
            "Epoch 88:\n",
            "Training loss: 0.3409, Training accuracy: 0.8860\n",
            "Validation loss: 0.8274, Validation accuracy: 0.7304\n",
            "\n",
            "Epoch 89:\n",
            "Training loss: 0.3358, Training accuracy: 0.8888\n",
            "Validation loss: 0.8102, Validation accuracy: 0.7355\n",
            "\n",
            "Epoch 90:\n",
            "Training loss: 0.3351, Training accuracy: 0.8890\n",
            "Validation loss: 0.8266, Validation accuracy: 0.7327\n",
            "\n",
            "Epoch 91:\n",
            "Training loss: 0.3336, Training accuracy: 0.8879\n",
            "Validation loss: 0.8254, Validation accuracy: 0.7304\n",
            "\n",
            "Epoch 92:\n",
            "Training loss: 0.3333, Training accuracy: 0.8888\n",
            "Validation loss: 0.8332, Validation accuracy: 0.7285\n",
            "\n",
            "Epoch 93:\n",
            "Training loss: 0.3326, Training accuracy: 0.8901\n",
            "Validation loss: 0.8223, Validation accuracy: 0.7293\n",
            "\n",
            "Epoch 94:\n",
            "Training loss: 0.3287, Training accuracy: 0.8902\n",
            "Validation loss: 0.8148, Validation accuracy: 0.7319\n",
            "\n",
            "Epoch 95:\n",
            "Training loss: 0.3302, Training accuracy: 0.8905\n",
            "Validation loss: 0.8332, Validation accuracy: 0.7312\n",
            "\n",
            "Epoch 96:\n",
            "Training loss: 0.3306, Training accuracy: 0.8911\n",
            "Validation loss: 0.8246, Validation accuracy: 0.7258\n",
            "\n",
            "Epoch 97:\n",
            "Training loss: 0.3283, Training accuracy: 0.8902\n",
            "Validation loss: 0.8405, Validation accuracy: 0.7284\n",
            "\n",
            "Epoch 98:\n",
            "Training loss: 0.3255, Training accuracy: 0.8931\n",
            "Validation loss: 0.8192, Validation accuracy: 0.7344\n",
            "\n",
            "Epoch 99:\n",
            "Training loss: 0.3214, Training accuracy: 0.8917\n",
            "Validation loss: 0.8319, Validation accuracy: 0.7253\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.7373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(valid_acc)\n",
        "plt.ylim(0.4,0.8)\n",
        "plt.xlabel('num_epochs')\n",
        "plt.ylabel('valid_acc')\n",
        "plt.title('NIN_block2_conv3_classify_wholelabel')"
      ],
      "metadata": {
        "id": "bh-9RLnQg3Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fc-layer + 2nb block feature map random init\n",
        "NIN_net_4block.to(device)\n",
        "out_feat_keys = ['conv1']\n",
        "classifier = Classifier({'num_classes': 10, 'nChannels': 96, 'cls_type': 'NIN_ConvBlock3'}).to(device)\n",
        "#hyperparameter setting\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "EPOCHS = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(classifier.parameters(), lr = INITIAL_LR,\n",
        "                      momentum = MOMENTUM,\n",
        "                      weight_decay=REG)"
      ],
      "metadata": {
        "id": "eFezQmLLnaKk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the folder where the trained model is saved\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "# start the training/validation process\n",
        "# the process should take about 5 minutes on a GTX 1070-Ti\n",
        "# if the code is written efficiently.\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY = 0.2\n",
        "valid_acc_fc = []\n",
        "losslist = []\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    if i  == 30 or i == 60 or i == 80:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    classifier.train()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    # this help you compute the training accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0 # track training loss if you want\n",
        "\n",
        "    # Train the model for 1 epoch.\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        ####################################\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # inputs = inputs.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "        feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "        targets = targets.to(device)\n",
        "        NIN_net_4block = NIN_net_4block.to(device)\n",
        "        # compute the output and loss\n",
        "        outputs = classifier(feature_map)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss\n",
        "        # zero the gradient\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        # apply gradient and update the weights\n",
        "        optimizer.step()\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "        _, predicted = outputs.max(1)  #make prediction based on the highest value\n",
        "        total_examples += targets.size(0) # in this case,128 for each batch\n",
        "        correct_examples += predicted.eq(targets).sum().item()\n",
        "        ####################################\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "\n",
        "    classifier.eval()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    # this help you compute the validation accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 # again, track the validation loss if you want\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            ####################################\n",
        "            # your code here\n",
        "            # copy inputs to device\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "            NIN_net_4block = NIN_net_4block.to(device)\n",
        "\n",
        "\n",
        "            # compute the output and loss\n",
        "\n",
        "            outputs = classifier(feature_map)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # count the number of correctly predicted samples in the current batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += predicted.eq(targets).sum().item()\n",
        "            ####################################\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    valid_acc.append(avg_acc)\n",
        "    losslist.append(avg_loss.item())\n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        #if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "        #    os.makedirs(CHECKPOINT_FOLDER)\n",
        "        #print(\"Saving ...\")\n",
        "\n",
        "        torch.save(classifier.state_dict(), 'classifier_conv3_NIN_block1.pth')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIrYOnSwoAVv",
        "outputId": "f5871fbf-96e4-4188-af3a-47bc3204f2f3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 1.0290, Training accuracy: 0.6358\n",
            "Validation loss: 1.1830, Validation accuracy: 0.5717\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 1.0098, Training accuracy: 0.6416\n",
            "Validation loss: 1.2225, Validation accuracy: 0.5823\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 0.9982, Training accuracy: 0.6476\n",
            "Validation loss: 1.2472, Validation accuracy: 0.5735\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 0.9897, Training accuracy: 0.6527\n",
            "Validation loss: 1.2467, Validation accuracy: 0.5576\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 0.9715, Training accuracy: 0.6577\n",
            "Validation loss: 1.2483, Validation accuracy: 0.5607\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 0.9639, Training accuracy: 0.6590\n",
            "Validation loss: 1.2321, Validation accuracy: 0.5738\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 0.9600, Training accuracy: 0.6611\n",
            "Validation loss: 1.1091, Validation accuracy: 0.6160\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 0.9536, Training accuracy: 0.6634\n",
            "Validation loss: 1.1143, Validation accuracy: 0.5973\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 0.9370, Training accuracy: 0.6710\n",
            "Validation loss: 1.2172, Validation accuracy: 0.5704\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 0.9342, Training accuracy: 0.6717\n",
            "Validation loss: 1.4636, Validation accuracy: 0.4983\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 0.9248, Training accuracy: 0.6764\n",
            "Validation loss: 1.2498, Validation accuracy: 0.5792\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 0.9229, Training accuracy: 0.6753\n",
            "Validation loss: 1.6976, Validation accuracy: 0.4654\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 0.9182, Training accuracy: 0.6781\n",
            "Validation loss: 1.4990, Validation accuracy: 0.5141\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 0.9184, Training accuracy: 0.6779\n",
            "Validation loss: 1.3987, Validation accuracy: 0.5494\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 0.9101, Training accuracy: 0.6810\n",
            "Validation loss: 1.0333, Validation accuracy: 0.6343\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 0.9094, Training accuracy: 0.6827\n",
            "Validation loss: 1.0766, Validation accuracy: 0.6250\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 0.9055, Training accuracy: 0.6842\n",
            "Validation loss: 1.2929, Validation accuracy: 0.5694\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 0.9116, Training accuracy: 0.6802\n",
            "Validation loss: 1.1430, Validation accuracy: 0.6056\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 0.8968, Training accuracy: 0.6855\n",
            "Validation loss: 1.7604, Validation accuracy: 0.5053\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 0.9012, Training accuracy: 0.6852\n",
            "Validation loss: 1.0528, Validation accuracy: 0.6358\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 0.8922, Training accuracy: 0.6881\n",
            "Validation loss: 1.3383, Validation accuracy: 0.5630\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 0.8950, Training accuracy: 0.6859\n",
            "Validation loss: 1.3524, Validation accuracy: 0.5388\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 0.8930, Training accuracy: 0.6868\n",
            "Validation loss: 0.9884, Validation accuracy: 0.6562\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 0.8891, Training accuracy: 0.6868\n",
            "Validation loss: 1.0574, Validation accuracy: 0.6376\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 0.8883, Training accuracy: 0.6905\n",
            "Validation loss: 1.0792, Validation accuracy: 0.6291\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 0.8854, Training accuracy: 0.6889\n",
            "Validation loss: 1.0369, Validation accuracy: 0.6331\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 0.8940, Training accuracy: 0.6862\n",
            "Validation loss: 1.1612, Validation accuracy: 0.5856\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 0.8852, Training accuracy: 0.6913\n",
            "Validation loss: 1.3295, Validation accuracy: 0.5548\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 0.8833, Training accuracy: 0.6925\n",
            "Validation loss: 1.2258, Validation accuracy: 0.5722\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 0.8827, Training accuracy: 0.6910\n",
            "Validation loss: 1.3243, Validation accuracy: 0.5602\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 30:\n",
            "Training loss: 0.7139, Training accuracy: 0.7562\n",
            "Validation loss: 0.8178, Validation accuracy: 0.7183\n",
            "\n",
            "Epoch 31:\n",
            "Training loss: 0.6671, Training accuracy: 0.7705\n",
            "Validation loss: 0.7651, Validation accuracy: 0.7321\n",
            "\n",
            "Epoch 32:\n",
            "Training loss: 0.6534, Training accuracy: 0.7751\n",
            "Validation loss: 0.9049, Validation accuracy: 0.6949\n",
            "\n",
            "Epoch 33:\n",
            "Training loss: 0.6395, Training accuracy: 0.7798\n",
            "Validation loss: 0.8467, Validation accuracy: 0.7102\n",
            "\n",
            "Epoch 34:\n",
            "Training loss: 0.6409, Training accuracy: 0.7790\n",
            "Validation loss: 0.8190, Validation accuracy: 0.7221\n",
            "\n",
            "Epoch 35:\n",
            "Training loss: 0.6405, Training accuracy: 0.7793\n",
            "Validation loss: 0.8634, Validation accuracy: 0.7028\n",
            "\n",
            "Epoch 36:\n",
            "Training loss: 0.6286, Training accuracy: 0.7829\n",
            "Validation loss: 0.8449, Validation accuracy: 0.7163\n",
            "\n",
            "Epoch 37:\n",
            "Training loss: 0.6314, Training accuracy: 0.7819\n",
            "Validation loss: 0.7888, Validation accuracy: 0.7282\n",
            "\n",
            "Epoch 38:\n",
            "Training loss: 0.6322, Training accuracy: 0.7832\n",
            "Validation loss: 0.8458, Validation accuracy: 0.7107\n",
            "\n",
            "Epoch 39:\n",
            "Training loss: 0.6292, Training accuracy: 0.7828\n",
            "Validation loss: 0.9792, Validation accuracy: 0.6835\n",
            "\n",
            "Epoch 40:\n",
            "Training loss: 0.6237, Training accuracy: 0.7852\n",
            "Validation loss: 0.9993, Validation accuracy: 0.6689\n",
            "\n",
            "Epoch 41:\n",
            "Training loss: 0.6254, Training accuracy: 0.7842\n",
            "Validation loss: 0.8749, Validation accuracy: 0.7048\n",
            "\n",
            "Epoch 42:\n",
            "Training loss: 0.6255, Training accuracy: 0.7820\n",
            "Validation loss: 0.8747, Validation accuracy: 0.7001\n",
            "\n",
            "Epoch 43:\n",
            "Training loss: 0.6236, Training accuracy: 0.7842\n",
            "Validation loss: 0.8550, Validation accuracy: 0.7063\n",
            "\n",
            "Epoch 44:\n",
            "Training loss: 0.6219, Training accuracy: 0.7844\n",
            "Validation loss: 0.9487, Validation accuracy: 0.6802\n",
            "\n",
            "Epoch 45:\n",
            "Training loss: 0.6258, Training accuracy: 0.7836\n",
            "Validation loss: 0.9171, Validation accuracy: 0.6888\n",
            "\n",
            "Epoch 46:\n",
            "Training loss: 0.6232, Training accuracy: 0.7846\n",
            "Validation loss: 0.8098, Validation accuracy: 0.7218\n",
            "\n",
            "Epoch 47:\n",
            "Training loss: 0.6177, Training accuracy: 0.7850\n",
            "Validation loss: 0.8856, Validation accuracy: 0.6961\n",
            "\n",
            "Epoch 48:\n",
            "Training loss: 0.6153, Training accuracy: 0.7863\n",
            "Validation loss: 0.9662, Validation accuracy: 0.6776\n",
            "\n",
            "Epoch 49:\n",
            "Training loss: 0.6129, Training accuracy: 0.7876\n",
            "Validation loss: 0.9239, Validation accuracy: 0.6805\n",
            "\n",
            "Epoch 50:\n",
            "Training loss: 0.6140, Training accuracy: 0.7872\n",
            "Validation loss: 0.9788, Validation accuracy: 0.6738\n",
            "\n",
            "Epoch 51:\n",
            "Training loss: 0.6114, Training accuracy: 0.7893\n",
            "Validation loss: 0.9615, Validation accuracy: 0.6865\n",
            "\n",
            "Epoch 52:\n",
            "Training loss: 0.6148, Training accuracy: 0.7858\n",
            "Validation loss: 0.8345, Validation accuracy: 0.7199\n",
            "\n",
            "Epoch 53:\n",
            "Training loss: 0.6098, Training accuracy: 0.7881\n",
            "Validation loss: 0.8662, Validation accuracy: 0.7038\n",
            "\n",
            "Epoch 54:\n",
            "Training loss: 0.6047, Training accuracy: 0.7915\n",
            "Validation loss: 1.0410, Validation accuracy: 0.6520\n",
            "\n",
            "Epoch 55:\n",
            "Training loss: 0.6118, Training accuracy: 0.7856\n",
            "Validation loss: 0.8623, Validation accuracy: 0.7020\n",
            "\n",
            "Epoch 56:\n",
            "Training loss: 0.6055, Training accuracy: 0.7925\n",
            "Validation loss: 0.9255, Validation accuracy: 0.6855\n",
            "\n",
            "Epoch 57:\n",
            "Training loss: 0.6002, Training accuracy: 0.7929\n",
            "Validation loss: 0.8615, Validation accuracy: 0.7056\n",
            "\n",
            "Epoch 58:\n",
            "Training loss: 0.6000, Training accuracy: 0.7943\n",
            "Validation loss: 0.9975, Validation accuracy: 0.6748\n",
            "\n",
            "Epoch 59:\n",
            "Training loss: 0.6018, Training accuracy: 0.7909\n",
            "Validation loss: 0.9145, Validation accuracy: 0.6866\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 60:\n",
            "Training loss: 0.4841, Training accuracy: 0.8384\n",
            "Validation loss: 0.6679, Validation accuracy: 0.7717\n",
            "\n",
            "Epoch 61:\n",
            "Training loss: 0.4575, Training accuracy: 0.8501\n",
            "Validation loss: 0.6680, Validation accuracy: 0.7706\n",
            "\n",
            "Epoch 62:\n",
            "Training loss: 0.4484, Training accuracy: 0.8530\n",
            "Validation loss: 0.6669, Validation accuracy: 0.7754\n",
            "\n",
            "Epoch 63:\n",
            "Training loss: 0.4382, Training accuracy: 0.8563\n",
            "Validation loss: 0.6730, Validation accuracy: 0.7739\n",
            "\n",
            "Epoch 64:\n",
            "Training loss: 0.4343, Training accuracy: 0.8560\n",
            "Validation loss: 0.6694, Validation accuracy: 0.7767\n",
            "\n",
            "Epoch 65:\n",
            "Training loss: 0.4268, Training accuracy: 0.8587\n",
            "Validation loss: 0.6645, Validation accuracy: 0.7731\n",
            "\n",
            "Epoch 66:\n",
            "Training loss: 0.4231, Training accuracy: 0.8599\n",
            "Validation loss: 0.6592, Validation accuracy: 0.7790\n",
            "\n",
            "Epoch 67:\n",
            "Training loss: 0.4193, Training accuracy: 0.8612\n",
            "Validation loss: 0.6706, Validation accuracy: 0.7709\n",
            "\n",
            "Epoch 68:\n",
            "Training loss: 0.4161, Training accuracy: 0.8627\n",
            "Validation loss: 0.6648, Validation accuracy: 0.7754\n",
            "\n",
            "Epoch 69:\n",
            "Training loss: 0.4087, Training accuracy: 0.8652\n",
            "Validation loss: 0.6586, Validation accuracy: 0.7791\n",
            "\n",
            "Epoch 70:\n",
            "Training loss: 0.4085, Training accuracy: 0.8649\n",
            "Validation loss: 0.6729, Validation accuracy: 0.7756\n",
            "\n",
            "Epoch 71:\n",
            "Training loss: 0.4054, Training accuracy: 0.8647\n",
            "Validation loss: 0.6746, Validation accuracy: 0.7742\n",
            "\n",
            "Epoch 72:\n",
            "Training loss: 0.4091, Training accuracy: 0.8621\n",
            "Validation loss: 0.6741, Validation accuracy: 0.7763\n",
            "\n",
            "Epoch 73:\n",
            "Training loss: 0.4009, Training accuracy: 0.8671\n",
            "Validation loss: 0.6670, Validation accuracy: 0.7817\n",
            "\n",
            "Epoch 74:\n",
            "Training loss: 0.4020, Training accuracy: 0.8651\n",
            "Validation loss: 0.6759, Validation accuracy: 0.7744\n",
            "\n",
            "Epoch 75:\n",
            "Training loss: 0.3974, Training accuracy: 0.8666\n",
            "Validation loss: 0.6724, Validation accuracy: 0.7730\n",
            "\n",
            "Epoch 76:\n",
            "Training loss: 0.3935, Training accuracy: 0.8691\n",
            "Validation loss: 0.6979, Validation accuracy: 0.7689\n",
            "\n",
            "Epoch 77:\n",
            "Training loss: 0.3936, Training accuracy: 0.8687\n",
            "Validation loss: 0.6934, Validation accuracy: 0.7710\n",
            "\n",
            "Epoch 78:\n",
            "Training loss: 0.3961, Training accuracy: 0.8686\n",
            "Validation loss: 0.6733, Validation accuracy: 0.7750\n",
            "\n",
            "Epoch 79:\n",
            "Training loss: 0.3908, Training accuracy: 0.8707\n",
            "Validation loss: 0.7024, Validation accuracy: 0.7713\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 80:\n",
            "Training loss: 0.3516, Training accuracy: 0.8846\n",
            "Validation loss: 0.6312, Validation accuracy: 0.7912\n",
            "\n",
            "Epoch 81:\n",
            "Training loss: 0.3426, Training accuracy: 0.8895\n",
            "Validation loss: 0.6386, Validation accuracy: 0.7880\n",
            "\n",
            "Epoch 82:\n",
            "Training loss: 0.3367, Training accuracy: 0.8927\n",
            "Validation loss: 0.6385, Validation accuracy: 0.7898\n",
            "\n",
            "Epoch 83:\n",
            "Training loss: 0.3338, Training accuracy: 0.8945\n",
            "Validation loss: 0.6322, Validation accuracy: 0.7905\n",
            "\n",
            "Epoch 84:\n",
            "Training loss: 0.3334, Training accuracy: 0.8947\n",
            "Validation loss: 0.6434, Validation accuracy: 0.7866\n",
            "\n",
            "Epoch 85:\n",
            "Training loss: 0.3308, Training accuracy: 0.8937\n",
            "Validation loss: 0.6363, Validation accuracy: 0.7842\n",
            "\n",
            "Epoch 86:\n",
            "Training loss: 0.3296, Training accuracy: 0.8957\n",
            "Validation loss: 0.6359, Validation accuracy: 0.7909\n",
            "\n",
            "Epoch 87:\n",
            "Training loss: 0.3270, Training accuracy: 0.8974\n",
            "Validation loss: 0.6311, Validation accuracy: 0.7848\n",
            "\n",
            "Epoch 88:\n",
            "Training loss: 0.3266, Training accuracy: 0.8965\n",
            "Validation loss: 0.6347, Validation accuracy: 0.7856\n",
            "\n",
            "Epoch 89:\n",
            "Training loss: 0.3257, Training accuracy: 0.8967\n",
            "Validation loss: 0.6267, Validation accuracy: 0.7887\n",
            "\n",
            "Epoch 90:\n",
            "Training loss: 0.3276, Training accuracy: 0.8951\n",
            "Validation loss: 0.6398, Validation accuracy: 0.7919\n",
            "\n",
            "Epoch 91:\n",
            "Training loss: 0.3233, Training accuracy: 0.8974\n",
            "Validation loss: 0.6408, Validation accuracy: 0.7833\n",
            "\n",
            "Epoch 92:\n",
            "Training loss: 0.3237, Training accuracy: 0.8984\n",
            "Validation loss: 0.6337, Validation accuracy: 0.7901\n",
            "\n",
            "Epoch 93:\n",
            "Training loss: 0.3233, Training accuracy: 0.8981\n",
            "Validation loss: 0.6419, Validation accuracy: 0.7844\n",
            "\n",
            "Epoch 94:\n",
            "Training loss: 0.3211, Training accuracy: 0.8998\n",
            "Validation loss: 0.6359, Validation accuracy: 0.7878\n",
            "\n",
            "Epoch 95:\n",
            "Training loss: 0.3201, Training accuracy: 0.9001\n",
            "Validation loss: 0.6379, Validation accuracy: 0.7888\n",
            "\n",
            "Epoch 96:\n",
            "Training loss: 0.3170, Training accuracy: 0.8996\n",
            "Validation loss: 0.6392, Validation accuracy: 0.7860\n",
            "\n",
            "Epoch 97:\n",
            "Training loss: 0.3178, Training accuracy: 0.8998\n",
            "Validation loss: 0.6407, Validation accuracy: 0.7869\n",
            "\n",
            "Epoch 98:\n",
            "Training loss: 0.3152, Training accuracy: 0.8997\n",
            "Validation loss: 0.6477, Validation accuracy: 0.7841\n",
            "\n",
            "Epoch 99:\n",
            "Training loss: 0.3153, Training accuracy: 0.8998\n",
            "Validation loss: 0.6496, Validation accuracy: 0.7825\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.7919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fc-layer + 3rd block feature map random init\n",
        "NIN_net_4block.to(device)\n",
        "out_feat_keys = ['conv4']\n",
        "classifier = Classifier({'num_classes': 10, 'nChannels': 192, 'cls_type': 'NIN_ConvBlock3'}).to(device)\n",
        "#hyperparameter setting\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "EPOCHS = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(classifier.parameters(), lr = INITIAL_LR,\n",
        "                      momentum = MOMENTUM,\n",
        "                      weight_decay=REG)"
      ],
      "metadata": {
        "id": "pBhJrDtJtfmn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the folder where the trained model is saved\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "# start the training/validation process\n",
        "# the process should take about 5 minutes on a GTX 1070-Ti\n",
        "# if the code is written efficiently.\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY = 0.2\n",
        "valid_acc_block4 = []\n",
        "losslist = []\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    if i  == 30 or i == 60 or i == 80:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    classifier.train()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    # this help you compute the training accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0 # track training loss if you want\n",
        "\n",
        "    # Train the model for 1 epoch.\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        ####################################\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # inputs = inputs.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "        feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "        targets = targets.to(device)\n",
        "        NIN_net_4block = NIN_net_4block.to(device)\n",
        "        # compute the output and loss\n",
        "        outputs = classifier(feature_map)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss\n",
        "        # zero the gradient\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        # apply gradient and update the weights\n",
        "        optimizer.step()\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "        _, predicted = outputs.max(1)  #make prediction based on the highest value\n",
        "        total_examples += targets.size(0) # in this case,128 for each batch\n",
        "        correct_examples += predicted.eq(targets).sum().item()\n",
        "        ####################################\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "\n",
        "    classifier.eval()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    # this help you compute the validation accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 # again, track the validation loss if you want\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            ####################################\n",
        "            # your code here\n",
        "            # copy inputs to device\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "            NIN_net_4block = NIN_net_4block.to(device)\n",
        "\n",
        "\n",
        "            # compute the output and loss\n",
        "\n",
        "            outputs = classifier(feature_map)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # count the number of correctly predicted samples in the current batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += predicted.eq(targets).sum().item()\n",
        "            ####################################\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    valid_acc.append(avg_acc)\n",
        "    losslist.append(avg_loss.item())\n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        #if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "        #    os.makedirs(CHECKPOINT_FOLDER)\n",
        "        #print(\"Saving ...\")\n",
        "\n",
        "        torch.save(classifier.state_dict(), 'classifier_conv3_NIN_block4.pth')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrxqsVGft7wS",
        "outputId": "f11d50ee-131b-4663-e11f-8ef1fbb8b200"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 1.6165, Training accuracy: 0.4141\n",
            "Validation loss: 1.6221, Validation accuracy: 0.4140\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 1.5593, Training accuracy: 0.4344\n",
            "Validation loss: 1.5896, Validation accuracy: 0.4279\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 1.5311, Training accuracy: 0.4461\n",
            "Validation loss: 1.5531, Validation accuracy: 0.4328\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 1.5064, Training accuracy: 0.4546\n",
            "Validation loss: 1.5506, Validation accuracy: 0.4331\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 1.4940, Training accuracy: 0.4606\n",
            "Validation loss: 1.5376, Validation accuracy: 0.4496\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 1.4779, Training accuracy: 0.4655\n",
            "Validation loss: 1.5103, Validation accuracy: 0.4618\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 1.4748, Training accuracy: 0.4708\n",
            "Validation loss: 1.5053, Validation accuracy: 0.4567\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 1.4653, Training accuracy: 0.4718\n",
            "Validation loss: 1.5250, Validation accuracy: 0.4534\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 1.4657, Training accuracy: 0.4725\n",
            "Validation loss: 1.5331, Validation accuracy: 0.4398\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 1.4562, Training accuracy: 0.4754\n",
            "Validation loss: 1.5331, Validation accuracy: 0.4504\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 1.4474, Training accuracy: 0.4789\n",
            "Validation loss: 1.5302, Validation accuracy: 0.4554\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 1.4426, Training accuracy: 0.4803\n",
            "Validation loss: 1.5524, Validation accuracy: 0.4464\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 1.4361, Training accuracy: 0.4842\n",
            "Validation loss: 1.4921, Validation accuracy: 0.4697\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 1.4375, Training accuracy: 0.4820\n",
            "Validation loss: 1.4829, Validation accuracy: 0.4567\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 1.4306, Training accuracy: 0.4861\n",
            "Validation loss: 1.5748, Validation accuracy: 0.4455\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 1.4303, Training accuracy: 0.4857\n",
            "Validation loss: 1.4843, Validation accuracy: 0.4721\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 1.4311, Training accuracy: 0.4857\n",
            "Validation loss: 1.5714, Validation accuracy: 0.4471\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 1.4265, Training accuracy: 0.4871\n",
            "Validation loss: 1.4789, Validation accuracy: 0.4756\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 1.4261, Training accuracy: 0.4857\n",
            "Validation loss: 1.5068, Validation accuracy: 0.4612\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 1.4247, Training accuracy: 0.4873\n",
            "Validation loss: 1.5019, Validation accuracy: 0.4600\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 1.4250, Training accuracy: 0.4889\n",
            "Validation loss: 1.5819, Validation accuracy: 0.4423\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 1.4136, Training accuracy: 0.4939\n",
            "Validation loss: 1.5411, Validation accuracy: 0.4553\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 1.4188, Training accuracy: 0.4911\n",
            "Validation loss: 1.4739, Validation accuracy: 0.4698\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 1.4214, Training accuracy: 0.4883\n",
            "Validation loss: 1.4805, Validation accuracy: 0.4680\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 1.4153, Training accuracy: 0.4927\n",
            "Validation loss: 1.5053, Validation accuracy: 0.4511\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 1.4147, Training accuracy: 0.4925\n",
            "Validation loss: 1.6893, Validation accuracy: 0.4105\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 1.4106, Training accuracy: 0.4896\n",
            "Validation loss: 1.5741, Validation accuracy: 0.4447\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 1.4076, Training accuracy: 0.4948\n",
            "Validation loss: 1.4664, Validation accuracy: 0.4760\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 1.4117, Training accuracy: 0.4906\n",
            "Validation loss: 1.5275, Validation accuracy: 0.4532\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 1.4206, Training accuracy: 0.4883\n",
            "Validation loss: 1.4783, Validation accuracy: 0.4733\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 30:\n",
            "Training loss: 1.3124, Training accuracy: 0.5317\n",
            "Validation loss: 1.3482, Validation accuracy: 0.5195\n",
            "\n",
            "Epoch 31:\n",
            "Training loss: 1.2836, Training accuracy: 0.5418\n",
            "Validation loss: 1.3364, Validation accuracy: 0.5233\n",
            "\n",
            "Epoch 32:\n",
            "Training loss: 1.2762, Training accuracy: 0.5465\n",
            "Validation loss: 1.3327, Validation accuracy: 0.5289\n",
            "\n",
            "Epoch 33:\n",
            "Training loss: 1.2663, Training accuracy: 0.5489\n",
            "Validation loss: 1.3299, Validation accuracy: 0.5257\n",
            "\n",
            "Epoch 34:\n",
            "Training loss: 1.2638, Training accuracy: 0.5499\n",
            "Validation loss: 1.3304, Validation accuracy: 0.5234\n",
            "\n",
            "Epoch 35:\n",
            "Training loss: 1.2541, Training accuracy: 0.5521\n",
            "Validation loss: 1.3316, Validation accuracy: 0.5216\n",
            "\n",
            "Epoch 36:\n",
            "Training loss: 1.2541, Training accuracy: 0.5521\n",
            "Validation loss: 1.3479, Validation accuracy: 0.5236\n",
            "\n",
            "Epoch 37:\n",
            "Training loss: 1.2582, Training accuracy: 0.5508\n",
            "Validation loss: 1.3460, Validation accuracy: 0.5158\n",
            "\n",
            "Epoch 38:\n",
            "Training loss: 1.2550, Training accuracy: 0.5525\n",
            "Validation loss: 1.3291, Validation accuracy: 0.5275\n",
            "\n",
            "Epoch 39:\n",
            "Training loss: 1.2469, Training accuracy: 0.5540\n",
            "Validation loss: 1.3331, Validation accuracy: 0.5269\n",
            "\n",
            "Epoch 40:\n",
            "Training loss: 1.2525, Training accuracy: 0.5541\n",
            "Validation loss: 1.3381, Validation accuracy: 0.5303\n",
            "\n",
            "Epoch 41:\n",
            "Training loss: 1.2536, Training accuracy: 0.5521\n",
            "Validation loss: 1.3761, Validation accuracy: 0.5153\n",
            "\n",
            "Epoch 42:\n",
            "Training loss: 1.2476, Training accuracy: 0.5562\n",
            "Validation loss: 1.3515, Validation accuracy: 0.5213\n",
            "\n",
            "Epoch 43:\n",
            "Training loss: 1.2473, Training accuracy: 0.5554\n",
            "Validation loss: 1.3406, Validation accuracy: 0.5257\n",
            "\n",
            "Epoch 44:\n",
            "Training loss: 1.2457, Training accuracy: 0.5562\n",
            "Validation loss: 1.3397, Validation accuracy: 0.5257\n",
            "\n",
            "Epoch 45:\n",
            "Training loss: 1.2532, Training accuracy: 0.5539\n",
            "Validation loss: 1.3554, Validation accuracy: 0.5183\n",
            "\n",
            "Epoch 46:\n",
            "Training loss: 1.2459, Training accuracy: 0.5555\n",
            "Validation loss: 1.3280, Validation accuracy: 0.5289\n",
            "\n",
            "Epoch 47:\n",
            "Training loss: 1.2392, Training accuracy: 0.5576\n",
            "Validation loss: 1.3821, Validation accuracy: 0.5052\n",
            "\n",
            "Epoch 48:\n",
            "Training loss: 1.2409, Training accuracy: 0.5579\n",
            "Validation loss: 1.3606, Validation accuracy: 0.5161\n",
            "\n",
            "Epoch 49:\n",
            "Training loss: 1.2415, Training accuracy: 0.5580\n",
            "Validation loss: 1.3291, Validation accuracy: 0.5291\n",
            "\n",
            "Epoch 50:\n",
            "Training loss: 1.2438, Training accuracy: 0.5561\n",
            "Validation loss: 1.3563, Validation accuracy: 0.5218\n",
            "\n",
            "Epoch 51:\n",
            "Training loss: 1.2433, Training accuracy: 0.5603\n",
            "Validation loss: 1.3376, Validation accuracy: 0.5266\n",
            "\n",
            "Epoch 52:\n",
            "Training loss: 1.2385, Training accuracy: 0.5584\n",
            "Validation loss: 1.3262, Validation accuracy: 0.5329\n",
            "\n",
            "Epoch 53:\n",
            "Training loss: 1.2337, Training accuracy: 0.5569\n",
            "Validation loss: 1.3580, Validation accuracy: 0.5182\n",
            "\n",
            "Epoch 54:\n",
            "Training loss: 1.2346, Training accuracy: 0.5608\n",
            "Validation loss: 1.3611, Validation accuracy: 0.5103\n",
            "\n",
            "Epoch 55:\n",
            "Training loss: 1.2397, Training accuracy: 0.5595\n",
            "Validation loss: 1.3303, Validation accuracy: 0.5272\n",
            "\n",
            "Epoch 56:\n",
            "Training loss: 1.2363, Training accuracy: 0.5602\n",
            "Validation loss: 1.3422, Validation accuracy: 0.5279\n",
            "\n",
            "Epoch 57:\n",
            "Training loss: 1.2380, Training accuracy: 0.5596\n",
            "Validation loss: 1.3566, Validation accuracy: 0.5159\n",
            "\n",
            "Epoch 58:\n",
            "Training loss: 1.2286, Training accuracy: 0.5639\n",
            "Validation loss: 1.3277, Validation accuracy: 0.5256\n",
            "\n",
            "Epoch 59:\n",
            "Training loss: 1.2348, Training accuracy: 0.5600\n",
            "Validation loss: 1.3177, Validation accuracy: 0.5329\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 60:\n",
            "Training loss: 1.1670, Training accuracy: 0.5842\n",
            "Validation loss: 1.2671, Validation accuracy: 0.5520\n",
            "\n",
            "Epoch 61:\n",
            "Training loss: 1.1489, Training accuracy: 0.5907\n",
            "Validation loss: 1.2558, Validation accuracy: 0.5546\n",
            "\n",
            "Epoch 62:\n",
            "Training loss: 1.1425, Training accuracy: 0.5931\n",
            "Validation loss: 1.2602, Validation accuracy: 0.5560\n",
            "\n",
            "Epoch 63:\n",
            "Training loss: 1.1423, Training accuracy: 0.5964\n",
            "Validation loss: 1.2616, Validation accuracy: 0.5558\n",
            "\n",
            "Epoch 64:\n",
            "Training loss: 1.1369, Training accuracy: 0.5972\n",
            "Validation loss: 1.2508, Validation accuracy: 0.5576\n",
            "\n",
            "Epoch 65:\n",
            "Training loss: 1.1339, Training accuracy: 0.5992\n",
            "Validation loss: 1.2497, Validation accuracy: 0.5553\n",
            "\n",
            "Epoch 66:\n",
            "Training loss: 1.1291, Training accuracy: 0.5969\n",
            "Validation loss: 1.2509, Validation accuracy: 0.5596\n",
            "\n",
            "Epoch 67:\n",
            "Training loss: 1.1237, Training accuracy: 0.6011\n",
            "Validation loss: 1.2616, Validation accuracy: 0.5523\n",
            "\n",
            "Epoch 68:\n",
            "Training loss: 1.1204, Training accuracy: 0.6027\n",
            "Validation loss: 1.2499, Validation accuracy: 0.5594\n",
            "\n",
            "Epoch 69:\n",
            "Training loss: 1.1277, Training accuracy: 0.5995\n",
            "Validation loss: 1.2513, Validation accuracy: 0.5564\n",
            "\n",
            "Epoch 70:\n",
            "Training loss: 1.1170, Training accuracy: 0.6026\n",
            "Validation loss: 1.2494, Validation accuracy: 0.5612\n",
            "\n",
            "Epoch 71:\n",
            "Training loss: 1.1144, Training accuracy: 0.6061\n",
            "Validation loss: 1.2554, Validation accuracy: 0.5507\n",
            "\n",
            "Epoch 72:\n",
            "Training loss: 1.1137, Training accuracy: 0.6036\n",
            "Validation loss: 1.2525, Validation accuracy: 0.5557\n",
            "\n",
            "Epoch 73:\n",
            "Training loss: 1.1119, Training accuracy: 0.6064\n",
            "Validation loss: 1.2483, Validation accuracy: 0.5613\n",
            "\n",
            "Epoch 74:\n",
            "Training loss: 1.1154, Training accuracy: 0.6039\n",
            "Validation loss: 1.2611, Validation accuracy: 0.5526\n",
            "\n",
            "Epoch 75:\n",
            "Training loss: 1.1089, Training accuracy: 0.6070\n",
            "Validation loss: 1.2547, Validation accuracy: 0.5519\n",
            "\n",
            "Epoch 76:\n",
            "Training loss: 1.1059, Training accuracy: 0.6063\n",
            "Validation loss: 1.2495, Validation accuracy: 0.5567\n",
            "\n",
            "Epoch 77:\n",
            "Training loss: 1.1087, Training accuracy: 0.6056\n",
            "Validation loss: 1.2514, Validation accuracy: 0.5554\n",
            "\n",
            "Epoch 78:\n",
            "Training loss: 1.1012, Training accuracy: 0.6108\n",
            "Validation loss: 1.2639, Validation accuracy: 0.5512\n",
            "\n",
            "Epoch 79:\n",
            "Training loss: 1.1115, Training accuracy: 0.6052\n",
            "Validation loss: 1.2737, Validation accuracy: 0.5526\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 80:\n",
            "Training loss: 1.0877, Training accuracy: 0.6141\n",
            "Validation loss: 1.2396, Validation accuracy: 0.5567\n",
            "\n",
            "Epoch 81:\n",
            "Training loss: 1.0779, Training accuracy: 0.6182\n",
            "Validation loss: 1.2379, Validation accuracy: 0.5673\n",
            "\n",
            "Epoch 82:\n",
            "Training loss: 1.0770, Training accuracy: 0.6193\n",
            "Validation loss: 1.2310, Validation accuracy: 0.5682\n",
            "\n",
            "Epoch 83:\n",
            "Training loss: 1.0707, Training accuracy: 0.6207\n",
            "Validation loss: 1.2358, Validation accuracy: 0.5639\n",
            "\n",
            "Epoch 84:\n",
            "Training loss: 1.0715, Training accuracy: 0.6197\n",
            "Validation loss: 1.2298, Validation accuracy: 0.5674\n",
            "\n",
            "Epoch 85:\n",
            "Training loss: 1.0764, Training accuracy: 0.6180\n",
            "Validation loss: 1.2391, Validation accuracy: 0.5587\n",
            "\n",
            "Epoch 86:\n",
            "Training loss: 1.0678, Training accuracy: 0.6220\n",
            "Validation loss: 1.2437, Validation accuracy: 0.5653\n",
            "\n",
            "Epoch 87:\n",
            "Training loss: 1.0691, Training accuracy: 0.6233\n",
            "Validation loss: 1.2260, Validation accuracy: 0.5631\n",
            "\n",
            "Epoch 88:\n",
            "Training loss: 1.0671, Training accuracy: 0.6251\n",
            "Validation loss: 1.2350, Validation accuracy: 0.5642\n",
            "\n",
            "Epoch 89:\n",
            "Training loss: 1.0644, Training accuracy: 0.6250\n",
            "Validation loss: 1.2429, Validation accuracy: 0.5604\n",
            "\n",
            "Epoch 90:\n",
            "Training loss: 1.0662, Training accuracy: 0.6225\n",
            "Validation loss: 1.2400, Validation accuracy: 0.5623\n",
            "\n",
            "Epoch 91:\n",
            "Training loss: 1.0660, Training accuracy: 0.6235\n",
            "Validation loss: 1.2336, Validation accuracy: 0.5667\n",
            "\n",
            "Epoch 92:\n",
            "Training loss: 1.0617, Training accuracy: 0.6251\n",
            "Validation loss: 1.2360, Validation accuracy: 0.5649\n",
            "\n",
            "Epoch 93:\n",
            "Training loss: 1.0649, Training accuracy: 0.6235\n",
            "Validation loss: 1.2339, Validation accuracy: 0.5642\n",
            "\n",
            "Epoch 94:\n",
            "Training loss: 1.0569, Training accuracy: 0.6247\n",
            "Validation loss: 1.2214, Validation accuracy: 0.5711\n",
            "\n",
            "Epoch 95:\n",
            "Training loss: 1.0579, Training accuracy: 0.6265\n",
            "Validation loss: 1.2459, Validation accuracy: 0.5631\n",
            "\n",
            "Epoch 96:\n",
            "Training loss: 1.0596, Training accuracy: 0.6260\n",
            "Validation loss: 1.2215, Validation accuracy: 0.5682\n",
            "\n",
            "Epoch 97:\n",
            "Training loss: 1.0594, Training accuracy: 0.6252\n",
            "Validation loss: 1.2203, Validation accuracy: 0.5660\n",
            "\n",
            "Epoch 98:\n",
            "Training loss: 1.0511, Training accuracy: 0.6252\n",
            "Validation loss: 1.2269, Validation accuracy: 0.5688\n",
            "\n",
            "Epoch 99:\n",
            "Training loss: 1.0566, Training accuracy: 0.6259\n",
            "Validation loss: 1.2330, Validation accuracy: 0.5627\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.5711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fc-layer + 3rd block feature map\n",
        "NIN_net_4block.to(device)\n",
        "out_feat_keys = ['conv3']\n",
        "classifier = Classifier({'num_classes': 10, 'nChannels': 192, 'cls_type': 'NIN_ConvBlock3'}).to(device)\n",
        "#hyperparameter setting\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "EPOCHS = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(classifier.parameters(), lr = INITIAL_LR,\n",
        "                      momentum = MOMENTUM,\n",
        "                      weight_decay=REG)"
      ],
      "metadata": {
        "id": "7M78KpPcyeGU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the folder where the trained model is saved\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "# start the training/validation process\n",
        "# the process should take about 5 minutes on a GTX 1070-Ti\n",
        "# if the code is written efficiently.\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY = 0.2\n",
        "valid_acc_block4 = []\n",
        "losslist = []\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    if i  == 30 or i == 60 or i == 80:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    classifier.train()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    # this help you compute the training accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0 # track training loss if you want\n",
        "\n",
        "    # Train the model for 1 epoch.\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        ####################################\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # inputs = inputs.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "        feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "        targets = targets.to(device)\n",
        "        NIN_net_4block = NIN_net_4block.to(device)\n",
        "        # compute the output and loss\n",
        "        outputs = classifier(feature_map)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss\n",
        "        # zero the gradient\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        # apply gradient and update the weights\n",
        "        optimizer.step()\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "        _, predicted = outputs.max(1)  #make prediction based on the highest value\n",
        "        total_examples += targets.size(0) # in this case,128 for each batch\n",
        "        correct_examples += predicted.eq(targets).sum().item()\n",
        "        ####################################\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "\n",
        "    classifier.eval()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    # this help you compute the validation accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 # again, track the validation loss if you want\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            ####################################\n",
        "            # your code here\n",
        "            # copy inputs to device\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "            NIN_net_4block = NIN_net_4block.to(device)\n",
        "\n",
        "\n",
        "            # compute the output and loss\n",
        "\n",
        "            outputs = classifier(feature_map)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # count the number of correctly predicted samples in the current batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += predicted.eq(targets).sum().item()\n",
        "            ####################################\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    valid_acc_block3.append(avg_acc)\n",
        "    losslist.append(avg_loss.item())\n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        #if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "        #    os.makedirs(CHECKPOINT_FOLDER)\n",
        "        #print(\"Saving ...\")\n",
        "\n",
        "        torch.save(classifier.state_dict(), 'classifier_conv3_NIN_block3.pth')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkfcQwyUye3p",
        "outputId": "88a12fdd-1900-48df-c1dc-7b0fc6897e1a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 1.6941, Training accuracy: 0.3983\n",
            "Validation loss: 1.5268, Validation accuracy: 0.4509\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 1.4466, Training accuracy: 0.4770\n",
            "Validation loss: 1.5224, Validation accuracy: 0.4500\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 1.3810, Training accuracy: 0.5027\n",
            "Validation loss: 1.4823, Validation accuracy: 0.4624\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 1.3361, Training accuracy: 0.5207\n",
            "Validation loss: 1.5306, Validation accuracy: 0.4663\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 1.3011, Training accuracy: 0.5325\n",
            "Validation loss: 1.3750, Validation accuracy: 0.5113\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 1.2901, Training accuracy: 0.5376\n",
            "Validation loss: 1.3466, Validation accuracy: 0.5138\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 1.2706, Training accuracy: 0.5463\n",
            "Validation loss: 1.3835, Validation accuracy: 0.5116\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 1.2513, Training accuracy: 0.5522\n",
            "Validation loss: 1.3216, Validation accuracy: 0.5251\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 1.2451, Training accuracy: 0.5553\n",
            "Validation loss: 1.3024, Validation accuracy: 0.5338\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 1.2292, Training accuracy: 0.5627\n",
            "Validation loss: 1.3136, Validation accuracy: 0.5249\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 1.2228, Training accuracy: 0.5657\n",
            "Validation loss: 1.3532, Validation accuracy: 0.5237\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 1.2159, Training accuracy: 0.5664\n",
            "Validation loss: 1.2796, Validation accuracy: 0.5457\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 1.2114, Training accuracy: 0.5671\n",
            "Validation loss: 1.3261, Validation accuracy: 0.5238\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 1.2014, Training accuracy: 0.5740\n",
            "Validation loss: 1.3028, Validation accuracy: 0.5402\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 1.1987, Training accuracy: 0.5755\n",
            "Validation loss: 1.3437, Validation accuracy: 0.5225\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 1.1944, Training accuracy: 0.5755\n",
            "Validation loss: 1.2913, Validation accuracy: 0.5366\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 1.1928, Training accuracy: 0.5757\n",
            "Validation loss: 1.3680, Validation accuracy: 0.5324\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 1.1852, Training accuracy: 0.5805\n",
            "Validation loss: 1.3255, Validation accuracy: 0.5409\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 1.1878, Training accuracy: 0.5785\n",
            "Validation loss: 1.3265, Validation accuracy: 0.5258\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 1.1862, Training accuracy: 0.5788\n",
            "Validation loss: 1.3019, Validation accuracy: 0.5503\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 1.1787, Training accuracy: 0.5822\n",
            "Validation loss: 1.3524, Validation accuracy: 0.5253\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 1.1773, Training accuracy: 0.5809\n",
            "Validation loss: 1.2710, Validation accuracy: 0.5524\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 1.1749, Training accuracy: 0.5828\n",
            "Validation loss: 1.2632, Validation accuracy: 0.5551\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 1.1768, Training accuracy: 0.5808\n",
            "Validation loss: 1.3290, Validation accuracy: 0.5357\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 1.1719, Training accuracy: 0.5832\n",
            "Validation loss: 1.2655, Validation accuracy: 0.5549\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 1.1737, Training accuracy: 0.5832\n",
            "Validation loss: 1.2699, Validation accuracy: 0.5484\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 1.1700, Training accuracy: 0.5843\n",
            "Validation loss: 1.3349, Validation accuracy: 0.5243\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 1.1754, Training accuracy: 0.5825\n",
            "Validation loss: 1.3439, Validation accuracy: 0.5229\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 1.1643, Training accuracy: 0.5852\n",
            "Validation loss: 1.2912, Validation accuracy: 0.5420\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 1.1643, Training accuracy: 0.5876\n",
            "Validation loss: 1.2933, Validation accuracy: 0.5445\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 30:\n",
            "Training loss: 1.0288, Training accuracy: 0.6386\n",
            "Validation loss: 1.1037, Validation accuracy: 0.6111\n",
            "\n",
            "Epoch 31:\n",
            "Training loss: 0.9875, Training accuracy: 0.6526\n",
            "Validation loss: 1.1262, Validation accuracy: 0.6073\n",
            "\n",
            "Epoch 32:\n",
            "Training loss: 0.9734, Training accuracy: 0.6567\n",
            "Validation loss: 1.1008, Validation accuracy: 0.6106\n",
            "\n",
            "Epoch 33:\n",
            "Training loss: 0.9598, Training accuracy: 0.6630\n",
            "Validation loss: 1.1207, Validation accuracy: 0.6041\n",
            "\n",
            "Epoch 34:\n",
            "Training loss: 0.9534, Training accuracy: 0.6639\n",
            "Validation loss: 1.1286, Validation accuracy: 0.6063\n",
            "\n",
            "Epoch 35:\n",
            "Training loss: 0.9554, Training accuracy: 0.6619\n",
            "Validation loss: 1.0999, Validation accuracy: 0.6171\n",
            "\n",
            "Epoch 36:\n",
            "Training loss: 0.9500, Training accuracy: 0.6648\n",
            "Validation loss: 1.1036, Validation accuracy: 0.6100\n",
            "\n",
            "Epoch 37:\n",
            "Training loss: 0.9426, Training accuracy: 0.6693\n",
            "Validation loss: 1.1381, Validation accuracy: 0.6007\n",
            "\n",
            "Epoch 38:\n",
            "Training loss: 0.9415, Training accuracy: 0.6671\n",
            "Validation loss: 1.1195, Validation accuracy: 0.6066\n",
            "\n",
            "Epoch 39:\n",
            "Training loss: 0.9452, Training accuracy: 0.6679\n",
            "Validation loss: 1.1375, Validation accuracy: 0.6070\n",
            "\n",
            "Epoch 40:\n",
            "Training loss: 0.9377, Training accuracy: 0.6683\n",
            "Validation loss: 1.1314, Validation accuracy: 0.6038\n",
            "\n",
            "Epoch 41:\n",
            "Training loss: 0.9372, Training accuracy: 0.6690\n",
            "Validation loss: 1.1108, Validation accuracy: 0.6117\n",
            "\n",
            "Epoch 42:\n",
            "Training loss: 0.9361, Training accuracy: 0.6696\n",
            "Validation loss: 1.1852, Validation accuracy: 0.5923\n",
            "\n",
            "Epoch 43:\n",
            "Training loss: 0.9385, Training accuracy: 0.6670\n",
            "Validation loss: 1.1097, Validation accuracy: 0.6151\n",
            "\n",
            "Epoch 44:\n",
            "Training loss: 0.9374, Training accuracy: 0.6680\n",
            "Validation loss: 1.1372, Validation accuracy: 0.6061\n",
            "\n",
            "Epoch 45:\n",
            "Training loss: 0.9326, Training accuracy: 0.6722\n",
            "Validation loss: 1.1376, Validation accuracy: 0.6074\n",
            "\n",
            "Epoch 46:\n",
            "Training loss: 0.9314, Training accuracy: 0.6716\n",
            "Validation loss: 1.1253, Validation accuracy: 0.6105\n",
            "\n",
            "Epoch 47:\n",
            "Training loss: 0.9327, Training accuracy: 0.6719\n",
            "Validation loss: 1.1442, Validation accuracy: 0.6026\n",
            "\n",
            "Epoch 48:\n",
            "Training loss: 0.9243, Training accuracy: 0.6774\n",
            "Validation loss: 1.1124, Validation accuracy: 0.6177\n",
            "\n",
            "Epoch 49:\n",
            "Training loss: 0.9281, Training accuracy: 0.6728\n",
            "Validation loss: 1.1305, Validation accuracy: 0.6088\n",
            "\n",
            "Epoch 50:\n",
            "Training loss: 0.9288, Training accuracy: 0.6711\n",
            "Validation loss: 1.1378, Validation accuracy: 0.6069\n",
            "\n",
            "Epoch 51:\n",
            "Training loss: 0.9185, Training accuracy: 0.6752\n",
            "Validation loss: 1.1649, Validation accuracy: 0.6031\n",
            "\n",
            "Epoch 52:\n",
            "Training loss: 0.9203, Training accuracy: 0.6751\n",
            "Validation loss: 1.1149, Validation accuracy: 0.6127\n",
            "\n",
            "Epoch 53:\n",
            "Training loss: 0.9253, Training accuracy: 0.6752\n",
            "Validation loss: 1.1605, Validation accuracy: 0.5936\n",
            "\n",
            "Epoch 54:\n",
            "Training loss: 0.9188, Training accuracy: 0.6769\n",
            "Validation loss: 1.1225, Validation accuracy: 0.6063\n",
            "\n",
            "Epoch 55:\n",
            "Training loss: 0.9163, Training accuracy: 0.6771\n",
            "Validation loss: 1.1249, Validation accuracy: 0.6103\n",
            "\n",
            "Epoch 56:\n",
            "Training loss: 0.9111, Training accuracy: 0.6783\n",
            "Validation loss: 1.1286, Validation accuracy: 0.6044\n",
            "\n",
            "Epoch 57:\n",
            "Training loss: 0.9156, Training accuracy: 0.6793\n",
            "Validation loss: 1.1590, Validation accuracy: 0.6032\n",
            "\n",
            "Epoch 58:\n",
            "Training loss: 0.9151, Training accuracy: 0.6784\n",
            "Validation loss: 1.1177, Validation accuracy: 0.6075\n",
            "\n",
            "Epoch 59:\n",
            "Training loss: 0.9114, Training accuracy: 0.6800\n",
            "Validation loss: 1.1203, Validation accuracy: 0.6054\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 60:\n",
            "Training loss: 0.8245, Training accuracy: 0.7136\n",
            "Validation loss: 1.0354, Validation accuracy: 0.6410\n",
            "\n",
            "Epoch 61:\n",
            "Training loss: 0.7954, Training accuracy: 0.7216\n",
            "Validation loss: 1.0397, Validation accuracy: 0.6454\n",
            "\n",
            "Epoch 62:\n",
            "Training loss: 0.7852, Training accuracy: 0.7259\n",
            "Validation loss: 1.0368, Validation accuracy: 0.6418\n",
            "\n",
            "Epoch 63:\n",
            "Training loss: 0.7752, Training accuracy: 0.7295\n",
            "Validation loss: 1.0400, Validation accuracy: 0.6357\n",
            "\n",
            "Epoch 64:\n",
            "Training loss: 0.7691, Training accuracy: 0.7321\n",
            "Validation loss: 1.0269, Validation accuracy: 0.6424\n",
            "\n",
            "Epoch 65:\n",
            "Training loss: 0.7657, Training accuracy: 0.7321\n",
            "Validation loss: 1.0473, Validation accuracy: 0.6429\n",
            "\n",
            "Epoch 66:\n",
            "Training loss: 0.7582, Training accuracy: 0.7346\n",
            "Validation loss: 1.0411, Validation accuracy: 0.6464\n",
            "\n",
            "Epoch 67:\n",
            "Training loss: 0.7542, Training accuracy: 0.7367\n",
            "Validation loss: 1.0542, Validation accuracy: 0.6368\n",
            "\n",
            "Epoch 68:\n",
            "Training loss: 0.7511, Training accuracy: 0.7388\n",
            "Validation loss: 1.0388, Validation accuracy: 0.6433\n",
            "\n",
            "Epoch 69:\n",
            "Training loss: 0.7505, Training accuracy: 0.7379\n",
            "Validation loss: 1.0461, Validation accuracy: 0.6382\n",
            "\n",
            "Epoch 70:\n",
            "Training loss: 0.7420, Training accuracy: 0.7406\n",
            "Validation loss: 1.0454, Validation accuracy: 0.6397\n",
            "\n",
            "Epoch 71:\n",
            "Training loss: 0.7427, Training accuracy: 0.7398\n",
            "Validation loss: 1.0774, Validation accuracy: 0.6274\n",
            "\n",
            "Epoch 72:\n",
            "Training loss: 0.7393, Training accuracy: 0.7415\n",
            "Validation loss: 1.0426, Validation accuracy: 0.6406\n",
            "\n",
            "Epoch 73:\n",
            "Training loss: 0.7368, Training accuracy: 0.7427\n",
            "Validation loss: 1.0485, Validation accuracy: 0.6412\n",
            "\n",
            "Epoch 74:\n",
            "Training loss: 0.7334, Training accuracy: 0.7437\n",
            "Validation loss: 1.0409, Validation accuracy: 0.6431\n",
            "\n",
            "Epoch 75:\n",
            "Training loss: 0.7342, Training accuracy: 0.7419\n",
            "Validation loss: 1.0555, Validation accuracy: 0.6403\n",
            "\n",
            "Epoch 76:\n",
            "Training loss: 0.7273, Training accuracy: 0.7464\n",
            "Validation loss: 1.0595, Validation accuracy: 0.6358\n",
            "\n",
            "Epoch 77:\n",
            "Training loss: 0.7269, Training accuracy: 0.7475\n",
            "Validation loss: 1.0655, Validation accuracy: 0.6356\n",
            "\n",
            "Epoch 78:\n",
            "Training loss: 0.7255, Training accuracy: 0.7446\n",
            "Validation loss: 1.0715, Validation accuracy: 0.6361\n",
            "\n",
            "Epoch 79:\n",
            "Training loss: 0.7273, Training accuracy: 0.7453\n",
            "Validation loss: 1.0676, Validation accuracy: 0.6373\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 80:\n",
            "Training loss: 0.6897, Training accuracy: 0.7597\n",
            "Validation loss: 1.0389, Validation accuracy: 0.6404\n",
            "\n",
            "Epoch 81:\n",
            "Training loss: 0.6804, Training accuracy: 0.7642\n",
            "Validation loss: 1.0334, Validation accuracy: 0.6476\n",
            "\n",
            "Epoch 82:\n",
            "Training loss: 0.6818, Training accuracy: 0.7615\n",
            "Validation loss: 1.0262, Validation accuracy: 0.6458\n",
            "\n",
            "Epoch 83:\n",
            "Training loss: 0.6747, Training accuracy: 0.7662\n",
            "Validation loss: 1.0280, Validation accuracy: 0.6503\n",
            "\n",
            "Epoch 84:\n",
            "Training loss: 0.6742, Training accuracy: 0.7654\n",
            "Validation loss: 1.0290, Validation accuracy: 0.6490\n",
            "\n",
            "Epoch 85:\n",
            "Training loss: 0.6715, Training accuracy: 0.7668\n",
            "Validation loss: 1.0331, Validation accuracy: 0.6436\n",
            "\n",
            "Epoch 86:\n",
            "Training loss: 0.6734, Training accuracy: 0.7690\n",
            "Validation loss: 1.0399, Validation accuracy: 0.6464\n",
            "\n",
            "Epoch 87:\n",
            "Training loss: 0.6673, Training accuracy: 0.7678\n",
            "Validation loss: 1.0394, Validation accuracy: 0.6431\n",
            "\n",
            "Epoch 88:\n",
            "Training loss: 0.6630, Training accuracy: 0.7674\n",
            "Validation loss: 1.0365, Validation accuracy: 0.6480\n",
            "\n",
            "Epoch 89:\n",
            "Training loss: 0.6733, Training accuracy: 0.7655\n",
            "Validation loss: 1.0457, Validation accuracy: 0.6411\n",
            "\n",
            "Epoch 90:\n",
            "Training loss: 0.6636, Training accuracy: 0.7698\n",
            "Validation loss: 1.0328, Validation accuracy: 0.6460\n",
            "\n",
            "Epoch 91:\n",
            "Training loss: 0.6632, Training accuracy: 0.7693\n",
            "Validation loss: 1.0429, Validation accuracy: 0.6462\n",
            "\n",
            "Epoch 92:\n",
            "Training loss: 0.6606, Training accuracy: 0.7708\n",
            "Validation loss: 1.0420, Validation accuracy: 0.6464\n",
            "\n",
            "Epoch 93:\n",
            "Training loss: 0.6606, Training accuracy: 0.7690\n",
            "Validation loss: 1.0418, Validation accuracy: 0.6453\n",
            "\n",
            "Epoch 94:\n",
            "Training loss: 0.6627, Training accuracy: 0.7686\n",
            "Validation loss: 1.0467, Validation accuracy: 0.6436\n",
            "\n",
            "Epoch 95:\n",
            "Training loss: 0.6573, Training accuracy: 0.7716\n",
            "Validation loss: 1.0349, Validation accuracy: 0.6449\n",
            "\n",
            "Epoch 96:\n",
            "Training loss: 0.6574, Training accuracy: 0.7706\n",
            "Validation loss: 1.0341, Validation accuracy: 0.6484\n",
            "\n",
            "Epoch 97:\n",
            "Training loss: 0.6551, Training accuracy: 0.7713\n",
            "Validation loss: 1.0436, Validation accuracy: 0.6443\n",
            "\n",
            "Epoch 98:\n",
            "Training loss: 0.6522, Training accuracy: 0.7735\n",
            "Validation loss: 1.0307, Validation accuracy: 0.6509\n",
            "\n",
            "Epoch 99:\n",
            "Training loss: 0.6562, Training accuracy: 0.7706\n",
            "Validation loss: 1.0457, Validation accuracy: 0.6443\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.6509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**conv + Rotnet(including limited label test and best feature map finding)**"
      ],
      "metadata": {
        "id": "QC4oHP1OtvBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(size = 32, padding=4),#random cropping\n",
        "    transforms.RandomHorizontalFlip() ,#random flip\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transform_train = transform\n",
        "\n",
        "transform_val = transform\n",
        "\n",
        "DATA_ROOT = \"./data\"\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "VAL_BATCH_SIZE = 128\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "train_set = CIFAR10(\n",
        "    root=DATA_ROOT,\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "\n",
        "val_set = CIFAR10(\n",
        "    root=DATA_ROOT,\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_val\n",
        ")\n",
        "\n",
        "# construct dataloader\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_set,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_set,\n",
        "    batch_size=VAL_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5NJEiwKvSrk",
        "outputId": "f91f3030-ba6c-496e-f639-2c63b61882a5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** for feature map generated by conv1**"
      ],
      "metadata": {
        "id": "V7eSIsKCYGbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#example of get the feature map generated by the pretrained model(2nd block)\n",
        "\n",
        "NIN_net_4block = NetworkInNetwork({'num_classes': 4, 'num_stages': 4, 'use_avg_on_conv3': False})\n",
        "\n",
        "# Load the state_dict using torch.load\n",
        "checkpoint_path = '/content/NIN_net_4block.pth'\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# Load the state_dict into the model\n",
        "NIN_net_4block.load_state_dict(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfR4NXV4YF-E",
        "outputId": "82deef50-133e-46af-8af8-7a9a342dee79"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conv-3 + 1st block feature map\n",
        "out_feat_keys = ['conv1']\n",
        "classifier = Classifier({'num_classes': 10, 'nChannels': 96, 'cls_type': 'NIN_ConvBlock3'}).to(device)\n",
        "#hyperparameter setting\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "EPOCHS = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(classifier.parameters(), lr = INITIAL_LR,\n",
        "                      momentum = MOMENTUM,\n",
        "                      weight_decay=REG)"
      ],
      "metadata": {
        "id": "KnhzK1AJYRqU"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the folder where the trained model is saved\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "# start the training/validation process\n",
        "# the process should take about 5 minutes on a GTX 1070-Ti\n",
        "# if the code is written efficiently.\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY = 0.2\n",
        "valid_acc_block4_with_featuremap1 = []\n",
        "losslist = []\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    if i  == 30 or i == 60 or i == 80:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    classifier.train()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    # this help you compute the training accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0 # track training loss if you want\n",
        "\n",
        "    # Train the model for 1 epoch.\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        ####################################\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # inputs = inputs.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "        NIN_net_4block = NIN_net_4block.to(device)\n",
        "        feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # compute the output and loss\n",
        "        outputs = classifier(feature_map)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss\n",
        "        # zero the gradient\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        # apply gradient and update the weights\n",
        "        optimizer.step()\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "        _, predicted = outputs.max(1)  #make prediction based on the highest value\n",
        "        total_examples += targets.size(0) # in this case,128 for each batch\n",
        "        correct_examples += predicted.eq(targets).sum().item()\n",
        "        ####################################\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "\n",
        "    classifier.eval()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    # this help you compute the validation accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 # again, track the validation loss if you want\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            ####################################\n",
        "            # your code here\n",
        "            # copy inputs to device\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "            NIN_net_4block = NIN_net_4block.to(device)\n",
        "\n",
        "\n",
        "            # compute the output and loss\n",
        "\n",
        "            outputs = classifier(feature_map)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # count the number of correctly predicted samples in the current batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += predicted.eq(targets).sum().item()\n",
        "            ####################################\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    valid_acc_block4_with_featuremap1.append(avg_acc)\n",
        "    losslist.append(avg_loss.item())\n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        #if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "        #    os.makedirs(CHECKPOINT_FOLDER)\n",
        "        #print(\"Saving ...\")\n",
        "\n",
        "        torch.save(classifier.state_dict(), 'classifier_conv3_NIN_with_featuremap1.pth')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI8Xaps2YX5I",
        "outputId": "9578ed26-ce03-4c79-b4bf-82b90b0554f3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 1.0790, Training accuracy: 0.6289\n",
            "Validation loss: 1.2768, Validation accuracy: 0.5843\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 0.7813, Training accuracy: 0.7272\n",
            "Validation loss: 0.8854, Validation accuracy: 0.6956\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 0.6993, Training accuracy: 0.7584\n",
            "Validation loss: 1.1032, Validation accuracy: 0.6385\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 0.6447, Training accuracy: 0.7776\n",
            "Validation loss: 0.8926, Validation accuracy: 0.6887\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 0.6130, Training accuracy: 0.7884\n",
            "Validation loss: 0.9410, Validation accuracy: 0.6965\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 0.5900, Training accuracy: 0.7985\n",
            "Validation loss: 0.7384, Validation accuracy: 0.7506\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 0.5729, Training accuracy: 0.8031\n",
            "Validation loss: 0.7851, Validation accuracy: 0.7256\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 0.5562, Training accuracy: 0.8101\n",
            "Validation loss: 0.8792, Validation accuracy: 0.6995\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 0.5452, Training accuracy: 0.8128\n",
            "Validation loss: 0.7925, Validation accuracy: 0.7316\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 0.5441, Training accuracy: 0.8121\n",
            "Validation loss: 0.8302, Validation accuracy: 0.7296\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 0.5312, Training accuracy: 0.8196\n",
            "Validation loss: 0.7657, Validation accuracy: 0.7366\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 0.5195, Training accuracy: 0.8227\n",
            "Validation loss: 0.6744, Validation accuracy: 0.7704\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 0.5182, Training accuracy: 0.8219\n",
            "Validation loss: 0.8104, Validation accuracy: 0.7202\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 0.5129, Training accuracy: 0.8239\n",
            "Validation loss: 0.7663, Validation accuracy: 0.7570\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 0.5054, Training accuracy: 0.8273\n",
            "Validation loss: 0.8284, Validation accuracy: 0.7202\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 0.5085, Training accuracy: 0.8268\n",
            "Validation loss: 0.6675, Validation accuracy: 0.7735\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 0.5097, Training accuracy: 0.8270\n",
            "Validation loss: 0.8728, Validation accuracy: 0.7096\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 0.4992, Training accuracy: 0.8284\n",
            "Validation loss: 0.8572, Validation accuracy: 0.7272\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 0.4917, Training accuracy: 0.8316\n",
            "Validation loss: 0.9056, Validation accuracy: 0.7080\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 0.4997, Training accuracy: 0.8292\n",
            "Validation loss: 0.6865, Validation accuracy: 0.7716\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 0.4907, Training accuracy: 0.8322\n",
            "Validation loss: 0.7426, Validation accuracy: 0.7432\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 0.4887, Training accuracy: 0.8344\n",
            "Validation loss: 0.8957, Validation accuracy: 0.7234\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 0.4881, Training accuracy: 0.8339\n",
            "Validation loss: 0.8399, Validation accuracy: 0.7100\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 0.4884, Training accuracy: 0.8331\n",
            "Validation loss: 0.6278, Validation accuracy: 0.7775\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 0.4868, Training accuracy: 0.8323\n",
            "Validation loss: 0.9090, Validation accuracy: 0.6951\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 0.4865, Training accuracy: 0.8354\n",
            "Validation loss: 0.6878, Validation accuracy: 0.7707\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 0.4791, Training accuracy: 0.8367\n",
            "Validation loss: 0.6091, Validation accuracy: 0.7957\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 0.4827, Training accuracy: 0.8347\n",
            "Validation loss: 0.6691, Validation accuracy: 0.7784\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 0.4818, Training accuracy: 0.8347\n",
            "Validation loss: 0.6547, Validation accuracy: 0.7799\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 0.4875, Training accuracy: 0.8329\n",
            "Validation loss: 0.6560, Validation accuracy: 0.7772\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 30:\n",
            "Training loss: 0.3439, Training accuracy: 0.8874\n",
            "Validation loss: 0.4254, Validation accuracy: 0.8565\n",
            "\n",
            "Epoch 31:\n",
            "Training loss: 0.3080, Training accuracy: 0.8997\n",
            "Validation loss: 0.4144, Validation accuracy: 0.8563\n",
            "\n",
            "Epoch 32:\n",
            "Training loss: 0.2909, Training accuracy: 0.9048\n",
            "Validation loss: 0.4120, Validation accuracy: 0.8598\n",
            "\n",
            "Epoch 33:\n",
            "Training loss: 0.2837, Training accuracy: 0.9060\n",
            "Validation loss: 0.4168, Validation accuracy: 0.8607\n",
            "\n",
            "Epoch 34:\n",
            "Training loss: 0.2790, Training accuracy: 0.9075\n",
            "Validation loss: 0.4299, Validation accuracy: 0.8549\n",
            "\n",
            "Epoch 35:\n",
            "Training loss: 0.2722, Training accuracy: 0.9096\n",
            "Validation loss: 0.4239, Validation accuracy: 0.8587\n",
            "\n",
            "Epoch 36:\n",
            "Training loss: 0.2724, Training accuracy: 0.9089\n",
            "Validation loss: 0.4264, Validation accuracy: 0.8552\n",
            "\n",
            "Epoch 37:\n",
            "Training loss: 0.2702, Training accuracy: 0.9107\n",
            "Validation loss: 0.4334, Validation accuracy: 0.8530\n",
            "\n",
            "Epoch 38:\n",
            "Training loss: 0.2665, Training accuracy: 0.9104\n",
            "Validation loss: 0.4989, Validation accuracy: 0.8365\n",
            "\n",
            "Epoch 39:\n",
            "Training loss: 0.2724, Training accuracy: 0.9094\n",
            "Validation loss: 0.4921, Validation accuracy: 0.8319\n",
            "\n",
            "Epoch 40:\n",
            "Training loss: 0.2714, Training accuracy: 0.9096\n",
            "Validation loss: 0.4814, Validation accuracy: 0.8371\n",
            "\n",
            "Epoch 41:\n",
            "Training loss: 0.2670, Training accuracy: 0.9104\n",
            "Validation loss: 0.5068, Validation accuracy: 0.8366\n",
            "\n",
            "Epoch 42:\n",
            "Training loss: 0.2742, Training accuracy: 0.9080\n",
            "Validation loss: 0.4834, Validation accuracy: 0.8366\n",
            "\n",
            "Epoch 43:\n",
            "Training loss: 0.2714, Training accuracy: 0.9093\n",
            "Validation loss: 0.4272, Validation accuracy: 0.8582\n",
            "\n",
            "Epoch 44:\n",
            "Training loss: 0.2726, Training accuracy: 0.9084\n",
            "Validation loss: 0.6617, Validation accuracy: 0.7954\n",
            "\n",
            "Epoch 45:\n",
            "Training loss: 0.2677, Training accuracy: 0.9109\n",
            "Validation loss: 0.5727, Validation accuracy: 0.8146\n",
            "\n",
            "Epoch 46:\n",
            "Training loss: 0.2680, Training accuracy: 0.9099\n",
            "Validation loss: 0.4682, Validation accuracy: 0.8433\n",
            "\n",
            "Epoch 47:\n",
            "Training loss: 0.2679, Training accuracy: 0.9099\n",
            "Validation loss: 0.5128, Validation accuracy: 0.8300\n",
            "\n",
            "Epoch 48:\n",
            "Training loss: 0.2669, Training accuracy: 0.9102\n",
            "Validation loss: 0.5511, Validation accuracy: 0.8160\n",
            "\n",
            "Epoch 49:\n",
            "Training loss: 0.2663, Training accuracy: 0.9108\n",
            "Validation loss: 0.5216, Validation accuracy: 0.8274\n",
            "\n",
            "Epoch 50:\n",
            "Training loss: 0.2624, Training accuracy: 0.9118\n",
            "Validation loss: 0.6377, Validation accuracy: 0.7975\n",
            "\n",
            "Epoch 51:\n",
            "Training loss: 0.2620, Training accuracy: 0.9125\n",
            "Validation loss: 0.4828, Validation accuracy: 0.8433\n",
            "\n",
            "Epoch 52:\n",
            "Training loss: 0.2651, Training accuracy: 0.9114\n",
            "Validation loss: 0.4608, Validation accuracy: 0.8439\n",
            "\n",
            "Epoch 53:\n",
            "Training loss: 0.2655, Training accuracy: 0.9099\n",
            "Validation loss: 0.5015, Validation accuracy: 0.8316\n",
            "\n",
            "Epoch 54:\n",
            "Training loss: 0.2676, Training accuracy: 0.9094\n",
            "Validation loss: 0.6570, Validation accuracy: 0.7831\n",
            "\n",
            "Epoch 55:\n",
            "Training loss: 0.2635, Training accuracy: 0.9115\n",
            "Validation loss: 0.5298, Validation accuracy: 0.8221\n",
            "\n",
            "Epoch 56:\n",
            "Training loss: 0.2619, Training accuracy: 0.9125\n",
            "Validation loss: 0.5088, Validation accuracy: 0.8271\n",
            "\n",
            "Epoch 57:\n",
            "Training loss: 0.2576, Training accuracy: 0.9142\n",
            "Validation loss: 0.5155, Validation accuracy: 0.8280\n",
            "\n",
            "Epoch 58:\n",
            "Training loss: 0.2642, Training accuracy: 0.9116\n",
            "Validation loss: 0.5558, Validation accuracy: 0.8132\n",
            "\n",
            "Epoch 59:\n",
            "Training loss: 0.2573, Training accuracy: 0.9127\n",
            "Validation loss: 0.5009, Validation accuracy: 0.8355\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 60:\n",
            "Training loss: 0.1874, Training accuracy: 0.9424\n",
            "Validation loss: 0.3694, Validation accuracy: 0.8778\n",
            "\n",
            "Epoch 61:\n",
            "Training loss: 0.1638, Training accuracy: 0.9527\n",
            "Validation loss: 0.3659, Validation accuracy: 0.8793\n",
            "\n",
            "Epoch 62:\n",
            "Training loss: 0.1566, Training accuracy: 0.9551\n",
            "Validation loss: 0.3690, Validation accuracy: 0.8736\n",
            "\n",
            "Epoch 63:\n",
            "Training loss: 0.1524, Training accuracy: 0.9575\n",
            "Validation loss: 0.3691, Validation accuracy: 0.8752\n",
            "\n",
            "Epoch 64:\n",
            "Training loss: 0.1491, Training accuracy: 0.9581\n",
            "Validation loss: 0.3655, Validation accuracy: 0.8769\n",
            "\n",
            "Epoch 65:\n",
            "Training loss: 0.1466, Training accuracy: 0.9591\n",
            "Validation loss: 0.3616, Validation accuracy: 0.8815\n",
            "\n",
            "Epoch 66:\n",
            "Training loss: 0.1410, Training accuracy: 0.9604\n",
            "Validation loss: 0.3750, Validation accuracy: 0.8746\n",
            "\n",
            "Epoch 67:\n",
            "Training loss: 0.1393, Training accuracy: 0.9615\n",
            "Validation loss: 0.3617, Validation accuracy: 0.8804\n",
            "\n",
            "Epoch 68:\n",
            "Training loss: 0.1370, Training accuracy: 0.9615\n",
            "Validation loss: 0.3576, Validation accuracy: 0.8794\n",
            "\n",
            "Epoch 69:\n",
            "Training loss: 0.1390, Training accuracy: 0.9616\n",
            "Validation loss: 0.3726, Validation accuracy: 0.8758\n",
            "\n",
            "Epoch 70:\n",
            "Training loss: 0.1347, Training accuracy: 0.9631\n",
            "Validation loss: 0.3784, Validation accuracy: 0.8760\n",
            "\n",
            "Epoch 71:\n",
            "Training loss: 0.1314, Training accuracy: 0.9636\n",
            "Validation loss: 0.3695, Validation accuracy: 0.8771\n",
            "\n",
            "Epoch 72:\n",
            "Training loss: 0.1291, Training accuracy: 0.9647\n",
            "Validation loss: 0.3756, Validation accuracy: 0.8755\n",
            "\n",
            "Epoch 73:\n",
            "Training loss: 0.1287, Training accuracy: 0.9650\n",
            "Validation loss: 0.3736, Validation accuracy: 0.8719\n",
            "\n",
            "Epoch 74:\n",
            "Training loss: 0.1278, Training accuracy: 0.9652\n",
            "Validation loss: 0.3701, Validation accuracy: 0.8758\n",
            "\n",
            "Epoch 75:\n",
            "Training loss: 0.1266, Training accuracy: 0.9652\n",
            "Validation loss: 0.3957, Validation accuracy: 0.8685\n",
            "\n",
            "Epoch 76:\n",
            "Training loss: 0.1263, Training accuracy: 0.9653\n",
            "Validation loss: 0.3737, Validation accuracy: 0.8764\n",
            "\n",
            "Epoch 77:\n",
            "Training loss: 0.1225, Training accuracy: 0.9668\n",
            "Validation loss: 0.3983, Validation accuracy: 0.8649\n",
            "\n",
            "Epoch 78:\n",
            "Training loss: 0.1226, Training accuracy: 0.9666\n",
            "Validation loss: 0.3944, Validation accuracy: 0.8733\n",
            "\n",
            "Epoch 79:\n",
            "Training loss: 0.1219, Training accuracy: 0.9678\n",
            "Validation loss: 0.3845, Validation accuracy: 0.8760\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 80:\n",
            "Training loss: 0.1074, Training accuracy: 0.9744\n",
            "Validation loss: 0.3613, Validation accuracy: 0.8797\n",
            "\n",
            "Epoch 81:\n",
            "Training loss: 0.1037, Training accuracy: 0.9753\n",
            "Validation loss: 0.3530, Validation accuracy: 0.8827\n",
            "\n",
            "Epoch 82:\n",
            "Training loss: 0.1025, Training accuracy: 0.9757\n",
            "Validation loss: 0.3559, Validation accuracy: 0.8784\n",
            "\n",
            "Epoch 83:\n",
            "Training loss: 0.0997, Training accuracy: 0.9775\n",
            "Validation loss: 0.3573, Validation accuracy: 0.8833\n",
            "\n",
            "Epoch 84:\n",
            "Training loss: 0.1015, Training accuracy: 0.9761\n",
            "Validation loss: 0.3545, Validation accuracy: 0.8815\n",
            "\n",
            "Epoch 85:\n",
            "Training loss: 0.0998, Training accuracy: 0.9773\n",
            "Validation loss: 0.3596, Validation accuracy: 0.8831\n",
            "\n",
            "Epoch 86:\n",
            "Training loss: 0.0972, Training accuracy: 0.9793\n",
            "Validation loss: 0.3585, Validation accuracy: 0.8783\n",
            "\n",
            "Epoch 87:\n",
            "Training loss: 0.0983, Training accuracy: 0.9775\n",
            "Validation loss: 0.3626, Validation accuracy: 0.8796\n",
            "\n",
            "Epoch 88:\n",
            "Training loss: 0.0977, Training accuracy: 0.9780\n",
            "Validation loss: 0.3546, Validation accuracy: 0.8786\n",
            "\n",
            "Epoch 89:\n",
            "Training loss: 0.0977, Training accuracy: 0.9778\n",
            "Validation loss: 0.3616, Validation accuracy: 0.8765\n",
            "\n",
            "Epoch 90:\n",
            "Training loss: 0.0967, Training accuracy: 0.9790\n",
            "Validation loss: 0.3547, Validation accuracy: 0.8824\n",
            "\n",
            "Epoch 91:\n",
            "Training loss: 0.0971, Training accuracy: 0.9782\n",
            "Validation loss: 0.3610, Validation accuracy: 0.8819\n",
            "\n",
            "Epoch 92:\n",
            "Training loss: 0.0962, Training accuracy: 0.9787\n",
            "Validation loss: 0.3534, Validation accuracy: 0.8843\n",
            "\n",
            "Epoch 93:\n",
            "Training loss: 0.0978, Training accuracy: 0.9779\n",
            "Validation loss: 0.3483, Validation accuracy: 0.8797\n",
            "\n",
            "Epoch 94:\n",
            "Training loss: 0.0937, Training accuracy: 0.9795\n",
            "Validation loss: 0.3500, Validation accuracy: 0.8848\n",
            "\n",
            "Epoch 95:\n",
            "Training loss: 0.0948, Training accuracy: 0.9790\n",
            "Validation loss: 0.3569, Validation accuracy: 0.8813\n",
            "\n",
            "Epoch 96:\n",
            "Training loss: 0.0937, Training accuracy: 0.9797\n",
            "Validation loss: 0.3546, Validation accuracy: 0.8826\n",
            "\n",
            "Epoch 97:\n",
            "Training loss: 0.0938, Training accuracy: 0.9795\n",
            "Validation loss: 0.3580, Validation accuracy: 0.8815\n",
            "\n",
            "Epoch 98:\n",
            "Training loss: 0.0949, Training accuracy: 0.9790\n",
            "Validation loss: 0.3602, Validation accuracy: 0.8821\n",
            "\n",
            "Epoch 99:\n",
            "Training loss: 0.0923, Training accuracy: 0.9804\n",
            "Validation loss: 0.3560, Validation accuracy: 0.8813\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.8848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#example of get the feature map generated by the pretrained model(2nd block)\n",
        "\n",
        "NIN_net_4block = NetworkInNetwork({'num_classes': 4, 'num_stages': 4, 'use_avg_on_conv3': False})\n",
        "\n",
        "# Load the state_dict using torch.load\n",
        "checkpoint_path = '/content/NIN_net_4block.pth'\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# Load the state_dict into the model\n",
        "NIN_net_4block.load_state_dict(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSo1mnlsbW7o",
        "outputId": "7e9c8db8-7ab4-4b10-ff7d-b9965bd2400d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conv-3 + 1st block feature map\n",
        "out_feat_keys = ['conv3']\n",
        "classifier = Classifier({'num_classes': 10, 'nChannels': 192, 'cls_type': 'NIN_ConvBlock3'}).to(device)\n",
        "#hyperparameter setting\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "EPOCHS = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(classifier.parameters(), lr = INITIAL_LR,\n",
        "                      momentum = MOMENTUM,\n",
        "                      weight_decay=REG)"
      ],
      "metadata": {
        "id": "iTCGBSkkbXAP"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the folder where the trained model is saved\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "# start the training/validation process\n",
        "# the process should take about 5 minutes on a GTX 1070-Ti\n",
        "# if the code is written efficiently.\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY = 0.2\n",
        "valid_acc_block4_with_featuremap3 = []\n",
        "losslist = []\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    if i  == 30 or i == 60 or i == 80:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    classifier.train()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    # this help you compute the training accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0 # track training loss if you want\n",
        "\n",
        "    # Train the model for 1 epoch.\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        ####################################\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # inputs = inputs.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "        NIN_net_4block = NIN_net_4block.to(device)\n",
        "        feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # compute the output and loss\n",
        "        outputs = classifier(feature_map)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss\n",
        "        # zero the gradient\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        # apply gradient and update the weights\n",
        "        optimizer.step()\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "        _, predicted = outputs.max(1)  #make prediction based on the highest value\n",
        "        total_examples += targets.size(0) # in this case,128 for each batch\n",
        "        correct_examples += predicted.eq(targets).sum().item()\n",
        "        ####################################\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "\n",
        "    classifier.eval()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    # this help you compute the validation accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 # again, track the validation loss if you want\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            ####################################\n",
        "            # your code here\n",
        "            # copy inputs to device\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "            NIN_net_4block = NIN_net_4block.to(device)\n",
        "\n",
        "\n",
        "            # compute the output and loss\n",
        "\n",
        "            outputs = classifier(feature_map)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # count the number of correctly predicted samples in the current batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += predicted.eq(targets).sum().item()\n",
        "            ####################################\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    valid_acc_block4_with_featuremap3.append(avg_acc)\n",
        "    losslist.append(avg_loss.item())\n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        #if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "        #    os.makedirs(CHECKPOINT_FOLDER)\n",
        "        #print(\"Saving ...\")\n",
        "\n",
        "        torch.save(classifier.state_dict(), 'classifier_conv3_NIN_with_featuremap3.pth')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mHJl15abXEp",
        "outputId": "9381d0bb-68e4-4b7e-cfb2-fa0728e3ec76"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 0.8976, Training accuracy: 0.6862\n",
            "Validation loss: 0.7750, Validation accuracy: 0.7311\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 0.7229, Training accuracy: 0.7447\n",
            "Validation loss: 0.8105, Validation accuracy: 0.7143\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 0.6721, Training accuracy: 0.7657\n",
            "Validation loss: 0.7401, Validation accuracy: 0.7413\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 0.6481, Training accuracy: 0.7736\n",
            "Validation loss: 0.7075, Validation accuracy: 0.7508\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 0.6275, Training accuracy: 0.7827\n",
            "Validation loss: 0.6962, Validation accuracy: 0.7522\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 0.6158, Training accuracy: 0.7852\n",
            "Validation loss: 0.6767, Validation accuracy: 0.7652\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 0.6146, Training accuracy: 0.7848\n",
            "Validation loss: 0.6678, Validation accuracy: 0.7706\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 0.6057, Training accuracy: 0.7879\n",
            "Validation loss: 0.6689, Validation accuracy: 0.7652\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 0.6017, Training accuracy: 0.7911\n",
            "Validation loss: 0.6432, Validation accuracy: 0.7795\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 0.5897, Training accuracy: 0.7943\n",
            "Validation loss: 0.6620, Validation accuracy: 0.7686\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 0.5911, Training accuracy: 0.7942\n",
            "Validation loss: 0.6748, Validation accuracy: 0.7665\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 0.5880, Training accuracy: 0.7936\n",
            "Validation loss: 0.7077, Validation accuracy: 0.7494\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 0.5871, Training accuracy: 0.7961\n",
            "Validation loss: 0.6224, Validation accuracy: 0.7844\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 0.5811, Training accuracy: 0.7983\n",
            "Validation loss: 0.6684, Validation accuracy: 0.7656\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 0.5791, Training accuracy: 0.7995\n",
            "Validation loss: 0.6521, Validation accuracy: 0.7768\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 0.5808, Training accuracy: 0.7977\n",
            "Validation loss: 0.6775, Validation accuracy: 0.7631\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 0.5728, Training accuracy: 0.7989\n",
            "Validation loss: 0.6387, Validation accuracy: 0.7735\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 0.5736, Training accuracy: 0.7996\n",
            "Validation loss: 0.7333, Validation accuracy: 0.7500\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 0.5724, Training accuracy: 0.7985\n",
            "Validation loss: 0.6350, Validation accuracy: 0.7764\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 0.5796, Training accuracy: 0.7970\n",
            "Validation loss: 0.6593, Validation accuracy: 0.7688\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 0.5714, Training accuracy: 0.8004\n",
            "Validation loss: 0.7818, Validation accuracy: 0.7196\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 0.5739, Training accuracy: 0.7992\n",
            "Validation loss: 0.6467, Validation accuracy: 0.7767\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 0.5688, Training accuracy: 0.8011\n",
            "Validation loss: 0.6665, Validation accuracy: 0.7655\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 0.5694, Training accuracy: 0.8025\n",
            "Validation loss: 0.6640, Validation accuracy: 0.7668\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 0.5688, Training accuracy: 0.8018\n",
            "Validation loss: 0.6515, Validation accuracy: 0.7705\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 0.5676, Training accuracy: 0.8012\n",
            "Validation loss: 0.6820, Validation accuracy: 0.7637\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 0.5689, Training accuracy: 0.8005\n",
            "Validation loss: 0.6173, Validation accuracy: 0.7803\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 0.5694, Training accuracy: 0.8034\n",
            "Validation loss: 0.6715, Validation accuracy: 0.7674\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 0.5695, Training accuracy: 0.8014\n",
            "Validation loss: 0.7300, Validation accuracy: 0.7506\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 0.5646, Training accuracy: 0.8020\n",
            "Validation loss: 0.6117, Validation accuracy: 0.7862\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 30:\n",
            "Training loss: 0.4734, Training accuracy: 0.8363\n",
            "Validation loss: 0.5379, Validation accuracy: 0.8138\n",
            "\n",
            "Epoch 31:\n",
            "Training loss: 0.4462, Training accuracy: 0.8446\n",
            "Validation loss: 0.5282, Validation accuracy: 0.8178\n",
            "\n",
            "Epoch 32:\n",
            "Training loss: 0.4369, Training accuracy: 0.8484\n",
            "Validation loss: 0.5228, Validation accuracy: 0.8198\n",
            "\n",
            "Epoch 33:\n",
            "Training loss: 0.4328, Training accuracy: 0.8497\n",
            "Validation loss: 0.5409, Validation accuracy: 0.8115\n",
            "\n",
            "Epoch 34:\n",
            "Training loss: 0.4239, Training accuracy: 0.8522\n",
            "Validation loss: 0.5218, Validation accuracy: 0.8191\n",
            "\n",
            "Epoch 35:\n",
            "Training loss: 0.4225, Training accuracy: 0.8533\n",
            "Validation loss: 0.5258, Validation accuracy: 0.8188\n",
            "\n",
            "Epoch 36:\n",
            "Training loss: 0.4190, Training accuracy: 0.8541\n",
            "Validation loss: 0.5259, Validation accuracy: 0.8180\n",
            "\n",
            "Epoch 37:\n",
            "Training loss: 0.4211, Training accuracy: 0.8530\n",
            "Validation loss: 0.5441, Validation accuracy: 0.8120\n",
            "\n",
            "Epoch 38:\n",
            "Training loss: 0.4162, Training accuracy: 0.8553\n",
            "Validation loss: 0.5693, Validation accuracy: 0.8028\n",
            "\n",
            "Epoch 39:\n",
            "Training loss: 0.4149, Training accuracy: 0.8543\n",
            "Validation loss: 0.5445, Validation accuracy: 0.8114\n",
            "\n",
            "Epoch 40:\n",
            "Training loss: 0.4133, Training accuracy: 0.8557\n",
            "Validation loss: 0.5332, Validation accuracy: 0.8167\n",
            "\n",
            "Epoch 41:\n",
            "Training loss: 0.4117, Training accuracy: 0.8558\n",
            "Validation loss: 0.5379, Validation accuracy: 0.8162\n",
            "\n",
            "Epoch 42:\n",
            "Training loss: 0.4165, Training accuracy: 0.8539\n",
            "Validation loss: 0.5539, Validation accuracy: 0.8111\n",
            "\n",
            "Epoch 43:\n",
            "Training loss: 0.4154, Training accuracy: 0.8540\n",
            "Validation loss: 0.5530, Validation accuracy: 0.8062\n",
            "\n",
            "Epoch 44:\n",
            "Training loss: 0.4107, Training accuracy: 0.8564\n",
            "Validation loss: 0.5579, Validation accuracy: 0.8071\n",
            "\n",
            "Epoch 45:\n",
            "Training loss: 0.4099, Training accuracy: 0.8575\n",
            "Validation loss: 0.5418, Validation accuracy: 0.8133\n",
            "\n",
            "Epoch 46:\n",
            "Training loss: 0.4093, Training accuracy: 0.8587\n",
            "Validation loss: 0.5289, Validation accuracy: 0.8202\n",
            "\n",
            "Epoch 47:\n",
            "Training loss: 0.4046, Training accuracy: 0.8576\n",
            "Validation loss: 0.5506, Validation accuracy: 0.8076\n",
            "\n",
            "Epoch 48:\n",
            "Training loss: 0.4040, Training accuracy: 0.8584\n",
            "Validation loss: 0.5630, Validation accuracy: 0.8096\n",
            "\n",
            "Epoch 49:\n",
            "Training loss: 0.4093, Training accuracy: 0.8565\n",
            "Validation loss: 0.5373, Validation accuracy: 0.8140\n",
            "\n",
            "Epoch 50:\n",
            "Training loss: 0.4024, Training accuracy: 0.8595\n",
            "Validation loss: 0.5679, Validation accuracy: 0.8074\n",
            "\n",
            "Epoch 51:\n",
            "Training loss: 0.4069, Training accuracy: 0.8572\n",
            "Validation loss: 0.5688, Validation accuracy: 0.8018\n",
            "\n",
            "Epoch 52:\n",
            "Training loss: 0.4042, Training accuracy: 0.8592\n",
            "Validation loss: 0.5483, Validation accuracy: 0.8102\n",
            "\n",
            "Epoch 53:\n",
            "Training loss: 0.4067, Training accuracy: 0.8570\n",
            "Validation loss: 0.5600, Validation accuracy: 0.8086\n",
            "\n",
            "Epoch 54:\n",
            "Training loss: 0.3946, Training accuracy: 0.8623\n",
            "Validation loss: 0.5348, Validation accuracy: 0.8199\n",
            "\n",
            "Epoch 55:\n",
            "Training loss: 0.3993, Training accuracy: 0.8593\n",
            "Validation loss: 0.5355, Validation accuracy: 0.8121\n",
            "\n",
            "Epoch 56:\n",
            "Training loss: 0.4026, Training accuracy: 0.8585\n",
            "Validation loss: 0.5674, Validation accuracy: 0.8017\n",
            "\n",
            "Epoch 57:\n",
            "Training loss: 0.3980, Training accuracy: 0.8616\n",
            "Validation loss: 0.5484, Validation accuracy: 0.8107\n",
            "\n",
            "Epoch 58:\n",
            "Training loss: 0.4028, Training accuracy: 0.8603\n",
            "Validation loss: 0.5286, Validation accuracy: 0.8161\n",
            "\n",
            "Epoch 59:\n",
            "Training loss: 0.3995, Training accuracy: 0.8604\n",
            "Validation loss: 0.5676, Validation accuracy: 0.8062\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 60:\n",
            "Training loss: 0.3443, Training accuracy: 0.8822\n",
            "Validation loss: 0.4839, Validation accuracy: 0.8307\n",
            "\n",
            "Epoch 61:\n",
            "Training loss: 0.3294, Training accuracy: 0.8862\n",
            "Validation loss: 0.4982, Validation accuracy: 0.8284\n",
            "\n",
            "Epoch 62:\n",
            "Training loss: 0.3213, Training accuracy: 0.8904\n",
            "Validation loss: 0.4858, Validation accuracy: 0.8332\n",
            "\n",
            "Epoch 63:\n",
            "Training loss: 0.3125, Training accuracy: 0.8938\n",
            "Validation loss: 0.4848, Validation accuracy: 0.8321\n",
            "\n",
            "Epoch 64:\n",
            "Training loss: 0.3112, Training accuracy: 0.8953\n",
            "Validation loss: 0.4912, Validation accuracy: 0.8303\n",
            "\n",
            "Epoch 65:\n",
            "Training loss: 0.3040, Training accuracy: 0.8968\n",
            "Validation loss: 0.4947, Validation accuracy: 0.8286\n",
            "\n",
            "Epoch 66:\n",
            "Training loss: 0.3028, Training accuracy: 0.8965\n",
            "Validation loss: 0.5022, Validation accuracy: 0.8289\n",
            "\n",
            "Epoch 67:\n",
            "Training loss: 0.2996, Training accuracy: 0.8977\n",
            "Validation loss: 0.4913, Validation accuracy: 0.8296\n",
            "\n",
            "Epoch 68:\n",
            "Training loss: 0.3001, Training accuracy: 0.8977\n",
            "Validation loss: 0.4848, Validation accuracy: 0.8375\n",
            "\n",
            "Epoch 69:\n",
            "Training loss: 0.2872, Training accuracy: 0.9018\n",
            "Validation loss: 0.4877, Validation accuracy: 0.8340\n",
            "\n",
            "Epoch 70:\n",
            "Training loss: 0.2915, Training accuracy: 0.8992\n",
            "Validation loss: 0.4932, Validation accuracy: 0.8352\n",
            "\n",
            "Epoch 71:\n",
            "Training loss: 0.2924, Training accuracy: 0.8987\n",
            "Validation loss: 0.5062, Validation accuracy: 0.8287\n",
            "\n",
            "Epoch 72:\n",
            "Training loss: 0.2848, Training accuracy: 0.9029\n",
            "Validation loss: 0.4955, Validation accuracy: 0.8335\n",
            "\n",
            "Epoch 73:\n",
            "Training loss: 0.2896, Training accuracy: 0.9001\n",
            "Validation loss: 0.4969, Validation accuracy: 0.8316\n",
            "\n",
            "Epoch 74:\n",
            "Training loss: 0.2832, Training accuracy: 0.9028\n",
            "Validation loss: 0.4968, Validation accuracy: 0.8297\n",
            "\n",
            "Epoch 75:\n",
            "Training loss: 0.2856, Training accuracy: 0.9036\n",
            "Validation loss: 0.5062, Validation accuracy: 0.8293\n",
            "\n",
            "Epoch 76:\n",
            "Training loss: 0.2813, Training accuracy: 0.9033\n",
            "Validation loss: 0.5049, Validation accuracy: 0.8317\n",
            "\n",
            "Epoch 77:\n",
            "Training loss: 0.2786, Training accuracy: 0.9052\n",
            "Validation loss: 0.5128, Validation accuracy: 0.8266\n",
            "\n",
            "Epoch 78:\n",
            "Training loss: 0.2765, Training accuracy: 0.9048\n",
            "Validation loss: 0.5006, Validation accuracy: 0.8339\n",
            "\n",
            "Epoch 79:\n",
            "Training loss: 0.2811, Training accuracy: 0.9025\n",
            "Validation loss: 0.4968, Validation accuracy: 0.8350\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 80:\n",
            "Training loss: 0.2619, Training accuracy: 0.9109\n",
            "Validation loss: 0.4999, Validation accuracy: 0.8320\n",
            "\n",
            "Epoch 81:\n",
            "Training loss: 0.2526, Training accuracy: 0.9136\n",
            "Validation loss: 0.4889, Validation accuracy: 0.8327\n",
            "\n",
            "Epoch 82:\n",
            "Training loss: 0.2561, Training accuracy: 0.9145\n",
            "Validation loss: 0.4989, Validation accuracy: 0.8329\n",
            "\n",
            "Epoch 83:\n",
            "Training loss: 0.2541, Training accuracy: 0.9148\n",
            "Validation loss: 0.4969, Validation accuracy: 0.8335\n",
            "\n",
            "Epoch 84:\n",
            "Training loss: 0.2484, Training accuracy: 0.9174\n",
            "Validation loss: 0.4887, Validation accuracy: 0.8370\n",
            "\n",
            "Epoch 85:\n",
            "Training loss: 0.2488, Training accuracy: 0.9165\n",
            "Validation loss: 0.4914, Validation accuracy: 0.8311\n",
            "\n",
            "Epoch 86:\n",
            "Training loss: 0.2450, Training accuracy: 0.9176\n",
            "Validation loss: 0.4835, Validation accuracy: 0.8351\n",
            "\n",
            "Epoch 87:\n",
            "Training loss: 0.2492, Training accuracy: 0.9156\n",
            "Validation loss: 0.4820, Validation accuracy: 0.8299\n",
            "\n",
            "Epoch 88:\n",
            "Training loss: 0.2446, Training accuracy: 0.9174\n",
            "Validation loss: 0.4823, Validation accuracy: 0.8372\n",
            "\n",
            "Epoch 89:\n",
            "Training loss: 0.2439, Training accuracy: 0.9188\n",
            "Validation loss: 0.4921, Validation accuracy: 0.8338\n",
            "\n",
            "Epoch 90:\n",
            "Training loss: 0.2452, Training accuracy: 0.9168\n",
            "Validation loss: 0.5016, Validation accuracy: 0.8325\n",
            "\n",
            "Epoch 91:\n",
            "Training loss: 0.2423, Training accuracy: 0.9175\n",
            "Validation loss: 0.4956, Validation accuracy: 0.8368\n",
            "\n",
            "Epoch 92:\n",
            "Training loss: 0.2435, Training accuracy: 0.9172\n",
            "Validation loss: 0.4903, Validation accuracy: 0.8327\n",
            "\n",
            "Epoch 93:\n",
            "Training loss: 0.2389, Training accuracy: 0.9204\n",
            "Validation loss: 0.4852, Validation accuracy: 0.8383\n",
            "\n",
            "Epoch 94:\n",
            "Training loss: 0.2437, Training accuracy: 0.9192\n",
            "Validation loss: 0.4936, Validation accuracy: 0.8354\n",
            "\n",
            "Epoch 95:\n",
            "Training loss: 0.2454, Training accuracy: 0.9175\n",
            "Validation loss: 0.4933, Validation accuracy: 0.8344\n",
            "\n",
            "Epoch 96:\n",
            "Training loss: 0.2429, Training accuracy: 0.9177\n",
            "Validation loss: 0.5060, Validation accuracy: 0.8297\n",
            "\n",
            "Epoch 97:\n",
            "Training loss: 0.2395, Training accuracy: 0.9203\n",
            "Validation loss: 0.4811, Validation accuracy: 0.8382\n",
            "\n",
            "Epoch 98:\n",
            "Training loss: 0.2364, Training accuracy: 0.9202\n",
            "Validation loss: 0.4938, Validation accuracy: 0.8328\n",
            "\n",
            "Epoch 99:\n",
            "Training loss: 0.2382, Training accuracy: 0.9208\n",
            "Validation loss: 0.4819, Validation accuracy: 0.8378\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.8383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#example of get the feature map generated by the pretrained model(4th block)\n",
        "\n",
        "NIN_net_4block = NetworkInNetwork({'num_classes': 4, 'num_stages': 4, 'use_avg_on_conv3': False})\n",
        "\n",
        "# Load the state_dict using torch.load\n",
        "checkpoint_path = '/content/NIN_net_4block.pth'\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# Load the state_dict into the model\n",
        "NIN_net_4block.load_state_dict(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNsSt_K1fRNJ",
        "outputId": "a54feb30-8a0f-43b7-b5b5-730c97d26d0f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conv-3 + 4th block feature map\n",
        "out_feat_keys = ['conv4']\n",
        "classifier = Classifier({'num_classes': 10, 'nChannels': 192, 'cls_type': 'NIN_ConvBlock3'}).to(device)\n",
        "#hyperparameter setting\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "EPOCHS = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(classifier.parameters(), lr = INITIAL_LR,\n",
        "                      momentum = MOMENTUM,\n",
        "                      weight_decay=REG)"
      ],
      "metadata": {
        "id": "wErO_5chfcqe"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the folder where the trained model is saved\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "# start the training/validation process\n",
        "# the process should take about 5 minutes on a GTX 1070-Ti\n",
        "# if the code is written efficiently.\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY = 0.2\n",
        "valid_acc_block4_with_featuremap4 = []\n",
        "losslist = []\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    if i  == 30 or i == 60 or i == 80:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    classifier.train()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    # this help you compute the training accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0 # track training loss if you want\n",
        "\n",
        "    # Train the model for 1 epoch.\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        ####################################\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # inputs = inputs.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "        NIN_net_4block = NIN_net_4block.to(device)\n",
        "        feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # compute the output and loss\n",
        "        outputs = classifier(feature_map)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss\n",
        "        # zero the gradient\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        # apply gradient and update the weights\n",
        "        optimizer.step()\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "        _, predicted = outputs.max(1)  #make prediction based on the highest value\n",
        "        total_examples += targets.size(0) # in this case,128 for each batch\n",
        "        correct_examples += predicted.eq(targets).sum().item()\n",
        "        ####################################\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "\n",
        "    classifier.eval()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    # this help you compute the validation accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 # again, track the validation loss if you want\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            ####################################\n",
        "            # your code here\n",
        "            # copy inputs to device\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "            NIN_net_4block = NIN_net_4block.to(device)\n",
        "\n",
        "\n",
        "            # compute the output and loss\n",
        "\n",
        "            outputs = classifier(feature_map)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # count the number of correctly predicted samples in the current batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += predicted.eq(targets).sum().item()\n",
        "            ####################################\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    valid_acc_block4_with_featuremap4.append(avg_acc)\n",
        "    losslist.append(avg_loss.item())\n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        #if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "        #    os.makedirs(CHECKPOINT_FOLDER)\n",
        "        #print(\"Saving ...\")\n",
        "\n",
        "        torch.save(classifier.state_dict(), 'classifier_conv3_NIN_with_featuremap4.pth')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0BZxIC7fi_z",
        "outputId": "48ba61a7-0718-47db-f2db-621e36053683"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 1.9751, Training accuracy: 0.2964\n",
            "Validation loss: 1.8665, Validation accuracy: 0.3178\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 1.7771, Training accuracy: 0.3589\n",
            "Validation loss: 1.7393, Validation accuracy: 0.3685\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 1.7186, Training accuracy: 0.3768\n",
            "Validation loss: 1.7539, Validation accuracy: 0.3754\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 1.6871, Training accuracy: 0.3928\n",
            "Validation loss: 1.7003, Validation accuracy: 0.3841\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 1.6613, Training accuracy: 0.4008\n",
            "Validation loss: 1.9548, Validation accuracy: 0.3133\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 1.6597, Training accuracy: 0.3974\n",
            "Validation loss: 1.7620, Validation accuracy: 0.3610\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 1.6439, Training accuracy: 0.4051\n",
            "Validation loss: 1.7442, Validation accuracy: 0.3764\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 1.6390, Training accuracy: 0.4088\n",
            "Validation loss: 1.6699, Validation accuracy: 0.3985\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 1.6263, Training accuracy: 0.4114\n",
            "Validation loss: 1.6801, Validation accuracy: 0.3980\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 1.6289, Training accuracy: 0.4128\n",
            "Validation loss: 1.6729, Validation accuracy: 0.4019\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 1.6289, Training accuracy: 0.4080\n",
            "Validation loss: 1.6160, Validation accuracy: 0.4171\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 1.6163, Training accuracy: 0.4166\n",
            "Validation loss: 1.6988, Validation accuracy: 0.3860\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 1.6117, Training accuracy: 0.4180\n",
            "Validation loss: 1.6246, Validation accuracy: 0.4199\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 1.6147, Training accuracy: 0.4160\n",
            "Validation loss: 1.6252, Validation accuracy: 0.4121\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 1.6080, Training accuracy: 0.4186\n",
            "Validation loss: 1.7192, Validation accuracy: 0.3794\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 1.6107, Training accuracy: 0.4190\n",
            "Validation loss: 1.6801, Validation accuracy: 0.4007\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 1.6078, Training accuracy: 0.4174\n",
            "Validation loss: 1.6744, Validation accuracy: 0.3944\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 1.6079, Training accuracy: 0.4156\n",
            "Validation loss: 1.7678, Validation accuracy: 0.3715\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 1.6051, Training accuracy: 0.4199\n",
            "Validation loss: 1.6619, Validation accuracy: 0.3974\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 1.6063, Training accuracy: 0.4195\n",
            "Validation loss: 1.6914, Validation accuracy: 0.3959\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 1.6096, Training accuracy: 0.4189\n",
            "Validation loss: 1.6531, Validation accuracy: 0.3998\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 1.5997, Training accuracy: 0.4208\n",
            "Validation loss: 1.7328, Validation accuracy: 0.3837\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 1.6017, Training accuracy: 0.4200\n",
            "Validation loss: 1.6779, Validation accuracy: 0.3901\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 1.5946, Training accuracy: 0.4236\n",
            "Validation loss: 1.7593, Validation accuracy: 0.3748\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 1.6002, Training accuracy: 0.4225\n",
            "Validation loss: 1.6206, Validation accuracy: 0.4092\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 1.5996, Training accuracy: 0.4223\n",
            "Validation loss: 1.6308, Validation accuracy: 0.4150\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 1.5942, Training accuracy: 0.4234\n",
            "Validation loss: 1.7024, Validation accuracy: 0.3740\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 1.5949, Training accuracy: 0.4233\n",
            "Validation loss: 1.6665, Validation accuracy: 0.4006\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 1.5921, Training accuracy: 0.4241\n",
            "Validation loss: 1.6453, Validation accuracy: 0.4071\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 1.5992, Training accuracy: 0.4240\n",
            "Validation loss: 1.6387, Validation accuracy: 0.4085\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 30:\n",
            "Training loss: 1.5035, Training accuracy: 0.4576\n",
            "Validation loss: 1.5162, Validation accuracy: 0.4520\n",
            "\n",
            "Epoch 31:\n",
            "Training loss: 1.4797, Training accuracy: 0.4650\n",
            "Validation loss: 1.5032, Validation accuracy: 0.4624\n",
            "\n",
            "Epoch 32:\n",
            "Training loss: 1.4816, Training accuracy: 0.4659\n",
            "Validation loss: 1.5090, Validation accuracy: 0.4568\n",
            "\n",
            "Epoch 33:\n",
            "Training loss: 1.4754, Training accuracy: 0.4685\n",
            "Validation loss: 1.4960, Validation accuracy: 0.4572\n",
            "\n",
            "Epoch 34:\n",
            "Training loss: 1.4825, Training accuracy: 0.4659\n",
            "Validation loss: 1.5454, Validation accuracy: 0.4426\n",
            "\n",
            "Epoch 35:\n",
            "Training loss: 1.4758, Training accuracy: 0.4676\n",
            "Validation loss: 1.4971, Validation accuracy: 0.4543\n",
            "\n",
            "Epoch 36:\n",
            "Training loss: 1.4751, Training accuracy: 0.4682\n",
            "Validation loss: 1.4894, Validation accuracy: 0.4643\n",
            "\n",
            "Epoch 37:\n",
            "Training loss: 1.4725, Training accuracy: 0.4702\n",
            "Validation loss: 1.5588, Validation accuracy: 0.4444\n",
            "\n",
            "Epoch 38:\n",
            "Training loss: 1.4704, Training accuracy: 0.4702\n",
            "Validation loss: 1.4842, Validation accuracy: 0.4658\n",
            "\n",
            "Epoch 39:\n",
            "Training loss: 1.4751, Training accuracy: 0.4696\n",
            "Validation loss: 1.5007, Validation accuracy: 0.4605\n",
            "\n",
            "Epoch 40:\n",
            "Training loss: 1.4742, Training accuracy: 0.4678\n",
            "Validation loss: 1.4819, Validation accuracy: 0.4683\n",
            "\n",
            "Epoch 41:\n",
            "Training loss: 1.4676, Training accuracy: 0.4702\n",
            "Validation loss: 1.4827, Validation accuracy: 0.4605\n",
            "\n",
            "Epoch 42:\n",
            "Training loss: 1.4660, Training accuracy: 0.4708\n",
            "Validation loss: 1.5118, Validation accuracy: 0.4583\n",
            "\n",
            "Epoch 43:\n",
            "Training loss: 1.4729, Training accuracy: 0.4677\n",
            "Validation loss: 1.5543, Validation accuracy: 0.4436\n",
            "\n",
            "Epoch 44:\n",
            "Training loss: 1.4681, Training accuracy: 0.4703\n",
            "Validation loss: 1.4945, Validation accuracy: 0.4642\n",
            "\n",
            "Epoch 45:\n",
            "Training loss: 1.4688, Training accuracy: 0.4720\n",
            "Validation loss: 1.5914, Validation accuracy: 0.4334\n",
            "\n",
            "Epoch 46:\n",
            "Training loss: 1.4705, Training accuracy: 0.4663\n",
            "Validation loss: 1.4817, Validation accuracy: 0.4656\n",
            "\n",
            "Epoch 47:\n",
            "Training loss: 1.4712, Training accuracy: 0.4692\n",
            "Validation loss: 1.5575, Validation accuracy: 0.4458\n",
            "\n",
            "Epoch 48:\n",
            "Training loss: 1.4683, Training accuracy: 0.4721\n",
            "Validation loss: 1.5146, Validation accuracy: 0.4562\n",
            "\n",
            "Epoch 49:\n",
            "Training loss: 1.4630, Training accuracy: 0.4712\n",
            "Validation loss: 1.5092, Validation accuracy: 0.4597\n",
            "\n",
            "Epoch 50:\n",
            "Training loss: 1.4658, Training accuracy: 0.4718\n",
            "Validation loss: 1.5275, Validation accuracy: 0.4554\n",
            "\n",
            "Epoch 51:\n",
            "Training loss: 1.4574, Training accuracy: 0.4734\n",
            "Validation loss: 1.5686, Validation accuracy: 0.4359\n",
            "\n",
            "Epoch 52:\n",
            "Training loss: 1.4660, Training accuracy: 0.4722\n",
            "Validation loss: 1.4773, Validation accuracy: 0.4695\n",
            "\n",
            "Epoch 53:\n",
            "Training loss: 1.4621, Training accuracy: 0.4718\n",
            "Validation loss: 1.4967, Validation accuracy: 0.4641\n",
            "\n",
            "Epoch 54:\n",
            "Training loss: 1.4673, Training accuracy: 0.4705\n",
            "Validation loss: 1.4864, Validation accuracy: 0.4612\n",
            "\n",
            "Epoch 55:\n",
            "Training loss: 1.4659, Training accuracy: 0.4701\n",
            "Validation loss: 1.6338, Validation accuracy: 0.4249\n",
            "\n",
            "Epoch 56:\n",
            "Training loss: 1.4658, Training accuracy: 0.4719\n",
            "Validation loss: 1.4944, Validation accuracy: 0.4597\n",
            "\n",
            "Epoch 57:\n",
            "Training loss: 1.4556, Training accuracy: 0.4750\n",
            "Validation loss: 1.5373, Validation accuracy: 0.4518\n",
            "\n",
            "Epoch 58:\n",
            "Training loss: 1.4542, Training accuracy: 0.4777\n",
            "Validation loss: 1.7513, Validation accuracy: 0.4034\n",
            "\n",
            "Epoch 59:\n",
            "Training loss: 1.4645, Training accuracy: 0.4716\n",
            "Validation loss: 1.5391, Validation accuracy: 0.4422\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 60:\n",
            "Training loss: 1.4025, Training accuracy: 0.4957\n",
            "Validation loss: 1.4030, Validation accuracy: 0.4955\n",
            "\n",
            "Epoch 61:\n",
            "Training loss: 1.3874, Training accuracy: 0.5011\n",
            "Validation loss: 1.4088, Validation accuracy: 0.4922\n",
            "\n",
            "Epoch 62:\n",
            "Training loss: 1.3842, Training accuracy: 0.5020\n",
            "Validation loss: 1.3964, Validation accuracy: 0.4984\n",
            "\n",
            "Epoch 63:\n",
            "Training loss: 1.3853, Training accuracy: 0.5007\n",
            "Validation loss: 1.4048, Validation accuracy: 0.4902\n",
            "\n",
            "Epoch 64:\n",
            "Training loss: 1.3836, Training accuracy: 0.5012\n",
            "Validation loss: 1.4213, Validation accuracy: 0.4871\n",
            "\n",
            "Epoch 65:\n",
            "Training loss: 1.3717, Training accuracy: 0.5074\n",
            "Validation loss: 1.4130, Validation accuracy: 0.4955\n",
            "\n",
            "Epoch 66:\n",
            "Training loss: 1.3723, Training accuracy: 0.5043\n",
            "Validation loss: 1.4528, Validation accuracy: 0.4798\n",
            "\n",
            "Epoch 67:\n",
            "Training loss: 1.3718, Training accuracy: 0.5050\n",
            "Validation loss: 1.3938, Validation accuracy: 0.4949\n",
            "\n",
            "Epoch 68:\n",
            "Training loss: 1.3722, Training accuracy: 0.5073\n",
            "Validation loss: 1.3911, Validation accuracy: 0.5016\n",
            "\n",
            "Epoch 69:\n",
            "Training loss: 1.3714, Training accuracy: 0.5040\n",
            "Validation loss: 1.4006, Validation accuracy: 0.4958\n",
            "\n",
            "Epoch 70:\n",
            "Training loss: 1.3708, Training accuracy: 0.5062\n",
            "Validation loss: 1.4282, Validation accuracy: 0.4898\n",
            "\n",
            "Epoch 71:\n",
            "Training loss: 1.3617, Training accuracy: 0.5103\n",
            "Validation loss: 1.3820, Validation accuracy: 0.5068\n",
            "\n",
            "Epoch 72:\n",
            "Training loss: 1.3674, Training accuracy: 0.5061\n",
            "Validation loss: 1.4486, Validation accuracy: 0.4853\n",
            "\n",
            "Epoch 73:\n",
            "Training loss: 1.3681, Training accuracy: 0.5075\n",
            "Validation loss: 1.4179, Validation accuracy: 0.4912\n",
            "\n",
            "Epoch 74:\n",
            "Training loss: 1.3641, Training accuracy: 0.5098\n",
            "Validation loss: 1.4152, Validation accuracy: 0.4920\n",
            "\n",
            "Epoch 75:\n",
            "Training loss: 1.3681, Training accuracy: 0.5067\n",
            "Validation loss: 1.4266, Validation accuracy: 0.4887\n",
            "\n",
            "Epoch 76:\n",
            "Training loss: 1.3656, Training accuracy: 0.5082\n",
            "Validation loss: 1.4010, Validation accuracy: 0.4972\n",
            "\n",
            "Epoch 77:\n",
            "Training loss: 1.3656, Training accuracy: 0.5082\n",
            "Validation loss: 1.3899, Validation accuracy: 0.5031\n",
            "\n",
            "Epoch 78:\n",
            "Training loss: 1.3636, Training accuracy: 0.5124\n",
            "Validation loss: 1.4667, Validation accuracy: 0.4829\n",
            "\n",
            "Epoch 79:\n",
            "Training loss: 1.3646, Training accuracy: 0.5100\n",
            "Validation loss: 1.3878, Validation accuracy: 0.5049\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 80:\n",
            "Training loss: 1.3357, Training accuracy: 0.5177\n",
            "Validation loss: 1.3622, Validation accuracy: 0.5103\n",
            "\n",
            "Epoch 81:\n",
            "Training loss: 1.3322, Training accuracy: 0.5216\n",
            "Validation loss: 1.3419, Validation accuracy: 0.5229\n",
            "\n",
            "Epoch 82:\n",
            "Training loss: 1.3318, Training accuracy: 0.5214\n",
            "Validation loss: 1.3531, Validation accuracy: 0.5162\n",
            "\n",
            "Epoch 83:\n",
            "Training loss: 1.3269, Training accuracy: 0.5237\n",
            "Validation loss: 1.3511, Validation accuracy: 0.5212\n",
            "\n",
            "Epoch 84:\n",
            "Training loss: 1.3246, Training accuracy: 0.5253\n",
            "Validation loss: 1.3444, Validation accuracy: 0.5139\n",
            "\n",
            "Epoch 85:\n",
            "Training loss: 1.3231, Training accuracy: 0.5220\n",
            "Validation loss: 1.3453, Validation accuracy: 0.5229\n",
            "\n",
            "Epoch 86:\n",
            "Training loss: 1.3255, Training accuracy: 0.5217\n",
            "Validation loss: 1.3439, Validation accuracy: 0.5155\n",
            "\n",
            "Epoch 87:\n",
            "Training loss: 1.3223, Training accuracy: 0.5252\n",
            "Validation loss: 1.3568, Validation accuracy: 0.5105\n",
            "\n",
            "Epoch 88:\n",
            "Training loss: 1.3257, Training accuracy: 0.5231\n",
            "Validation loss: 1.3349, Validation accuracy: 0.5216\n",
            "\n",
            "Epoch 89:\n",
            "Training loss: 1.3178, Training accuracy: 0.5260\n",
            "Validation loss: 1.3689, Validation accuracy: 0.5071\n",
            "\n",
            "Epoch 90:\n",
            "Training loss: 1.3171, Training accuracy: 0.5256\n",
            "Validation loss: 1.3474, Validation accuracy: 0.5088\n",
            "\n",
            "Epoch 91:\n",
            "Training loss: 1.3218, Training accuracy: 0.5231\n",
            "Validation loss: 1.3385, Validation accuracy: 0.5189\n",
            "\n",
            "Epoch 92:\n",
            "Training loss: 1.3231, Training accuracy: 0.5238\n",
            "Validation loss: 1.3668, Validation accuracy: 0.5084\n",
            "\n",
            "Epoch 93:\n",
            "Training loss: 1.3117, Training accuracy: 0.5288\n",
            "Validation loss: 1.3555, Validation accuracy: 0.5168\n",
            "\n",
            "Epoch 94:\n",
            "Training loss: 1.3139, Training accuracy: 0.5294\n",
            "Validation loss: 1.3486, Validation accuracy: 0.5200\n",
            "\n",
            "Epoch 95:\n",
            "Training loss: 1.3151, Training accuracy: 0.5255\n",
            "Validation loss: 1.3544, Validation accuracy: 0.5187\n",
            "\n",
            "Epoch 96:\n",
            "Training loss: 1.3090, Training accuracy: 0.5280\n",
            "Validation loss: 1.3426, Validation accuracy: 0.5210\n",
            "\n",
            "Epoch 97:\n",
            "Training loss: 1.3177, Training accuracy: 0.5234\n",
            "Validation loss: 1.3441, Validation accuracy: 0.5161\n",
            "\n",
            "Epoch 98:\n",
            "Training loss: 1.3187, Training accuracy: 0.5257\n",
            "Validation loss: 1.3504, Validation accuracy: 0.5108\n",
            "\n",
            "Epoch 99:\n",
            "Training loss: 1.3196, Training accuracy: 0.5233\n",
            "Validation loss: 1.3446, Validation accuracy: 0.5098\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.5229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#example of get the feature map generated by the pretrained model(2nd block)\n",
        "\n",
        "NIN_net_4block = NetworkInNetwork({'num_classes': 4, 'num_stages': 4, 'use_avg_on_conv3': False})\n",
        "\n",
        "# Load the state_dict using torch.load\n",
        "checkpoint_path = '/content/NIN_net_4block.pth'\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# Load the state_dict into the model\n",
        "NIN_net_4block.load_state_dict(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No51xLoxFt6G",
        "outputId": "4ab765c2-f4cf-4096-c197-4bfdb388007a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conv-3 + 2nd block feature map\n",
        "out_feat_keys = ['conv2']\n",
        "classifier = Classifier({'num_classes': 10, 'nChannels': 192, 'cls_type': 'NIN_ConvBlock3'}).to(device)\n",
        "#hyperparameter setting\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "EPOCHS = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(classifier.parameters(), lr = INITIAL_LR,\n",
        "                      momentum = MOMENTUM,\n",
        "                      weight_decay=REG)"
      ],
      "metadata": {
        "id": "u-kG9-0guEyn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the folder where the trained model is saved\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "# start the training/validation process\n",
        "# the process should take about 5 minutes on a GTX 1070-Ti\n",
        "# if the code is written efficiently.\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY = 0.2\n",
        "valid_acc_block4_with_featuremap2 = []\n",
        "losslist = []\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    if i  == 30 or i == 60 or i == 80:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    classifier.train()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    # this help you compute the training accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0 # track training loss if you want\n",
        "\n",
        "    # Train the model for 1 epoch.\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        ####################################\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # inputs = inputs.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "        NIN_net_4block = NIN_net_4block.to(device)\n",
        "        feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # compute the output and loss\n",
        "        outputs = classifier(feature_map)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss\n",
        "        # zero the gradient\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        # apply gradient and update the weights\n",
        "        optimizer.step()\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "        _, predicted = outputs.max(1)  #make prediction based on the highest value\n",
        "        total_examples += targets.size(0) # in this case,128 for each batch\n",
        "        correct_examples += predicted.eq(targets).sum().item()\n",
        "        ####################################\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "\n",
        "    classifier.eval()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    # this help you compute the validation accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 # again, track the validation loss if you want\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            ####################################\n",
        "            # your code here\n",
        "            # copy inputs to device\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "            NIN_net_4block = NIN_net_4block.to(device)\n",
        "\n",
        "\n",
        "            # compute the output and loss\n",
        "\n",
        "            outputs = classifier(feature_map)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # count the number of correctly predicted samples in the current batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += predicted.eq(targets).sum().item()\n",
        "            ####################################\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    valid_acc_block4_with_featuremap2.append(avg_acc)\n",
        "    losslist.append(avg_loss.item())\n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        #if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "        #    os.makedirs(CHECKPOINT_FOLDER)\n",
        "        #print(\"Saving ...\")\n",
        "\n",
        "        torch.save(classifier.state_dict(), 'classifier_conv3_NIN_with_featuremap2.pth')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiZTC7KnGsIZ",
        "outputId": "c9be47e8-545f-48d0-cf95-eb55fd4dcb05"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 0.7636, Training accuracy: 0.7375\n",
            "Validation loss: 0.6181, Validation accuracy: 0.7880\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 0.5606, Training accuracy: 0.8048\n",
            "Validation loss: 0.6315, Validation accuracy: 0.7842\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 0.5051, Training accuracy: 0.8255\n",
            "Validation loss: 0.5343, Validation accuracy: 0.8156\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 0.4677, Training accuracy: 0.8390\n",
            "Validation loss: 0.5299, Validation accuracy: 0.8155\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 0.4487, Training accuracy: 0.8469\n",
            "Validation loss: 0.4999, Validation accuracy: 0.8329\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 0.4297, Training accuracy: 0.8520\n",
            "Validation loss: 0.5168, Validation accuracy: 0.8238\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 0.4249, Training accuracy: 0.8532\n",
            "Validation loss: 0.5441, Validation accuracy: 0.8092\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 0.4158, Training accuracy: 0.8569\n",
            "Validation loss: 0.5857, Validation accuracy: 0.8040\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 0.4058, Training accuracy: 0.8606\n",
            "Validation loss: 0.5067, Validation accuracy: 0.8312\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 0.4014, Training accuracy: 0.8620\n",
            "Validation loss: 0.4976, Validation accuracy: 0.8263\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 0.3948, Training accuracy: 0.8636\n",
            "Validation loss: 0.5605, Validation accuracy: 0.8108\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 0.3907, Training accuracy: 0.8656\n",
            "Validation loss: 0.4881, Validation accuracy: 0.8342\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 0.3875, Training accuracy: 0.8658\n",
            "Validation loss: 0.4882, Validation accuracy: 0.8353\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 0.3812, Training accuracy: 0.8682\n",
            "Validation loss: 0.5090, Validation accuracy: 0.8248\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 0.3823, Training accuracy: 0.8677\n",
            "Validation loss: 0.4813, Validation accuracy: 0.8355\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 0.3796, Training accuracy: 0.8705\n",
            "Validation loss: 0.5032, Validation accuracy: 0.8284\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 0.3769, Training accuracy: 0.8700\n",
            "Validation loss: 0.5131, Validation accuracy: 0.8277\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 0.3746, Training accuracy: 0.8704\n",
            "Validation loss: 0.5489, Validation accuracy: 0.8108\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 0.3735, Training accuracy: 0.8712\n",
            "Validation loss: 0.5119, Validation accuracy: 0.8203\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 0.3712, Training accuracy: 0.8722\n",
            "Validation loss: 0.6099, Validation accuracy: 0.8028\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 0.3702, Training accuracy: 0.8724\n",
            "Validation loss: 0.4801, Validation accuracy: 0.8360\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 0.3669, Training accuracy: 0.8750\n",
            "Validation loss: 0.5003, Validation accuracy: 0.8275\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 0.3665, Training accuracy: 0.8735\n",
            "Validation loss: 0.5357, Validation accuracy: 0.8218\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 0.3665, Training accuracy: 0.8724\n",
            "Validation loss: 0.4708, Validation accuracy: 0.8421\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 0.3627, Training accuracy: 0.8756\n",
            "Validation loss: 0.5070, Validation accuracy: 0.8282\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 0.3609, Training accuracy: 0.8747\n",
            "Validation loss: 0.5611, Validation accuracy: 0.8177\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 0.3617, Training accuracy: 0.8745\n",
            "Validation loss: 0.5428, Validation accuracy: 0.8168\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 0.3633, Training accuracy: 0.8755\n",
            "Validation loss: 0.5354, Validation accuracy: 0.8167\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 0.3593, Training accuracy: 0.8770\n",
            "Validation loss: 0.5336, Validation accuracy: 0.8211\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 0.3613, Training accuracy: 0.8746\n",
            "Validation loss: 0.5010, Validation accuracy: 0.8311\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 30:\n",
            "Training loss: 0.2604, Training accuracy: 0.9131\n",
            "Validation loss: 0.3643, Validation accuracy: 0.8782\n",
            "\n",
            "Epoch 31:\n",
            "Training loss: 0.2249, Training accuracy: 0.9248\n",
            "Validation loss: 0.3716, Validation accuracy: 0.8776\n",
            "\n",
            "Epoch 32:\n",
            "Training loss: 0.2096, Training accuracy: 0.9298\n",
            "Validation loss: 0.3724, Validation accuracy: 0.8742\n",
            "\n",
            "Epoch 33:\n",
            "Training loss: 0.2025, Training accuracy: 0.9330\n",
            "Validation loss: 0.3715, Validation accuracy: 0.8733\n",
            "\n",
            "Epoch 34:\n",
            "Training loss: 0.1928, Training accuracy: 0.9348\n",
            "Validation loss: 0.3670, Validation accuracy: 0.8758\n",
            "\n",
            "Epoch 35:\n",
            "Training loss: 0.1886, Training accuracy: 0.9375\n",
            "Validation loss: 0.3730, Validation accuracy: 0.8758\n",
            "\n",
            "Epoch 36:\n",
            "Training loss: 0.1877, Training accuracy: 0.9371\n",
            "Validation loss: 0.3856, Validation accuracy: 0.8716\n",
            "\n",
            "Epoch 37:\n",
            "Training loss: 0.1873, Training accuracy: 0.9359\n",
            "Validation loss: 0.3771, Validation accuracy: 0.8725\n",
            "\n",
            "Epoch 38:\n",
            "Training loss: 0.1827, Training accuracy: 0.9390\n",
            "Validation loss: 0.3768, Validation accuracy: 0.8723\n",
            "\n",
            "Epoch 39:\n",
            "Training loss: 0.1801, Training accuracy: 0.9397\n",
            "Validation loss: 0.4137, Validation accuracy: 0.8618\n",
            "\n",
            "Epoch 40:\n",
            "Training loss: 0.1805, Training accuracy: 0.9394\n",
            "Validation loss: 0.3881, Validation accuracy: 0.8715\n",
            "\n",
            "Epoch 41:\n",
            "Training loss: 0.1796, Training accuracy: 0.9405\n",
            "Validation loss: 0.4078, Validation accuracy: 0.8651\n",
            "\n",
            "Epoch 42:\n",
            "Training loss: 0.1826, Training accuracy: 0.9378\n",
            "Validation loss: 0.4290, Validation accuracy: 0.8621\n",
            "\n",
            "Epoch 43:\n",
            "Training loss: 0.1781, Training accuracy: 0.9388\n",
            "Validation loss: 0.3985, Validation accuracy: 0.8680\n",
            "\n",
            "Epoch 44:\n",
            "Training loss: 0.1794, Training accuracy: 0.9396\n",
            "Validation loss: 0.3989, Validation accuracy: 0.8656\n",
            "\n",
            "Epoch 45:\n",
            "Training loss: 0.1792, Training accuracy: 0.9388\n",
            "Validation loss: 0.3974, Validation accuracy: 0.8692\n",
            "\n",
            "Epoch 46:\n",
            "Training loss: 0.1767, Training accuracy: 0.9407\n",
            "Validation loss: 0.4120, Validation accuracy: 0.8669\n",
            "\n",
            "Epoch 47:\n",
            "Training loss: 0.1794, Training accuracy: 0.9407\n",
            "Validation loss: 0.4098, Validation accuracy: 0.8682\n",
            "\n",
            "Epoch 48:\n",
            "Training loss: 0.1798, Training accuracy: 0.9392\n",
            "Validation loss: 0.4252, Validation accuracy: 0.8620\n",
            "\n",
            "Epoch 49:\n",
            "Training loss: 0.1795, Training accuracy: 0.9385\n",
            "Validation loss: 0.3962, Validation accuracy: 0.8689\n",
            "\n",
            "Epoch 50:\n",
            "Training loss: 0.1755, Training accuracy: 0.9400\n",
            "Validation loss: 0.4041, Validation accuracy: 0.8715\n",
            "\n",
            "Epoch 51:\n",
            "Training loss: 0.1771, Training accuracy: 0.9398\n",
            "Validation loss: 0.4380, Validation accuracy: 0.8569\n",
            "\n",
            "Epoch 52:\n",
            "Training loss: 0.1793, Training accuracy: 0.9373\n",
            "Validation loss: 0.4129, Validation accuracy: 0.8641\n",
            "\n",
            "Epoch 53:\n",
            "Training loss: 0.1744, Training accuracy: 0.9417\n",
            "Validation loss: 0.4244, Validation accuracy: 0.8604\n",
            "\n",
            "Epoch 54:\n",
            "Training loss: 0.1723, Training accuracy: 0.9419\n",
            "Validation loss: 0.4429, Validation accuracy: 0.8653\n",
            "\n",
            "Epoch 55:\n",
            "Training loss: 0.1734, Training accuracy: 0.9422\n",
            "Validation loss: 0.4023, Validation accuracy: 0.8673\n",
            "\n",
            "Epoch 56:\n",
            "Training loss: 0.1706, Training accuracy: 0.9438\n",
            "Validation loss: 0.4304, Validation accuracy: 0.8608\n",
            "\n",
            "Epoch 57:\n",
            "Training loss: 0.1686, Training accuracy: 0.9439\n",
            "Validation loss: 0.4104, Validation accuracy: 0.8676\n",
            "\n",
            "Epoch 58:\n",
            "Training loss: 0.1711, Training accuracy: 0.9435\n",
            "Validation loss: 0.4274, Validation accuracy: 0.8617\n",
            "\n",
            "Epoch 59:\n",
            "Training loss: 0.1680, Training accuracy: 0.9427\n",
            "Validation loss: 0.4116, Validation accuracy: 0.8692\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 60:\n",
            "Training loss: 0.1226, Training accuracy: 0.9614\n",
            "Validation loss: 0.3650, Validation accuracy: 0.8829\n",
            "\n",
            "Epoch 61:\n",
            "Training loss: 0.1060, Training accuracy: 0.9681\n",
            "Validation loss: 0.3597, Validation accuracy: 0.8833\n",
            "\n",
            "Epoch 62:\n",
            "Training loss: 0.0946, Training accuracy: 0.9740\n",
            "Validation loss: 0.3471, Validation accuracy: 0.8839\n",
            "\n",
            "Epoch 63:\n",
            "Training loss: 0.0912, Training accuracy: 0.9742\n",
            "Validation loss: 0.3472, Validation accuracy: 0.8895\n",
            "\n",
            "Epoch 64:\n",
            "Training loss: 0.0864, Training accuracy: 0.9762\n",
            "Validation loss: 0.3496, Validation accuracy: 0.8830\n",
            "\n",
            "Epoch 65:\n",
            "Training loss: 0.0831, Training accuracy: 0.9778\n",
            "Validation loss: 0.3563, Validation accuracy: 0.8852\n",
            "\n",
            "Epoch 66:\n",
            "Training loss: 0.0796, Training accuracy: 0.9787\n",
            "Validation loss: 0.3624, Validation accuracy: 0.8841\n",
            "\n",
            "Epoch 67:\n",
            "Training loss: 0.0767, Training accuracy: 0.9796\n",
            "Validation loss: 0.3571, Validation accuracy: 0.8839\n",
            "\n",
            "Epoch 68:\n",
            "Training loss: 0.0776, Training accuracy: 0.9790\n",
            "Validation loss: 0.3673, Validation accuracy: 0.8810\n",
            "\n",
            "Epoch 69:\n",
            "Training loss: 0.0731, Training accuracy: 0.9812\n",
            "Validation loss: 0.3606, Validation accuracy: 0.8861\n",
            "\n",
            "Epoch 70:\n",
            "Training loss: 0.0728, Training accuracy: 0.9813\n",
            "Validation loss: 0.3637, Validation accuracy: 0.8803\n",
            "\n",
            "Epoch 71:\n",
            "Training loss: 0.0697, Training accuracy: 0.9815\n",
            "Validation loss: 0.3684, Validation accuracy: 0.8856\n",
            "\n",
            "Epoch 72:\n",
            "Training loss: 0.0682, Training accuracy: 0.9826\n",
            "Validation loss: 0.3535, Validation accuracy: 0.8896\n",
            "\n",
            "Epoch 73:\n",
            "Training loss: 0.0677, Training accuracy: 0.9830\n",
            "Validation loss: 0.3672, Validation accuracy: 0.8843\n",
            "\n",
            "Epoch 74:\n",
            "Training loss: 0.0685, Training accuracy: 0.9816\n",
            "Validation loss: 0.3694, Validation accuracy: 0.8844\n",
            "\n",
            "Epoch 75:\n",
            "Training loss: 0.0664, Training accuracy: 0.9832\n",
            "Validation loss: 0.3632, Validation accuracy: 0.8857\n",
            "\n",
            "Epoch 76:\n",
            "Training loss: 0.0650, Training accuracy: 0.9836\n",
            "Validation loss: 0.3635, Validation accuracy: 0.8867\n",
            "\n",
            "Epoch 77:\n",
            "Training loss: 0.0660, Training accuracy: 0.9833\n",
            "Validation loss: 0.3729, Validation accuracy: 0.8845\n",
            "\n",
            "Epoch 78:\n",
            "Training loss: 0.0640, Training accuracy: 0.9836\n",
            "Validation loss: 0.3702, Validation accuracy: 0.8832\n",
            "\n",
            "Epoch 79:\n",
            "Training loss: 0.0641, Training accuracy: 0.9840\n",
            "Validation loss: 0.3718, Validation accuracy: 0.8850\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 80:\n",
            "Training loss: 0.0557, Training accuracy: 0.9874\n",
            "Validation loss: 0.3611, Validation accuracy: 0.8869\n",
            "\n",
            "Epoch 81:\n",
            "Training loss: 0.0541, Training accuracy: 0.9879\n",
            "Validation loss: 0.3537, Validation accuracy: 0.8846\n",
            "\n",
            "Epoch 82:\n",
            "Training loss: 0.0542, Training accuracy: 0.9877\n",
            "Validation loss: 0.3618, Validation accuracy: 0.8816\n",
            "\n",
            "Epoch 83:\n",
            "Training loss: 0.0520, Training accuracy: 0.9892\n",
            "Validation loss: 0.3519, Validation accuracy: 0.8901\n",
            "\n",
            "Epoch 84:\n",
            "Training loss: 0.0527, Training accuracy: 0.9886\n",
            "Validation loss: 0.3542, Validation accuracy: 0.8895\n",
            "\n",
            "Epoch 85:\n",
            "Training loss: 0.0511, Training accuracy: 0.9893\n",
            "Validation loss: 0.3568, Validation accuracy: 0.8880\n",
            "\n",
            "Epoch 86:\n",
            "Training loss: 0.0519, Training accuracy: 0.9880\n",
            "Validation loss: 0.3574, Validation accuracy: 0.8859\n",
            "\n",
            "Epoch 87:\n",
            "Training loss: 0.0504, Training accuracy: 0.9894\n",
            "Validation loss: 0.3638, Validation accuracy: 0.8878\n",
            "\n",
            "Epoch 88:\n",
            "Training loss: 0.0499, Training accuracy: 0.9898\n",
            "Validation loss: 0.3505, Validation accuracy: 0.8903\n",
            "\n",
            "Epoch 89:\n",
            "Training loss: 0.0505, Training accuracy: 0.9895\n",
            "Validation loss: 0.3618, Validation accuracy: 0.8873\n",
            "\n",
            "Epoch 90:\n",
            "Training loss: 0.0497, Training accuracy: 0.9899\n",
            "Validation loss: 0.3752, Validation accuracy: 0.8862\n",
            "\n",
            "Epoch 91:\n",
            "Training loss: 0.0500, Training accuracy: 0.9896\n",
            "Validation loss: 0.3626, Validation accuracy: 0.8868\n",
            "\n",
            "Epoch 92:\n",
            "Training loss: 0.0480, Training accuracy: 0.9897\n",
            "Validation loss: 0.3526, Validation accuracy: 0.8878\n",
            "\n",
            "Epoch 93:\n",
            "Training loss: 0.0488, Training accuracy: 0.9897\n",
            "Validation loss: 0.3588, Validation accuracy: 0.8896\n",
            "\n",
            "Epoch 94:\n",
            "Training loss: 0.0488, Training accuracy: 0.9894\n",
            "Validation loss: 0.3665, Validation accuracy: 0.8865\n",
            "\n",
            "Epoch 95:\n",
            "Training loss: 0.0478, Training accuracy: 0.9901\n",
            "Validation loss: 0.3529, Validation accuracy: 0.8894\n",
            "\n",
            "Epoch 96:\n",
            "Training loss: 0.0473, Training accuracy: 0.9908\n",
            "Validation loss: 0.3629, Validation accuracy: 0.8885\n",
            "\n",
            "Epoch 97:\n",
            "Training loss: 0.0477, Training accuracy: 0.9901\n",
            "Validation loss: 0.3593, Validation accuracy: 0.8904\n",
            "\n",
            "Epoch 98:\n",
            "Training loss: 0.0455, Training accuracy: 0.9909\n",
            "Validation loss: 0.3555, Validation accuracy: 0.8855\n",
            "\n",
            "Epoch 99:\n",
            "Training loss: 0.0475, Training accuracy: 0.9901\n",
            "Validation loss: 0.3556, Validation accuracy: 0.8867\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.8904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.plot(valid_acc_block4_with_featuremap2,label ='object recognition accuracy of self-supervised model' ,color = 'yellow')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.yticks(np.arange(0.0, 1.1, 0.1))\n",
        "\n",
        "# Add a horizontal red line at y=0.919\n",
        "plt.axhline(y=0.919, color='red', linestyle='-', label='object recognition  Accuracy of supervised model')\n",
        "\n",
        "# Show legend\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "1EMQo_KzKHiM",
        "outputId": "fe639349-1145-46c9-f1b6-3525ada4b089"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**based on this, test the effect of amount label**"
      ],
      "metadata": {
        "id": "upZ7cjPsMAAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "class CIFAR10CustomDataset(Dataset):\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False, samples_per_class=None):\n",
        "        self.cifar10 = CIFAR10(root, train=train, transform=transform, target_transform=target_transform, download=download)\n",
        "        self.samples_per_class = samples_per_class\n",
        "        self.selected_indices = self.select_samples()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        original_index = self.selected_indices[index]\n",
        "        img, target = self.cifar10[original_index]\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.selected_indices)\n",
        "\n",
        "    def select_samples(self):\n",
        "        class_indices = [[] for _ in range(len(self.cifar10.classes))]\n",
        "\n",
        "        # Collect indices for each class\n",
        "        for i, (_, target) in enumerate(self.cifar10):\n",
        "            class_indices[target].append(i)\n",
        "\n",
        "        selected_indices = []\n",
        "\n",
        "        # Randomly select samples for each class\n",
        "        for indices in class_indices:\n",
        "            if self.samples_per_class is not None:\n",
        "                selected_indices.extend(np.random.choice(indices, size=self.samples_per_class, replace=False))\n",
        "            else:\n",
        "                selected_indices.extend(indices)\n",
        "\n",
        "        return selected_indices"
      ],
      "metadata": {
        "id": "T2qdHgkuMFO2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "k=1000"
      ],
      "metadata": {
        "id": "rFDnMimzP_pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(size = 32, padding=4),#random cropping\n",
        "    transforms.RandomHorizontalFlip() ,#random flip\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "samples_per_class = {'airplane': 1000, 'automobile': 1000, 'bird': 1000, 'cat': 1000, 'deer': 1000, 'dog': 1000,\n",
        "                     'frog': 1000, 'horse': 1000, 'ship': 1000, 'truck': 1000}\n",
        "\n",
        "# Create a custom dataset\n",
        "cifar10_1000 = CIFAR10CustomDataset(root='./data', train=True, transform=transform, download=True, samples_per_class=samples_per_class['airplane'])\n",
        "\n",
        "# Create a DataLoader for the custom dataset\n",
        "dataloader_1000 = DataLoader(cifar10_1000, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CyZ9fXWMK7g",
        "outputId": "c18d8ef4-c4e3-40ae-93bf-a04805760636"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conv-3 + 2nd block feature map\n",
        "out_feat_keys = ['conv2']\n",
        "classifier = Classifier({'num_classes': 10, 'nChannels': 192, 'cls_type': 'NIN_ConvBlock3'}).to(device)\n",
        "#hyperparameter setting\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "EPOCHS = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(classifier.parameters(), lr = INITIAL_LR,\n",
        "                      momentum = MOMENTUM,\n",
        "                      weight_decay=REG)"
      ],
      "metadata": {
        "id": "sHlz05xVMl90"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the folder where the trained model is saved\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "# start the training/validation process\n",
        "# the process should take about 5 minutes on a GTX 1070-Ti\n",
        "# if the code is written efficiently.\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY = 0.2\n",
        "valid_acc_block4_with_featuremap2_k1000 = []\n",
        "losslist = []\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    if i  == 30 or i == 60 or i == 80:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    classifier.train()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    # this help you compute the training accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0 # track training loss if you want\n",
        "\n",
        "    # Train the model for 1 epoch.\n",
        "    for batch_idx, (inputs, targets) in enumerate(dataloader_1000):\n",
        "        ####################################\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # inputs = inputs.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "        NIN_net_4block = NIN_net_4block.to(device)\n",
        "        feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # compute the output and loss\n",
        "        outputs = classifier(feature_map)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss\n",
        "        # zero the gradient\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        # apply gradient and update the weights\n",
        "        optimizer.step()\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "        _, predicted = outputs.max(1)  #make prediction based on the highest value\n",
        "        total_examples += targets.size(0) # in this case,128 for each batch\n",
        "        correct_examples += predicted.eq(targets).sum().item()\n",
        "        ####################################\n",
        "\n",
        "    avg_loss = train_loss / len(dataloader_1000)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "\n",
        "    classifier.eval()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    # this help you compute the validation accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 # again, track the validation loss if you want\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            ####################################\n",
        "            # your code here\n",
        "            # copy inputs to device\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "            NIN_net_4block = NIN_net_4block.to(device)\n",
        "\n",
        "\n",
        "            # compute the output and loss\n",
        "\n",
        "            outputs = classifier(feature_map)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # count the number of correctly predicted samples in the current batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += predicted.eq(targets).sum().item()\n",
        "            ####################################\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    valid_acc_block4_with_featuremap2_k1000.append(avg_acc)\n",
        "    losslist.append(avg_loss.item())\n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        #if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "        #    os.makedirs(CHECKPOINT_FOLDER)\n",
        "        #print(\"Saving ...\")\n",
        "\n",
        "        torch.save(classifier.state_dict(), 'classifier_conv3_NIN_with_featuremap2_k1000.pth')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4K0bSMPMvPk",
        "outputId": "edb2f2ee-63e4-4ef7-fed7-d9ee14350a25"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 1.0727, Training accuracy: 0.6300\n",
            "Validation loss: 1.1432, Validation accuracy: 0.6209\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 0.7129, Training accuracy: 0.7544\n",
            "Validation loss: 0.8819, Validation accuracy: 0.6934\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 0.6088, Training accuracy: 0.7880\n",
            "Validation loss: 0.7670, Validation accuracy: 0.7349\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 0.5581, Training accuracy: 0.8034\n",
            "Validation loss: 0.6889, Validation accuracy: 0.7591\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 0.5270, Training accuracy: 0.8167\n",
            "Validation loss: 0.6650, Validation accuracy: 0.7677\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 0.4652, Training accuracy: 0.8408\n",
            "Validation loss: 0.6417, Validation accuracy: 0.7834\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 0.4569, Training accuracy: 0.8418\n",
            "Validation loss: 0.6900, Validation accuracy: 0.7724\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 0.4371, Training accuracy: 0.8495\n",
            "Validation loss: 0.6969, Validation accuracy: 0.7571\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 0.4331, Training accuracy: 0.8511\n",
            "Validation loss: 0.7239, Validation accuracy: 0.7651\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 0.3899, Training accuracy: 0.8647\n",
            "Validation loss: 0.7436, Validation accuracy: 0.7510\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 0.3556, Training accuracy: 0.8745\n",
            "Validation loss: 0.7477, Validation accuracy: 0.7614\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 0.3466, Training accuracy: 0.8804\n",
            "Validation loss: 0.7593, Validation accuracy: 0.7591\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 0.3320, Training accuracy: 0.8849\n",
            "Validation loss: 0.6244, Validation accuracy: 0.7915\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 0.3266, Training accuracy: 0.8863\n",
            "Validation loss: 0.6566, Validation accuracy: 0.7843\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 0.3042, Training accuracy: 0.8972\n",
            "Validation loss: 0.6819, Validation accuracy: 0.7768\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 0.3300, Training accuracy: 0.8893\n",
            "Validation loss: 0.6286, Validation accuracy: 0.7956\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 0.2845, Training accuracy: 0.9003\n",
            "Validation loss: 0.7485, Validation accuracy: 0.7657\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 0.2740, Training accuracy: 0.9077\n",
            "Validation loss: 0.6988, Validation accuracy: 0.7722\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 0.2679, Training accuracy: 0.9074\n",
            "Validation loss: 0.6353, Validation accuracy: 0.7973\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 0.2573, Training accuracy: 0.9134\n",
            "Validation loss: 0.7101, Validation accuracy: 0.7789\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 0.2616, Training accuracy: 0.9101\n",
            "Validation loss: 0.6664, Validation accuracy: 0.7895\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 0.2396, Training accuracy: 0.9201\n",
            "Validation loss: 0.6886, Validation accuracy: 0.7787\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 0.2328, Training accuracy: 0.9191\n",
            "Validation loss: 0.7112, Validation accuracy: 0.7802\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 0.2530, Training accuracy: 0.9142\n",
            "Validation loss: 0.6801, Validation accuracy: 0.7956\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 0.2506, Training accuracy: 0.9175\n",
            "Validation loss: 0.6574, Validation accuracy: 0.7970\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 0.2269, Training accuracy: 0.9213\n",
            "Validation loss: 0.7510, Validation accuracy: 0.7777\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 0.1920, Training accuracy: 0.9350\n",
            "Validation loss: 0.7518, Validation accuracy: 0.7752\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 0.1954, Training accuracy: 0.9356\n",
            "Validation loss: 0.9035, Validation accuracy: 0.7414\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 0.2267, Training accuracy: 0.9260\n",
            "Validation loss: 0.7188, Validation accuracy: 0.7923\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 0.2220, Training accuracy: 0.9258\n",
            "Validation loss: 0.7485, Validation accuracy: 0.7745\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 30:\n",
            "Training loss: 0.1431, Training accuracy: 0.9573\n",
            "Validation loss: 0.5546, Validation accuracy: 0.8262\n",
            "\n",
            "Epoch 31:\n",
            "Training loss: 0.1003, Training accuracy: 0.9727\n",
            "Validation loss: 0.5478, Validation accuracy: 0.8277\n",
            "\n",
            "Epoch 32:\n",
            "Training loss: 0.0817, Training accuracy: 0.9797\n",
            "Validation loss: 0.5594, Validation accuracy: 0.8217\n",
            "\n",
            "Epoch 33:\n",
            "Training loss: 0.0732, Training accuracy: 0.9846\n",
            "Validation loss: 0.5533, Validation accuracy: 0.8255\n",
            "\n",
            "Epoch 34:\n",
            "Training loss: 0.0712, Training accuracy: 0.9840\n",
            "Validation loss: 0.5590, Validation accuracy: 0.8249\n",
            "\n",
            "Epoch 35:\n",
            "Training loss: 0.0671, Training accuracy: 0.9858\n",
            "Validation loss: 0.5591, Validation accuracy: 0.8279\n",
            "\n",
            "Epoch 36:\n",
            "Training loss: 0.0603, Training accuracy: 0.9891\n",
            "Validation loss: 0.5747, Validation accuracy: 0.8272\n",
            "\n",
            "Epoch 37:\n",
            "Training loss: 0.0600, Training accuracy: 0.9879\n",
            "Validation loss: 0.5832, Validation accuracy: 0.8257\n",
            "\n",
            "Epoch 38:\n",
            "Training loss: 0.0493, Training accuracy: 0.9925\n",
            "Validation loss: 0.5778, Validation accuracy: 0.8229\n",
            "\n",
            "Epoch 39:\n",
            "Training loss: 0.0503, Training accuracy: 0.9900\n",
            "Validation loss: 0.5763, Validation accuracy: 0.8244\n",
            "\n",
            "Epoch 40:\n",
            "Training loss: 0.0487, Training accuracy: 0.9922\n",
            "Validation loss: 0.5571, Validation accuracy: 0.8318\n",
            "\n",
            "Epoch 41:\n",
            "Training loss: 0.0436, Training accuracy: 0.9921\n",
            "Validation loss: 0.5854, Validation accuracy: 0.8237\n",
            "\n",
            "Epoch 42:\n",
            "Training loss: 0.0450, Training accuracy: 0.9910\n",
            "Validation loss: 0.5816, Validation accuracy: 0.8282\n",
            "\n",
            "Epoch 43:\n",
            "Training loss: 0.0408, Training accuracy: 0.9930\n",
            "Validation loss: 0.5613, Validation accuracy: 0.8310\n",
            "\n",
            "Epoch 44:\n",
            "Training loss: 0.0395, Training accuracy: 0.9944\n",
            "Validation loss: 0.5769, Validation accuracy: 0.8297\n",
            "\n",
            "Epoch 45:\n",
            "Training loss: 0.0423, Training accuracy: 0.9934\n",
            "Validation loss: 0.5770, Validation accuracy: 0.8279\n",
            "\n",
            "Epoch 46:\n",
            "Training loss: 0.0377, Training accuracy: 0.9953\n",
            "Validation loss: 0.5876, Validation accuracy: 0.8265\n",
            "\n",
            "Epoch 47:\n",
            "Training loss: 0.0390, Training accuracy: 0.9940\n",
            "Validation loss: 0.5906, Validation accuracy: 0.8285\n",
            "\n",
            "Epoch 48:\n",
            "Training loss: 0.0357, Training accuracy: 0.9951\n",
            "Validation loss: 0.5784, Validation accuracy: 0.8300\n",
            "\n",
            "Epoch 49:\n",
            "Training loss: 0.0353, Training accuracy: 0.9944\n",
            "Validation loss: 0.5861, Validation accuracy: 0.8274\n",
            "\n",
            "Epoch 50:\n",
            "Training loss: 0.0336, Training accuracy: 0.9965\n",
            "Validation loss: 0.5762, Validation accuracy: 0.8268\n",
            "\n",
            "Epoch 51:\n",
            "Training loss: 0.0403, Training accuracy: 0.9931\n",
            "Validation loss: 0.5828, Validation accuracy: 0.8290\n",
            "\n",
            "Epoch 52:\n",
            "Training loss: 0.0327, Training accuracy: 0.9951\n",
            "Validation loss: 0.6132, Validation accuracy: 0.8267\n",
            "\n",
            "Epoch 53:\n",
            "Training loss: 0.0361, Training accuracy: 0.9950\n",
            "Validation loss: 0.5843, Validation accuracy: 0.8274\n",
            "\n",
            "Epoch 54:\n",
            "Training loss: 0.0383, Training accuracy: 0.9932\n",
            "Validation loss: 0.5952, Validation accuracy: 0.8192\n",
            "\n",
            "Epoch 55:\n",
            "Training loss: 0.0337, Training accuracy: 0.9939\n",
            "Validation loss: 0.5766, Validation accuracy: 0.8309\n",
            "\n",
            "Epoch 56:\n",
            "Training loss: 0.0312, Training accuracy: 0.9953\n",
            "Validation loss: 0.5983, Validation accuracy: 0.8276\n",
            "\n",
            "Epoch 57:\n",
            "Training loss: 0.0319, Training accuracy: 0.9951\n",
            "Validation loss: 0.5956, Validation accuracy: 0.8316\n",
            "\n",
            "Epoch 58:\n",
            "Training loss: 0.0294, Training accuracy: 0.9964\n",
            "Validation loss: 0.5875, Validation accuracy: 0.8316\n",
            "\n",
            "Epoch 59:\n",
            "Training loss: 0.0276, Training accuracy: 0.9969\n",
            "Validation loss: 0.5813, Validation accuracy: 0.8294\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 60:\n",
            "Training loss: 0.0248, Training accuracy: 0.9969\n",
            "Validation loss: 0.5845, Validation accuracy: 0.8303\n",
            "\n",
            "Epoch 61:\n",
            "Training loss: 0.0245, Training accuracy: 0.9981\n",
            "Validation loss: 0.5766, Validation accuracy: 0.8278\n",
            "\n",
            "Epoch 62:\n",
            "Training loss: 0.0244, Training accuracy: 0.9973\n",
            "Validation loss: 0.5753, Validation accuracy: 0.8322\n",
            "\n",
            "Epoch 63:\n",
            "Training loss: 0.0237, Training accuracy: 0.9979\n",
            "Validation loss: 0.5763, Validation accuracy: 0.8306\n",
            "\n",
            "Epoch 64:\n",
            "Training loss: 0.0224, Training accuracy: 0.9981\n",
            "Validation loss: 0.5637, Validation accuracy: 0.8322\n",
            "\n",
            "Epoch 65:\n",
            "Training loss: 0.0218, Training accuracy: 0.9984\n",
            "Validation loss: 0.5794, Validation accuracy: 0.8294\n",
            "\n",
            "Epoch 66:\n",
            "Training loss: 0.0214, Training accuracy: 0.9984\n",
            "Validation loss: 0.5599, Validation accuracy: 0.8344\n",
            "\n",
            "Epoch 67:\n",
            "Training loss: 0.0212, Training accuracy: 0.9986\n",
            "Validation loss: 0.5602, Validation accuracy: 0.8319\n",
            "\n",
            "Epoch 68:\n",
            "Training loss: 0.0225, Training accuracy: 0.9979\n",
            "Validation loss: 0.5756, Validation accuracy: 0.8336\n",
            "\n",
            "Epoch 69:\n",
            "Training loss: 0.0216, Training accuracy: 0.9980\n",
            "Validation loss: 0.5814, Validation accuracy: 0.8354\n",
            "\n",
            "Epoch 70:\n",
            "Training loss: 0.0224, Training accuracy: 0.9983\n",
            "Validation loss: 0.5774, Validation accuracy: 0.8304\n",
            "\n",
            "Epoch 71:\n",
            "Training loss: 0.0213, Training accuracy: 0.9984\n",
            "Validation loss: 0.5834, Validation accuracy: 0.8279\n",
            "\n",
            "Epoch 72:\n",
            "Training loss: 0.0215, Training accuracy: 0.9980\n",
            "Validation loss: 0.5682, Validation accuracy: 0.8336\n",
            "\n",
            "Epoch 73:\n",
            "Training loss: 0.0201, Training accuracy: 0.9986\n",
            "Validation loss: 0.5853, Validation accuracy: 0.8319\n",
            "\n",
            "Epoch 74:\n",
            "Training loss: 0.0214, Training accuracy: 0.9977\n",
            "Validation loss: 0.5833, Validation accuracy: 0.8286\n",
            "\n",
            "Epoch 75:\n",
            "Training loss: 0.0208, Training accuracy: 0.9979\n",
            "Validation loss: 0.5786, Validation accuracy: 0.8326\n",
            "\n",
            "Epoch 76:\n",
            "Training loss: 0.0188, Training accuracy: 0.9982\n",
            "Validation loss: 0.5900, Validation accuracy: 0.8287\n",
            "\n",
            "Epoch 77:\n",
            "Training loss: 0.0201, Training accuracy: 0.9986\n",
            "Validation loss: 0.5646, Validation accuracy: 0.8342\n",
            "\n",
            "Epoch 78:\n",
            "Training loss: 0.0188, Training accuracy: 0.9984\n",
            "Validation loss: 0.5636, Validation accuracy: 0.8348\n",
            "\n",
            "Epoch 79:\n",
            "Training loss: 0.0196, Training accuracy: 0.9982\n",
            "Validation loss: 0.5721, Validation accuracy: 0.8352\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 80:\n",
            "Training loss: 0.0204, Training accuracy: 0.9993\n",
            "Validation loss: 0.5732, Validation accuracy: 0.8298\n",
            "\n",
            "Epoch 81:\n",
            "Training loss: 0.0212, Training accuracy: 0.9982\n",
            "Validation loss: 0.5782, Validation accuracy: 0.8345\n",
            "\n",
            "Epoch 82:\n",
            "Training loss: 0.0214, Training accuracy: 0.9985\n",
            "Validation loss: 0.5820, Validation accuracy: 0.8308\n",
            "\n",
            "Epoch 83:\n",
            "Training loss: 0.0279, Training accuracy: 0.9978\n",
            "Validation loss: 0.5602, Validation accuracy: 0.8327\n",
            "\n",
            "Epoch 84:\n",
            "Training loss: 0.0190, Training accuracy: 0.9988\n",
            "Validation loss: 0.5699, Validation accuracy: 0.8322\n",
            "\n",
            "Epoch 85:\n",
            "Training loss: 0.0207, Training accuracy: 0.9985\n",
            "Validation loss: 0.5685, Validation accuracy: 0.8378\n",
            "\n",
            "Epoch 86:\n",
            "Training loss: 0.0200, Training accuracy: 0.9987\n",
            "Validation loss: 0.5721, Validation accuracy: 0.8333\n",
            "\n",
            "Epoch 87:\n",
            "Training loss: 0.0185, Training accuracy: 0.9988\n",
            "Validation loss: 0.5602, Validation accuracy: 0.8328\n",
            "\n",
            "Epoch 88:\n",
            "Training loss: 0.0204, Training accuracy: 0.9989\n",
            "Validation loss: 0.5785, Validation accuracy: 0.8294\n",
            "\n",
            "Epoch 89:\n",
            "Training loss: 0.0197, Training accuracy: 0.9992\n",
            "Validation loss: 0.5905, Validation accuracy: 0.8311\n",
            "\n",
            "Epoch 90:\n",
            "Training loss: 0.0225, Training accuracy: 0.9978\n",
            "Validation loss: 0.5912, Validation accuracy: 0.8304\n",
            "\n",
            "Epoch 91:\n",
            "Training loss: 0.0182, Training accuracy: 0.9994\n",
            "Validation loss: 0.5807, Validation accuracy: 0.8320\n",
            "\n",
            "Epoch 92:\n",
            "Training loss: 0.0195, Training accuracy: 0.9985\n",
            "Validation loss: 0.5842, Validation accuracy: 0.8301\n",
            "\n",
            "Epoch 93:\n",
            "Training loss: 0.0195, Training accuracy: 0.9989\n",
            "Validation loss: 0.5684, Validation accuracy: 0.8330\n",
            "\n",
            "Epoch 94:\n",
            "Training loss: 0.0195, Training accuracy: 0.9985\n",
            "Validation loss: 0.5862, Validation accuracy: 0.8274\n",
            "\n",
            "Epoch 95:\n",
            "Training loss: 0.0188, Training accuracy: 0.9985\n",
            "Validation loss: 0.5754, Validation accuracy: 0.8380\n",
            "\n",
            "Epoch 96:\n",
            "Training loss: 0.0185, Training accuracy: 0.9990\n",
            "Validation loss: 0.5715, Validation accuracy: 0.8354\n",
            "\n",
            "Epoch 97:\n",
            "Training loss: 0.0186, Training accuracy: 0.9991\n",
            "Validation loss: 0.5611, Validation accuracy: 0.8309\n",
            "\n",
            "Epoch 98:\n",
            "Training loss: 0.0192, Training accuracy: 0.9990\n",
            "Validation loss: 0.5850, Validation accuracy: 0.8276\n",
            "\n",
            "Epoch 99:\n",
            "Training loss: 0.0219, Training accuracy: 0.9980\n",
            "Validation loss: 0.5652, Validation accuracy: 0.8336\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.8380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "k=400"
      ],
      "metadata": {
        "id": "o3HVUBaVQD0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(size = 32, padding=4),#random cropping\n",
        "    transforms.RandomHorizontalFlip() ,#random flip\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "samples_per_class = {'airplane': 400, 'automobile': 400, 'bird': 400, 'cat': 400, 'deer': 400, 'dog': 400,\n",
        "                     'frog': 400, 'horse': 400, 'ship': 400, 'truck': 400}\n",
        "\n",
        "# Create a custom dataset\n",
        "cifar10_400 = CIFAR10CustomDataset(root='./data', train=True, transform=transform, download=True, samples_per_class=samples_per_class['airplane'])\n",
        "\n",
        "# Create a DataLoader for the custom dataset\n",
        "dataloader_400 = DataLoader(cifar10_400, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4CIeWonQCNv",
        "outputId": "42b2ec7d-0e03-4c06-f904-57234e4ef3eb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conv-3 + 2nd block feature map with k400\n",
        "out_feat_keys = ['conv2']\n",
        "classifier = Classifier({'num_classes': 10, 'nChannels': 192, 'cls_type': 'NIN_ConvBlock3'}).to(device)\n",
        "#hyperparameter setting\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "EPOCHS = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(classifier.parameters(), lr = INITIAL_LR,\n",
        "                      momentum = MOMENTUM,\n",
        "                      weight_decay=REG)"
      ],
      "metadata": {
        "id": "DCRawu49QR3D"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the folder where the trained model is saved\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "# start the training/validation process\n",
        "# the process should take about 5 minutes on a GTX 1070-Ti\n",
        "# if the code is written efficiently.\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY = 0.2\n",
        "valid_acc_block4_with_featuremap2_k400 = []\n",
        "losslist = []\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    if i  == 30 or i == 60 or i == 80:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    classifier.train()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    # this help you compute the training accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0 # track training loss if you want\n",
        "\n",
        "    # Train the model for 1 epoch.\n",
        "    for batch_idx, (inputs, targets) in enumerate(dataloader_400):\n",
        "        ####################################\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # inputs = inputs.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "        NIN_net_4block = NIN_net_4block.to(device)\n",
        "        feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # compute the output and loss\n",
        "        outputs = classifier(feature_map)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss\n",
        "        # zero the gradient\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        # apply gradient and update the weights\n",
        "        optimizer.step()\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "        _, predicted = outputs.max(1)  #make prediction based on the highest value\n",
        "        total_examples += targets.size(0) # in this case,128 for each batch\n",
        "        correct_examples += predicted.eq(targets).sum().item()\n",
        "        ####################################\n",
        "\n",
        "    avg_loss = train_loss / len(dataloader_400)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "\n",
        "    classifier.eval()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    # this help you compute the validation accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 # again, track the validation loss if you want\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            ####################################\n",
        "            # your code here\n",
        "            # copy inputs to device\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "            NIN_net_4block = NIN_net_4block.to(device)\n",
        "\n",
        "\n",
        "            # compute the output and loss\n",
        "\n",
        "            outputs = classifier(feature_map)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # count the number of correctly predicted samples in the current batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += predicted.eq(targets).sum().item()\n",
        "            ####################################\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    valid_acc_block4_with_featuremap2_k400.append(avg_acc)\n",
        "    losslist.append(avg_loss.item())\n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        #if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "        #    os.makedirs(CHECKPOINT_FOLDER)\n",
        "        #print(\"Saving ...\")\n",
        "\n",
        "        torch.save(classifier.state_dict(), 'classifier_conv3_NIN_with_featuremap2_k400.pth')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx6MY0_QQY-R",
        "outputId": "02550104-1d70-495c-b03d-4f0614fd7469"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 1.6727, Training accuracy: 0.5135\n",
            "Validation loss: 1.6525, Validation accuracy: 0.4276\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 0.8775, Training accuracy: 0.7057\n",
            "Validation loss: 1.0168, Validation accuracy: 0.6520\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 0.7564, Training accuracy: 0.7385\n",
            "Validation loss: 0.8177, Validation accuracy: 0.7205\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 0.6570, Training accuracy: 0.7772\n",
            "Validation loss: 0.8419, Validation accuracy: 0.7105\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 0.5944, Training accuracy: 0.7917\n",
            "Validation loss: 0.8072, Validation accuracy: 0.7249\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 0.5596, Training accuracy: 0.8087\n",
            "Validation loss: 0.7416, Validation accuracy: 0.7467\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 0.5038, Training accuracy: 0.8273\n",
            "Validation loss: 0.7634, Validation accuracy: 0.7409\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 0.4598, Training accuracy: 0.8347\n",
            "Validation loss: 0.8634, Validation accuracy: 0.7077\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 0.4312, Training accuracy: 0.8572\n",
            "Validation loss: 0.7494, Validation accuracy: 0.7491\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 0.4119, Training accuracy: 0.8540\n",
            "Validation loss: 0.8820, Validation accuracy: 0.7156\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 0.3794, Training accuracy: 0.8738\n",
            "Validation loss: 0.7415, Validation accuracy: 0.7490\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 0.3605, Training accuracy: 0.8730\n",
            "Validation loss: 0.7571, Validation accuracy: 0.7506\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 0.3476, Training accuracy: 0.8845\n",
            "Validation loss: 0.7253, Validation accuracy: 0.7532\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 0.3133, Training accuracy: 0.8912\n",
            "Validation loss: 0.8761, Validation accuracy: 0.7347\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 0.2710, Training accuracy: 0.9058\n",
            "Validation loss: 0.7698, Validation accuracy: 0.7543\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 0.2471, Training accuracy: 0.9210\n",
            "Validation loss: 0.8236, Validation accuracy: 0.7430\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 0.2410, Training accuracy: 0.9243\n",
            "Validation loss: 0.7768, Validation accuracy: 0.7564\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 0.2557, Training accuracy: 0.9147\n",
            "Validation loss: 0.9138, Validation accuracy: 0.7163\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 0.2233, Training accuracy: 0.9230\n",
            "Validation loss: 0.7774, Validation accuracy: 0.7552\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 0.1953, Training accuracy: 0.9350\n",
            "Validation loss: 0.9571, Validation accuracy: 0.7286\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 0.2142, Training accuracy: 0.9270\n",
            "Validation loss: 0.7970, Validation accuracy: 0.7573\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 0.1791, Training accuracy: 0.9453\n",
            "Validation loss: 0.8097, Validation accuracy: 0.7607\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 0.1750, Training accuracy: 0.9460\n",
            "Validation loss: 0.8605, Validation accuracy: 0.7369\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 0.1568, Training accuracy: 0.9487\n",
            "Validation loss: 0.9219, Validation accuracy: 0.7355\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 0.1566, Training accuracy: 0.9535\n",
            "Validation loss: 0.8502, Validation accuracy: 0.7524\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 0.1549, Training accuracy: 0.9533\n",
            "Validation loss: 0.8607, Validation accuracy: 0.7451\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 0.1299, Training accuracy: 0.9590\n",
            "Validation loss: 0.8201, Validation accuracy: 0.7594\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 0.1469, Training accuracy: 0.9567\n",
            "Validation loss: 0.8050, Validation accuracy: 0.7645\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 0.1379, Training accuracy: 0.9560\n",
            "Validation loss: 0.8125, Validation accuracy: 0.7656\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 0.1278, Training accuracy: 0.9655\n",
            "Validation loss: 0.8629, Validation accuracy: 0.7461\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 30:\n",
            "Training loss: 0.0951, Training accuracy: 0.9732\n",
            "Validation loss: 0.7074, Validation accuracy: 0.7885\n",
            "\n",
            "Epoch 31:\n",
            "Training loss: 0.0594, Training accuracy: 0.9860\n",
            "Validation loss: 0.6886, Validation accuracy: 0.7904\n",
            "\n",
            "Epoch 32:\n",
            "Training loss: 0.0499, Training accuracy: 0.9932\n",
            "Validation loss: 0.6849, Validation accuracy: 0.7934\n",
            "\n",
            "Epoch 33:\n",
            "Training loss: 0.0485, Training accuracy: 0.9912\n",
            "Validation loss: 0.7155, Validation accuracy: 0.7906\n",
            "\n",
            "Epoch 34:\n",
            "Training loss: 0.0469, Training accuracy: 0.9910\n",
            "Validation loss: 0.6792, Validation accuracy: 0.7967\n",
            "\n",
            "Epoch 35:\n",
            "Training loss: 0.0432, Training accuracy: 0.9942\n",
            "Validation loss: 0.6900, Validation accuracy: 0.7895\n",
            "\n",
            "Epoch 36:\n",
            "Training loss: 0.0395, Training accuracy: 0.9935\n",
            "Validation loss: 0.6969, Validation accuracy: 0.7925\n",
            "\n",
            "Epoch 37:\n",
            "Training loss: 0.0400, Training accuracy: 0.9940\n",
            "Validation loss: 0.6956, Validation accuracy: 0.7959\n",
            "\n",
            "Epoch 38:\n",
            "Training loss: 0.0393, Training accuracy: 0.9938\n",
            "Validation loss: 0.7040, Validation accuracy: 0.7958\n",
            "\n",
            "Epoch 39:\n",
            "Training loss: 0.0346, Training accuracy: 0.9960\n",
            "Validation loss: 0.7098, Validation accuracy: 0.7916\n",
            "\n",
            "Epoch 40:\n",
            "Training loss: 0.0344, Training accuracy: 0.9965\n",
            "Validation loss: 0.6840, Validation accuracy: 0.7945\n",
            "\n",
            "Epoch 41:\n",
            "Training loss: 0.0283, Training accuracy: 0.9970\n",
            "Validation loss: 0.6922, Validation accuracy: 0.7975\n",
            "\n",
            "Epoch 42:\n",
            "Training loss: 0.0265, Training accuracy: 0.9988\n",
            "Validation loss: 0.7022, Validation accuracy: 0.7916\n",
            "\n",
            "Epoch 43:\n",
            "Training loss: 0.0265, Training accuracy: 0.9990\n",
            "Validation loss: 0.6761, Validation accuracy: 0.8011\n",
            "\n",
            "Epoch 44:\n",
            "Training loss: 0.0263, Training accuracy: 0.9970\n",
            "Validation loss: 0.6932, Validation accuracy: 0.7995\n",
            "\n",
            "Epoch 45:\n",
            "Training loss: 0.0296, Training accuracy: 0.9972\n",
            "Validation loss: 0.6854, Validation accuracy: 0.7969\n",
            "\n",
            "Epoch 46:\n",
            "Training loss: 0.0271, Training accuracy: 0.9972\n",
            "Validation loss: 0.6927, Validation accuracy: 0.8011\n",
            "\n",
            "Epoch 47:\n",
            "Training loss: 0.0263, Training accuracy: 0.9980\n",
            "Validation loss: 0.6987, Validation accuracy: 0.7947\n",
            "\n",
            "Epoch 48:\n",
            "Training loss: 0.0236, Training accuracy: 0.9980\n",
            "Validation loss: 0.6898, Validation accuracy: 0.8009\n",
            "\n",
            "Epoch 49:\n",
            "Training loss: 0.0236, Training accuracy: 0.9982\n",
            "Validation loss: 0.6804, Validation accuracy: 0.8001\n",
            "\n",
            "Epoch 50:\n",
            "Training loss: 0.0239, Training accuracy: 0.9982\n",
            "Validation loss: 0.7133, Validation accuracy: 0.7953\n",
            "\n",
            "Epoch 51:\n",
            "Training loss: 0.0239, Training accuracy: 0.9980\n",
            "Validation loss: 0.6875, Validation accuracy: 0.7949\n",
            "\n",
            "Epoch 52:\n",
            "Training loss: 0.0228, Training accuracy: 0.9982\n",
            "Validation loss: 0.6883, Validation accuracy: 0.7992\n",
            "\n",
            "Epoch 53:\n",
            "Training loss: 0.0225, Training accuracy: 0.9985\n",
            "Validation loss: 0.6902, Validation accuracy: 0.7980\n",
            "\n",
            "Epoch 54:\n",
            "Training loss: 0.0211, Training accuracy: 0.9985\n",
            "Validation loss: 0.6931, Validation accuracy: 0.7986\n",
            "\n",
            "Epoch 55:\n",
            "Training loss: 0.0214, Training accuracy: 0.9992\n",
            "Validation loss: 0.6943, Validation accuracy: 0.8014\n",
            "\n",
            "Epoch 56:\n",
            "Training loss: 0.0226, Training accuracy: 0.9972\n",
            "Validation loss: 0.7038, Validation accuracy: 0.7954\n",
            "\n",
            "Epoch 57:\n",
            "Training loss: 0.0202, Training accuracy: 0.9990\n",
            "Validation loss: 0.6849, Validation accuracy: 0.8008\n",
            "\n",
            "Epoch 58:\n",
            "Training loss: 0.0209, Training accuracy: 0.9992\n",
            "Validation loss: 0.6994, Validation accuracy: 0.7972\n",
            "\n",
            "Epoch 59:\n",
            "Training loss: 0.0188, Training accuracy: 0.9992\n",
            "Validation loss: 0.7030, Validation accuracy: 0.8004\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 60:\n",
            "Training loss: 0.0195, Training accuracy: 0.9995\n",
            "Validation loss: 0.6983, Validation accuracy: 0.7967\n",
            "\n",
            "Epoch 61:\n",
            "Training loss: 0.0188, Training accuracy: 0.9995\n",
            "Validation loss: 0.6948, Validation accuracy: 0.7997\n",
            "\n",
            "Epoch 62:\n",
            "Training loss: 0.0176, Training accuracy: 0.9998\n",
            "Validation loss: 0.6890, Validation accuracy: 0.8006\n",
            "\n",
            "Epoch 63:\n",
            "Training loss: 0.0199, Training accuracy: 0.9988\n",
            "Validation loss: 0.6861, Validation accuracy: 0.8052\n",
            "\n",
            "Epoch 64:\n",
            "Training loss: 0.0178, Training accuracy: 0.9988\n",
            "Validation loss: 0.6928, Validation accuracy: 0.7996\n",
            "\n",
            "Epoch 65:\n",
            "Training loss: 0.0187, Training accuracy: 0.9992\n",
            "Validation loss: 0.6941, Validation accuracy: 0.7937\n",
            "\n",
            "Epoch 66:\n",
            "Training loss: 0.0186, Training accuracy: 0.9992\n",
            "Validation loss: 0.6984, Validation accuracy: 0.7979\n",
            "\n",
            "Epoch 67:\n",
            "Training loss: 0.0180, Training accuracy: 0.9990\n",
            "Validation loss: 0.6899, Validation accuracy: 0.8007\n",
            "\n",
            "Epoch 68:\n",
            "Training loss: 0.0209, Training accuracy: 0.9992\n",
            "Validation loss: 0.6923, Validation accuracy: 0.7981\n",
            "\n",
            "Epoch 69:\n",
            "Training loss: 0.0165, Training accuracy: 0.9995\n",
            "Validation loss: 0.6966, Validation accuracy: 0.7968\n",
            "\n",
            "Epoch 70:\n",
            "Training loss: 0.0195, Training accuracy: 0.9990\n",
            "Validation loss: 0.6913, Validation accuracy: 0.7969\n",
            "\n",
            "Epoch 71:\n",
            "Training loss: 0.0209, Training accuracy: 0.9988\n",
            "Validation loss: 0.6985, Validation accuracy: 0.8007\n",
            "\n",
            "Epoch 72:\n",
            "Training loss: 0.0203, Training accuracy: 0.9980\n",
            "Validation loss: 0.6841, Validation accuracy: 0.8018\n",
            "\n",
            "Epoch 73:\n",
            "Training loss: 0.0168, Training accuracy: 0.9995\n",
            "Validation loss: 0.6875, Validation accuracy: 0.8005\n",
            "\n",
            "Epoch 74:\n",
            "Training loss: 0.0171, Training accuracy: 0.9995\n",
            "Validation loss: 0.6915, Validation accuracy: 0.8010\n",
            "\n",
            "Epoch 75:\n",
            "Training loss: 0.0182, Training accuracy: 0.9992\n",
            "Validation loss: 0.6934, Validation accuracy: 0.8010\n",
            "\n",
            "Epoch 76:\n",
            "Training loss: 0.0197, Training accuracy: 0.9988\n",
            "Validation loss: 0.6722, Validation accuracy: 0.8052\n",
            "\n",
            "Epoch 77:\n",
            "Training loss: 0.0164, Training accuracy: 1.0000\n",
            "Validation loss: 0.6960, Validation accuracy: 0.8003\n",
            "\n",
            "Epoch 78:\n",
            "Training loss: 0.0169, Training accuracy: 0.9992\n",
            "Validation loss: 0.6834, Validation accuracy: 0.8025\n",
            "\n",
            "Epoch 79:\n",
            "Training loss: 0.0159, Training accuracy: 0.9998\n",
            "Validation loss: 0.6907, Validation accuracy: 0.7987\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 80:\n",
            "Training loss: 0.0158, Training accuracy: 1.0000\n",
            "Validation loss: 0.6963, Validation accuracy: 0.8046\n",
            "\n",
            "Epoch 81:\n",
            "Training loss: 0.0155, Training accuracy: 0.9992\n",
            "Validation loss: 0.6887, Validation accuracy: 0.8008\n",
            "\n",
            "Epoch 82:\n",
            "Training loss: 0.0176, Training accuracy: 0.9988\n",
            "Validation loss: 0.6755, Validation accuracy: 0.8025\n",
            "\n",
            "Epoch 83:\n",
            "Training loss: 0.0179, Training accuracy: 0.9990\n",
            "Validation loss: 0.6913, Validation accuracy: 0.7975\n",
            "\n",
            "Epoch 84:\n",
            "Training loss: 0.0153, Training accuracy: 0.9998\n",
            "Validation loss: 0.6958, Validation accuracy: 0.7947\n",
            "\n",
            "Epoch 85:\n",
            "Training loss: 0.0154, Training accuracy: 0.9995\n",
            "Validation loss: 0.6907, Validation accuracy: 0.7977\n",
            "\n",
            "Epoch 86:\n",
            "Training loss: 0.0172, Training accuracy: 0.9995\n",
            "Validation loss: 0.6985, Validation accuracy: 0.8011\n",
            "\n",
            "Epoch 87:\n",
            "Training loss: 0.0161, Training accuracy: 0.9992\n",
            "Validation loss: 0.6930, Validation accuracy: 0.7963\n",
            "\n",
            "Epoch 88:\n",
            "Training loss: 0.0171, Training accuracy: 0.9992\n",
            "Validation loss: 0.6922, Validation accuracy: 0.7970\n",
            "\n",
            "Epoch 89:\n",
            "Training loss: 0.0181, Training accuracy: 0.9998\n",
            "Validation loss: 0.6944, Validation accuracy: 0.7991\n",
            "\n",
            "Epoch 90:\n",
            "Training loss: 0.0179, Training accuracy: 0.9990\n",
            "Validation loss: 0.6779, Validation accuracy: 0.8027\n",
            "\n",
            "Epoch 91:\n",
            "Training loss: 0.0168, Training accuracy: 0.9990\n",
            "Validation loss: 0.6781, Validation accuracy: 0.8033\n",
            "\n",
            "Epoch 92:\n",
            "Training loss: 0.0174, Training accuracy: 0.9995\n",
            "Validation loss: 0.6900, Validation accuracy: 0.8031\n",
            "\n",
            "Epoch 93:\n",
            "Training loss: 0.0167, Training accuracy: 0.9995\n",
            "Validation loss: 0.6908, Validation accuracy: 0.7934\n",
            "\n",
            "Epoch 94:\n",
            "Training loss: 0.0165, Training accuracy: 1.0000\n",
            "Validation loss: 0.7031, Validation accuracy: 0.8009\n",
            "\n",
            "Epoch 95:\n",
            "Training loss: 0.0150, Training accuracy: 0.9998\n",
            "Validation loss: 0.6987, Validation accuracy: 0.7981\n",
            "\n",
            "Epoch 96:\n",
            "Training loss: 0.0161, Training accuracy: 0.9998\n",
            "Validation loss: 0.6954, Validation accuracy: 0.8002\n",
            "\n",
            "Epoch 97:\n",
            "Training loss: 0.0162, Training accuracy: 1.0000\n",
            "Validation loss: 0.6922, Validation accuracy: 0.7964\n",
            "\n",
            "Epoch 98:\n",
            "Training loss: 0.0202, Training accuracy: 0.9998\n",
            "Validation loss: 0.6855, Validation accuracy: 0.8035\n",
            "\n",
            "Epoch 99:\n",
            "Training loss: 0.0160, Training accuracy: 0.9992\n",
            "Validation loss: 0.6853, Validation accuracy: 0.7989\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.8052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "k=100"
      ],
      "metadata": {
        "id": "Y2JO273CSGHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(size = 32, padding=4),#random cropping\n",
        "    transforms.RandomHorizontalFlip() ,#random flip\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "samples_per_class = {'airplane': 100, 'automobile': 100, 'bird': 100, 'cat': 100, 'deer': 100, 'dog': 100,\n",
        "                     'frog': 100, 'horse': 100, 'ship': 100, 'truck': 100}\n",
        "\n",
        "# Create a custom dataset\n",
        "cifar10_100 = CIFAR10CustomDataset(root='./data', train=True, transform=transform, download=True, samples_per_class=samples_per_class['airplane'])\n",
        "\n",
        "# Create a DataLoader for the custom dataset\n",
        "dataloader_100 = DataLoader(cifar10_100, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYei_EHHSGSU",
        "outputId": "2e98b3e9-8dd0-4de8-e856-23dad0d122a8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conv-3 + 2nd block feature map with k100\n",
        "out_feat_keys = ['conv2']\n",
        "classifier = Classifier({'num_classes': 10, 'nChannels': 192, 'cls_type': 'NIN_ConvBlock3'}).to(device)\n",
        "#hyperparameter setting\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "EPOCHS = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(classifier.parameters(), lr = INITIAL_LR,\n",
        "                      momentum = MOMENTUM,\n",
        "                      weight_decay=REG)"
      ],
      "metadata": {
        "id": "CzC26E37SVLX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the folder where the trained model is saved\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "# start the training/validation process\n",
        "# the process should take about 5 minutes on a GTX 1070-Ti\n",
        "# if the code is written efficiently.\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY = 0.2\n",
        "valid_acc_block4_with_featuremap2_k100 = []\n",
        "losslist = []\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    if i  == 30 or i == 60 or i == 80:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    classifier.train()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    # this help you compute the training accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0 # track training loss if you want\n",
        "\n",
        "    # Train the model for 1 epoch.\n",
        "    for batch_idx, (inputs, targets) in enumerate(dataloader_100):\n",
        "        ####################################\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # inputs = inputs.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "        NIN_net_4block = NIN_net_4block.to(device)\n",
        "        feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # compute the output and loss\n",
        "        outputs = classifier(feature_map)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss\n",
        "        # zero the gradient\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        # apply gradient and update the weights\n",
        "        optimizer.step()\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "        _, predicted = outputs.max(1)  #make prediction based on the highest value\n",
        "        total_examples += targets.size(0) # in this case,128 for each batch\n",
        "        correct_examples += predicted.eq(targets).sum().item()\n",
        "        ####################################\n",
        "\n",
        "    avg_loss = train_loss / len(dataloader_100)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "\n",
        "    classifier.eval()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    # this help you compute the validation accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 # again, track the validation loss if you want\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            ####################################\n",
        "            # your code here\n",
        "            # copy inputs to device\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "            NIN_net_4block = NIN_net_4block.to(device)\n",
        "\n",
        "\n",
        "            # compute the output and loss\n",
        "\n",
        "            outputs = classifier(feature_map)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # count the number of correctly predicted samples in the current batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += predicted.eq(targets).sum().item()\n",
        "            ####################################\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    valid_acc_block4_with_featuremap2_k100.append(avg_acc)\n",
        "    losslist.append(avg_loss.item())\n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        #if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "        #    os.makedirs(CHECKPOINT_FOLDER)\n",
        "        #print(\"Saving ...\")\n",
        "\n",
        "        torch.save(classifier.state_dict(), 'classifier_conv3_NIN_with_featuremap2_k100.pth')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE-lE9_HSYr2",
        "outputId": "d302d752-bde7-4156-a7da-7082e66d6a96"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 2.3207, Training accuracy: 0.3590\n",
            "Validation loss: 3.1070, Validation accuracy: 0.1000\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 1.2600, Training accuracy: 0.6060\n",
            "Validation loss: 2.5582, Validation accuracy: 0.2579\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 0.9476, Training accuracy: 0.6620\n",
            "Validation loss: 2.4446, Validation accuracy: 0.3320\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 0.7019, Training accuracy: 0.7450\n",
            "Validation loss: 2.1296, Validation accuracy: 0.3663\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 0.5802, Training accuracy: 0.8000\n",
            "Validation loss: 1.7692, Validation accuracy: 0.4539\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 0.4925, Training accuracy: 0.8280\n",
            "Validation loss: 1.3422, Validation accuracy: 0.5531\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 0.4399, Training accuracy: 0.8490\n",
            "Validation loss: 1.0483, Validation accuracy: 0.6438\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 0.3901, Training accuracy: 0.8700\n",
            "Validation loss: 0.9482, Validation accuracy: 0.6713\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 0.3392, Training accuracy: 0.8930\n",
            "Validation loss: 0.9283, Validation accuracy: 0.6837\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 0.2737, Training accuracy: 0.9170\n",
            "Validation loss: 0.9438, Validation accuracy: 0.6848\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 0.2269, Training accuracy: 0.9420\n",
            "Validation loss: 0.9614, Validation accuracy: 0.6889\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 0.2072, Training accuracy: 0.9420\n",
            "Validation loss: 1.0374, Validation accuracy: 0.6721\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 0.1820, Training accuracy: 0.9440\n",
            "Validation loss: 1.0548, Validation accuracy: 0.6737\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 0.1623, Training accuracy: 0.9600\n",
            "Validation loss: 1.0108, Validation accuracy: 0.6814\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 0.1349, Training accuracy: 0.9700\n",
            "Validation loss: 1.0490, Validation accuracy: 0.6803\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 0.1262, Training accuracy: 0.9720\n",
            "Validation loss: 1.0791, Validation accuracy: 0.6770\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 0.1308, Training accuracy: 0.9620\n",
            "Validation loss: 1.0539, Validation accuracy: 0.6813\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 0.0994, Training accuracy: 0.9780\n",
            "Validation loss: 1.1823, Validation accuracy: 0.6636\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 0.1235, Training accuracy: 0.9600\n",
            "Validation loss: 1.0807, Validation accuracy: 0.6892\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 0.0837, Training accuracy: 0.9830\n",
            "Validation loss: 1.0530, Validation accuracy: 0.6905\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 0.0801, Training accuracy: 0.9840\n",
            "Validation loss: 1.0943, Validation accuracy: 0.6849\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 0.0781, Training accuracy: 0.9800\n",
            "Validation loss: 1.0725, Validation accuracy: 0.6909\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 0.0582, Training accuracy: 0.9920\n",
            "Validation loss: 1.0743, Validation accuracy: 0.6953\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 0.0625, Training accuracy: 0.9910\n",
            "Validation loss: 1.0970, Validation accuracy: 0.6944\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 0.0482, Training accuracy: 0.9910\n",
            "Validation loss: 1.0302, Validation accuracy: 0.7010\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 0.0470, Training accuracy: 0.9920\n",
            "Validation loss: 1.0424, Validation accuracy: 0.6961\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 0.0533, Training accuracy: 0.9860\n",
            "Validation loss: 1.1454, Validation accuracy: 0.6887\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 0.0559, Training accuracy: 0.9880\n",
            "Validation loss: 1.1055, Validation accuracy: 0.6870\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 0.0458, Training accuracy: 0.9900\n",
            "Validation loss: 1.2340, Validation accuracy: 0.6593\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 0.0460, Training accuracy: 0.9910\n",
            "Validation loss: 1.0509, Validation accuracy: 0.7015\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 30:\n",
            "Training loss: 0.0335, Training accuracy: 0.9950\n",
            "Validation loss: 1.0609, Validation accuracy: 0.7024\n",
            "\n",
            "Epoch 31:\n",
            "Training loss: 0.0348, Training accuracy: 0.9980\n",
            "Validation loss: 1.0563, Validation accuracy: 0.7030\n",
            "\n",
            "Epoch 32:\n",
            "Training loss: 0.0248, Training accuracy: 0.9990\n",
            "Validation loss: 1.0669, Validation accuracy: 0.7070\n",
            "\n",
            "Epoch 33:\n",
            "Training loss: 0.0292, Training accuracy: 0.9950\n",
            "Validation loss: 1.0416, Validation accuracy: 0.7117\n",
            "\n",
            "Epoch 34:\n",
            "Training loss: 0.0247, Training accuracy: 0.9990\n",
            "Validation loss: 1.0286, Validation accuracy: 0.7102\n",
            "\n",
            "Epoch 35:\n",
            "Training loss: 0.0219, Training accuracy: 0.9980\n",
            "Validation loss: 1.0326, Validation accuracy: 0.7077\n",
            "\n",
            "Epoch 36:\n",
            "Training loss: 0.0225, Training accuracy: 0.9970\n",
            "Validation loss: 1.0488, Validation accuracy: 0.7094\n",
            "\n",
            "Epoch 37:\n",
            "Training loss: 0.0198, Training accuracy: 0.9990\n",
            "Validation loss: 1.0303, Validation accuracy: 0.7086\n",
            "\n",
            "Epoch 38:\n",
            "Training loss: 0.0201, Training accuracy: 1.0000\n",
            "Validation loss: 1.0191, Validation accuracy: 0.7112\n",
            "\n",
            "Epoch 39:\n",
            "Training loss: 0.0230, Training accuracy: 1.0000\n",
            "Validation loss: 1.0164, Validation accuracy: 0.7147\n",
            "\n",
            "Epoch 40:\n",
            "Training loss: 0.0241, Training accuracy: 0.9980\n",
            "Validation loss: 1.0415, Validation accuracy: 0.7103\n",
            "\n",
            "Epoch 41:\n",
            "Training loss: 0.0204, Training accuracy: 0.9990\n",
            "Validation loss: 1.0410, Validation accuracy: 0.7099\n",
            "\n",
            "Epoch 42:\n",
            "Training loss: 0.0175, Training accuracy: 1.0000\n",
            "Validation loss: 1.0305, Validation accuracy: 0.7113\n",
            "\n",
            "Epoch 43:\n",
            "Training loss: 0.0219, Training accuracy: 0.9990\n",
            "Validation loss: 1.0234, Validation accuracy: 0.7157\n",
            "\n",
            "Epoch 44:\n",
            "Training loss: 0.0184, Training accuracy: 0.9980\n",
            "Validation loss: 1.0204, Validation accuracy: 0.7134\n",
            "\n",
            "Epoch 45:\n",
            "Training loss: 0.0185, Training accuracy: 0.9990\n",
            "Validation loss: 1.0139, Validation accuracy: 0.7116\n",
            "\n",
            "Epoch 46:\n",
            "Training loss: 0.0175, Training accuracy: 1.0000\n",
            "Validation loss: 1.0072, Validation accuracy: 0.7132\n",
            "\n",
            "Epoch 47:\n",
            "Training loss: 0.0158, Training accuracy: 1.0000\n",
            "Validation loss: 1.0082, Validation accuracy: 0.7141\n",
            "\n",
            "Epoch 48:\n",
            "Training loss: 0.0172, Training accuracy: 0.9990\n",
            "Validation loss: 1.0251, Validation accuracy: 0.7126\n",
            "\n",
            "Epoch 49:\n",
            "Training loss: 0.0153, Training accuracy: 1.0000\n",
            "Validation loss: 1.0187, Validation accuracy: 0.7138\n",
            "\n",
            "Epoch 50:\n",
            "Training loss: 0.0151, Training accuracy: 0.9990\n",
            "Validation loss: 0.9933, Validation accuracy: 0.7179\n",
            "\n",
            "Epoch 51:\n",
            "Training loss: 0.0194, Training accuracy: 0.9980\n",
            "Validation loss: 1.0231, Validation accuracy: 0.7102\n",
            "\n",
            "Epoch 52:\n",
            "Training loss: 0.0135, Training accuracy: 1.0000\n",
            "Validation loss: 0.9990, Validation accuracy: 0.7180\n",
            "\n",
            "Epoch 53:\n",
            "Training loss: 0.0133, Training accuracy: 0.9990\n",
            "Validation loss: 1.0094, Validation accuracy: 0.7154\n",
            "\n",
            "Epoch 54:\n",
            "Training loss: 0.0174, Training accuracy: 0.9990\n",
            "Validation loss: 1.0040, Validation accuracy: 0.7197\n",
            "\n",
            "Epoch 55:\n",
            "Training loss: 0.0111, Training accuracy: 1.0000\n",
            "Validation loss: 1.0092, Validation accuracy: 0.7184\n",
            "\n",
            "Epoch 56:\n",
            "Training loss: 0.0143, Training accuracy: 0.9990\n",
            "Validation loss: 1.0122, Validation accuracy: 0.7179\n",
            "\n",
            "Epoch 57:\n",
            "Training loss: 0.0171, Training accuracy: 0.9980\n",
            "Validation loss: 1.0100, Validation accuracy: 0.7163\n",
            "\n",
            "Epoch 58:\n",
            "Training loss: 0.0167, Training accuracy: 1.0000\n",
            "Validation loss: 1.0149, Validation accuracy: 0.7136\n",
            "\n",
            "Epoch 59:\n",
            "Training loss: 0.0138, Training accuracy: 1.0000\n",
            "Validation loss: 1.0106, Validation accuracy: 0.7156\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 60:\n",
            "Training loss: 0.0136, Training accuracy: 0.9990\n",
            "Validation loss: 1.0138, Validation accuracy: 0.7185\n",
            "\n",
            "Epoch 61:\n",
            "Training loss: 0.0123, Training accuracy: 1.0000\n",
            "Validation loss: 1.0386, Validation accuracy: 0.7086\n",
            "\n",
            "Epoch 62:\n",
            "Training loss: 0.0133, Training accuracy: 0.9990\n",
            "Validation loss: 1.0162, Validation accuracy: 0.7187\n",
            "\n",
            "Epoch 63:\n",
            "Training loss: 0.0124, Training accuracy: 1.0000\n",
            "Validation loss: 1.0071, Validation accuracy: 0.7198\n",
            "\n",
            "Epoch 64:\n",
            "Training loss: 0.0127, Training accuracy: 1.0000\n",
            "Validation loss: 1.0172, Validation accuracy: 0.7139\n",
            "\n",
            "Epoch 65:\n",
            "Training loss: 0.0131, Training accuracy: 1.0000\n",
            "Validation loss: 1.0142, Validation accuracy: 0.7155\n",
            "\n",
            "Epoch 66:\n",
            "Training loss: 0.0137, Training accuracy: 1.0000\n",
            "Validation loss: 1.0162, Validation accuracy: 0.7186\n",
            "\n",
            "Epoch 67:\n",
            "Training loss: 0.0138, Training accuracy: 1.0000\n",
            "Validation loss: 1.0207, Validation accuracy: 0.7140\n",
            "\n",
            "Epoch 68:\n",
            "Training loss: 0.0134, Training accuracy: 1.0000\n",
            "Validation loss: 1.0058, Validation accuracy: 0.7189\n",
            "\n",
            "Epoch 69:\n",
            "Training loss: 0.0135, Training accuracy: 1.0000\n",
            "Validation loss: 1.0055, Validation accuracy: 0.7178\n",
            "\n",
            "Epoch 70:\n",
            "Training loss: 0.0126, Training accuracy: 1.0000\n",
            "Validation loss: 1.0181, Validation accuracy: 0.7168\n",
            "\n",
            "Epoch 71:\n",
            "Training loss: 0.0135, Training accuracy: 1.0000\n",
            "Validation loss: 1.0048, Validation accuracy: 0.7231\n",
            "\n",
            "Epoch 72:\n",
            "Training loss: 0.0142, Training accuracy: 1.0000\n",
            "Validation loss: 1.0105, Validation accuracy: 0.7176\n",
            "\n",
            "Epoch 73:\n",
            "Training loss: 0.0123, Training accuracy: 1.0000\n",
            "Validation loss: 1.0389, Validation accuracy: 0.7147\n",
            "\n",
            "Epoch 74:\n",
            "Training loss: 0.0153, Training accuracy: 1.0000\n",
            "Validation loss: 1.0289, Validation accuracy: 0.7151\n",
            "\n",
            "Epoch 75:\n",
            "Training loss: 0.0130, Training accuracy: 1.0000\n",
            "Validation loss: 1.0073, Validation accuracy: 0.7194\n",
            "\n",
            "Epoch 76:\n",
            "Training loss: 0.0127, Training accuracy: 1.0000\n",
            "Validation loss: 1.0084, Validation accuracy: 0.7172\n",
            "\n",
            "Epoch 77:\n",
            "Training loss: 0.0129, Training accuracy: 1.0000\n",
            "Validation loss: 1.0157, Validation accuracy: 0.7183\n",
            "\n",
            "Epoch 78:\n",
            "Training loss: 0.0145, Training accuracy: 1.0000\n",
            "Validation loss: 1.0246, Validation accuracy: 0.7171\n",
            "\n",
            "Epoch 79:\n",
            "Training loss: 0.0130, Training accuracy: 1.0000\n",
            "Validation loss: 1.0189, Validation accuracy: 0.7182\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 80:\n",
            "Training loss: 0.0134, Training accuracy: 1.0000\n",
            "Validation loss: 1.0249, Validation accuracy: 0.7178\n",
            "\n",
            "Epoch 81:\n",
            "Training loss: 0.0140, Training accuracy: 1.0000\n",
            "Validation loss: 1.0278, Validation accuracy: 0.7147\n",
            "\n",
            "Epoch 82:\n",
            "Training loss: 0.0123, Training accuracy: 1.0000\n",
            "Validation loss: 1.0042, Validation accuracy: 0.7151\n",
            "\n",
            "Epoch 83:\n",
            "Training loss: 0.0146, Training accuracy: 1.0000\n",
            "Validation loss: 1.0160, Validation accuracy: 0.7187\n",
            "\n",
            "Epoch 84:\n",
            "Training loss: 0.0158, Training accuracy: 1.0000\n",
            "Validation loss: 1.0017, Validation accuracy: 0.7220\n",
            "\n",
            "Epoch 85:\n",
            "Training loss: 0.0125, Training accuracy: 1.0000\n",
            "Validation loss: 1.0159, Validation accuracy: 0.7198\n",
            "\n",
            "Epoch 86:\n",
            "Training loss: 0.0120, Training accuracy: 1.0000\n",
            "Validation loss: 1.0164, Validation accuracy: 0.7165\n",
            "\n",
            "Epoch 87:\n",
            "Training loss: 0.0125, Training accuracy: 0.9990\n",
            "Validation loss: 1.0165, Validation accuracy: 0.7168\n",
            "\n",
            "Epoch 88:\n",
            "Training loss: 0.0141, Training accuracy: 1.0000\n",
            "Validation loss: 1.0149, Validation accuracy: 0.7133\n",
            "\n",
            "Epoch 89:\n",
            "Training loss: 0.0138, Training accuracy: 1.0000\n",
            "Validation loss: 1.0123, Validation accuracy: 0.7204\n",
            "\n",
            "Epoch 90:\n",
            "Training loss: 0.0127, Training accuracy: 1.0000\n",
            "Validation loss: 1.0209, Validation accuracy: 0.7177\n",
            "\n",
            "Epoch 91:\n",
            "Training loss: 0.0128, Training accuracy: 1.0000\n",
            "Validation loss: 1.0283, Validation accuracy: 0.7144\n",
            "\n",
            "Epoch 92:\n",
            "Training loss: 0.0113, Training accuracy: 1.0000\n",
            "Validation loss: 1.0289, Validation accuracy: 0.7150\n",
            "\n",
            "Epoch 93:\n",
            "Training loss: 0.0129, Training accuracy: 1.0000\n",
            "Validation loss: 1.0279, Validation accuracy: 0.7159\n",
            "\n",
            "Epoch 94:\n",
            "Training loss: 0.0137, Training accuracy: 1.0000\n",
            "Validation loss: 1.0114, Validation accuracy: 0.7206\n",
            "\n",
            "Epoch 95:\n",
            "Training loss: 0.0150, Training accuracy: 0.9990\n",
            "Validation loss: 1.0132, Validation accuracy: 0.7158\n",
            "\n",
            "Epoch 96:\n",
            "Training loss: 0.0127, Training accuracy: 1.0000\n",
            "Validation loss: 1.0283, Validation accuracy: 0.7165\n",
            "\n",
            "Epoch 97:\n",
            "Training loss: 0.0130, Training accuracy: 1.0000\n",
            "Validation loss: 1.0261, Validation accuracy: 0.7154\n",
            "\n",
            "Epoch 98:\n",
            "Training loss: 0.0128, Training accuracy: 1.0000\n",
            "Validation loss: 1.0232, Validation accuracy: 0.7165\n",
            "\n",
            "Epoch 99:\n",
            "Training loss: 0.0142, Training accuracy: 1.0000\n",
            "Validation loss: 1.0065, Validation accuracy: 0.7180\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.7231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "k = 20"
      ],
      "metadata": {
        "id": "GkILqG2BUhxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(size = 32, padding=4),#random cropping\n",
        "    transforms.RandomHorizontalFlip() ,#random flip\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "samples_per_class = {'airplane': 20, 'automobile': 20, 'bird': 20, 'cat': 20, 'deer': 20, 'dog': 20,\n",
        "                     'frog': 20, 'horse': 20, 'ship': 20, 'truck': 20}\n",
        "\n",
        "# Create a custom dataset\n",
        "cifar10_20 = CIFAR10CustomDataset(root='./data', train=True, transform=transform, download=True, samples_per_class=samples_per_class['airplane'])\n",
        "\n",
        "# Create a DataLoader for the custom dataset\n",
        "dataloader_20 = DataLoader(cifar10_20, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GETz3nAUg_c",
        "outputId": "6c22795d-70df-4735-ca81-1327b64d8cc5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conv-3 + 2nd block feature map with k20\n",
        "out_feat_keys = ['conv2']\n",
        "classifier = Classifier({'num_classes': 10, 'nChannels': 192, 'cls_type': 'NIN_ConvBlock3'}).to(device)\n",
        "#hyperparameter setting\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "EPOCHS = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(classifier.parameters(), lr = INITIAL_LR,\n",
        "                      momentum = MOMENTUM,\n",
        "                      weight_decay=REG)"
      ],
      "metadata": {
        "id": "7QF_HvS6U14L"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the folder where the trained model is saved\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "# start the training/validation process\n",
        "# the process should take about 5 minutes on a GTX 1070-Ti\n",
        "# if the code is written efficiently.\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY = 0.2\n",
        "valid_acc_block4_with_featuremap2_k20 = []\n",
        "losslist = []\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    if i  == 30 or i == 60 or i == 80:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    classifier.train()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    # this help you compute the training accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0 # track training loss if you want\n",
        "\n",
        "    # Train the model for 1 epoch.\n",
        "    for batch_idx, (inputs, targets) in enumerate(dataloader_20):\n",
        "        ####################################\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # inputs = inputs.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "        NIN_net_4block = NIN_net_4block.to(device)\n",
        "        feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # compute the output and loss\n",
        "        outputs = classifier(feature_map)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss\n",
        "        # zero the gradient\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        # apply gradient and update the weights\n",
        "        optimizer.step()\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "        _, predicted = outputs.max(1)  #make prediction based on the highest value\n",
        "        total_examples += targets.size(0) # in this case,128 for each batch\n",
        "        correct_examples += predicted.eq(targets).sum().item()\n",
        "        ####################################\n",
        "\n",
        "    avg_loss = train_loss / len(dataloader_20)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "\n",
        "    classifier.eval()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    # this help you compute the validation accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 # again, track the validation loss if you want\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            ####################################\n",
        "            # your code here\n",
        "            # copy inputs to device\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            feature_map = NIN_net_4block(inputs, out_feat_keys=out_feat_keys)\n",
        "            NIN_net_4block = NIN_net_4block.to(device)\n",
        "\n",
        "\n",
        "            # compute the output and loss\n",
        "\n",
        "            outputs = classifier(feature_map)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # count the number of correctly predicted samples in the current batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += predicted.eq(targets).sum().item()\n",
        "            ####################################\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    valid_acc_block4_with_featuremap2_k20.append(avg_acc)\n",
        "    losslist.append(avg_loss.item())\n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        #if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "        #    os.makedirs(CHECKPOINT_FOLDER)\n",
        "        #print(\"Saving ...\")\n",
        "\n",
        "        torch.save(classifier.state_dict(), 'classifier_conv3_NIN_with_featuremap2_k20.pth')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrdGCUNSU6S-",
        "outputId": "35dd1932-8488-47e2-b83a-de71e008af35"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 3.1175, Training accuracy: 0.1300\n",
            "Validation loss: 2.3592, Validation accuracy: 0.1678\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 1.4704, Training accuracy: 0.5500\n",
            "Validation loss: 2.5421, Validation accuracy: 0.1000\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 1.1630, Training accuracy: 0.6500\n",
            "Validation loss: 2.8609, Validation accuracy: 0.1000\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 0.9320, Training accuracy: 0.7200\n",
            "Validation loss: 3.2419, Validation accuracy: 0.1000\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 0.6587, Training accuracy: 0.8000\n",
            "Validation loss: 3.3983, Validation accuracy: 0.1000\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 0.4423, Training accuracy: 0.8450\n",
            "Validation loss: 3.5012, Validation accuracy: 0.1000\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 0.3489, Training accuracy: 0.9150\n",
            "Validation loss: 3.5918, Validation accuracy: 0.1002\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 0.3245, Training accuracy: 0.9100\n",
            "Validation loss: 3.6828, Validation accuracy: 0.1031\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 0.2249, Training accuracy: 0.9300\n",
            "Validation loss: 3.6115, Validation accuracy: 0.1237\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 0.1492, Training accuracy: 0.9800\n",
            "Validation loss: 3.4696, Validation accuracy: 0.1482\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 0.1681, Training accuracy: 0.9450\n",
            "Validation loss: 3.3501, Validation accuracy: 0.1600\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 0.1125, Training accuracy: 0.9800\n",
            "Validation loss: 3.2571, Validation accuracy: 0.1690\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 0.1020, Training accuracy: 0.9850\n",
            "Validation loss: 3.1076, Validation accuracy: 0.1751\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 0.1193, Training accuracy: 0.9850\n",
            "Validation loss: 2.9392, Validation accuracy: 0.1921\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 0.0792, Training accuracy: 0.9850\n",
            "Validation loss: 2.7705, Validation accuracy: 0.2235\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 0.0677, Training accuracy: 0.9850\n",
            "Validation loss: 2.6265, Validation accuracy: 0.2498\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 0.0632, Training accuracy: 0.9850\n",
            "Validation loss: 2.5012, Validation accuracy: 0.2867\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 0.0327, Training accuracy: 1.0000\n",
            "Validation loss: 2.4080, Validation accuracy: 0.3160\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 0.0327, Training accuracy: 0.9950\n",
            "Validation loss: 2.3182, Validation accuracy: 0.3413\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 0.0365, Training accuracy: 0.9950\n",
            "Validation loss: 2.2277, Validation accuracy: 0.3729\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 0.0370, Training accuracy: 0.9950\n",
            "Validation loss: 2.1027, Validation accuracy: 0.4029\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 0.0262, Training accuracy: 0.9950\n",
            "Validation loss: 1.9765, Validation accuracy: 0.4366\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 0.0304, Training accuracy: 0.9950\n",
            "Validation loss: 1.8639, Validation accuracy: 0.4652\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 0.0218, Training accuracy: 0.9950\n",
            "Validation loss: 1.7634, Validation accuracy: 0.4902\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 0.0241, Training accuracy: 1.0000\n",
            "Validation loss: 1.7134, Validation accuracy: 0.5076\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 0.0219, Training accuracy: 0.9950\n",
            "Validation loss: 1.6783, Validation accuracy: 0.5210\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 0.0164, Training accuracy: 1.0000\n",
            "Validation loss: 1.6589, Validation accuracy: 0.5305\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 0.0172, Training accuracy: 1.0000\n",
            "Validation loss: 1.6701, Validation accuracy: 0.5372\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 0.0158, Training accuracy: 1.0000\n",
            "Validation loss: 1.6725, Validation accuracy: 0.5458\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 0.0118, Training accuracy: 1.0000\n",
            "Validation loss: 1.6713, Validation accuracy: 0.5530\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 30:\n",
            "Training loss: 0.0148, Training accuracy: 0.9950\n",
            "Validation loss: 1.6792, Validation accuracy: 0.5539\n",
            "\n",
            "Epoch 31:\n",
            "Training loss: 0.0105, Training accuracy: 1.0000\n",
            "Validation loss: 1.6737, Validation accuracy: 0.5627\n",
            "\n",
            "Epoch 32:\n",
            "Training loss: 0.0207, Training accuracy: 0.9950\n",
            "Validation loss: 1.6728, Validation accuracy: 0.5636\n",
            "\n",
            "Epoch 33:\n",
            "Training loss: 0.0204, Training accuracy: 1.0000\n",
            "Validation loss: 1.6800, Validation accuracy: 0.5646\n",
            "\n",
            "Epoch 34:\n",
            "Training loss: 0.0155, Training accuracy: 1.0000\n",
            "Validation loss: 1.7192, Validation accuracy: 0.5642\n",
            "\n",
            "Epoch 35:\n",
            "Training loss: 0.0176, Training accuracy: 1.0000\n",
            "Validation loss: 1.7022, Validation accuracy: 0.5700\n",
            "\n",
            "Epoch 36:\n",
            "Training loss: 0.0140, Training accuracy: 1.0000\n",
            "Validation loss: 1.7001, Validation accuracy: 0.5690\n",
            "\n",
            "Epoch 37:\n",
            "Training loss: 0.0111, Training accuracy: 1.0000\n",
            "Validation loss: 1.7162, Validation accuracy: 0.5691\n",
            "\n",
            "Epoch 38:\n",
            "Training loss: 0.0138, Training accuracy: 1.0000\n",
            "Validation loss: 1.7216, Validation accuracy: 0.5665\n",
            "\n",
            "Epoch 39:\n",
            "Training loss: 0.0136, Training accuracy: 1.0000\n",
            "Validation loss: 1.7247, Validation accuracy: 0.5739\n",
            "\n",
            "Epoch 40:\n",
            "Training loss: 0.0233, Training accuracy: 0.9950\n",
            "Validation loss: 1.7213, Validation accuracy: 0.5671\n",
            "\n",
            "Epoch 41:\n",
            "Training loss: 0.0102, Training accuracy: 1.0000\n",
            "Validation loss: 1.7217, Validation accuracy: 0.5745\n",
            "\n",
            "Epoch 42:\n",
            "Training loss: 0.0110, Training accuracy: 1.0000\n",
            "Validation loss: 1.7196, Validation accuracy: 0.5739\n",
            "\n",
            "Epoch 43:\n",
            "Training loss: 0.0104, Training accuracy: 1.0000\n",
            "Validation loss: 1.7057, Validation accuracy: 0.5727\n",
            "\n",
            "Epoch 44:\n",
            "Training loss: 0.0130, Training accuracy: 1.0000\n",
            "Validation loss: 1.7125, Validation accuracy: 0.5720\n",
            "\n",
            "Epoch 45:\n",
            "Training loss: 0.0082, Training accuracy: 1.0000\n",
            "Validation loss: 1.7303, Validation accuracy: 0.5680\n",
            "\n",
            "Epoch 46:\n",
            "Training loss: 0.0136, Training accuracy: 1.0000\n",
            "Validation loss: 1.7214, Validation accuracy: 0.5748\n",
            "\n",
            "Epoch 47:\n",
            "Training loss: 0.0089, Training accuracy: 1.0000\n",
            "Validation loss: 1.7179, Validation accuracy: 0.5694\n",
            "\n",
            "Epoch 48:\n",
            "Training loss: 0.0087, Training accuracy: 1.0000\n",
            "Validation loss: 1.7028, Validation accuracy: 0.5788\n",
            "\n",
            "Epoch 49:\n",
            "Training loss: 0.0108, Training accuracy: 1.0000\n",
            "Validation loss: 1.7004, Validation accuracy: 0.5725\n",
            "\n",
            "Epoch 50:\n",
            "Training loss: 0.0084, Training accuracy: 1.0000\n",
            "Validation loss: 1.7344, Validation accuracy: 0.5724\n",
            "\n",
            "Epoch 51:\n",
            "Training loss: 0.0098, Training accuracy: 1.0000\n",
            "Validation loss: 1.7242, Validation accuracy: 0.5739\n",
            "\n",
            "Epoch 52:\n",
            "Training loss: 0.0098, Training accuracy: 1.0000\n",
            "Validation loss: 1.6878, Validation accuracy: 0.5812\n",
            "\n",
            "Epoch 53:\n",
            "Training loss: 0.0098, Training accuracy: 1.0000\n",
            "Validation loss: 1.7078, Validation accuracy: 0.5781\n",
            "\n",
            "Epoch 54:\n",
            "Training loss: 0.0062, Training accuracy: 1.0000\n",
            "Validation loss: 1.7080, Validation accuracy: 0.5758\n",
            "\n",
            "Epoch 55:\n",
            "Training loss: 0.0100, Training accuracy: 1.0000\n",
            "Validation loss: 1.7141, Validation accuracy: 0.5791\n",
            "\n",
            "Epoch 56:\n",
            "Training loss: 0.0076, Training accuracy: 1.0000\n",
            "Validation loss: 1.7073, Validation accuracy: 0.5750\n",
            "\n",
            "Epoch 57:\n",
            "Training loss: 0.0122, Training accuracy: 1.0000\n",
            "Validation loss: 1.7086, Validation accuracy: 0.5794\n",
            "\n",
            "Epoch 58:\n",
            "Training loss: 0.0068, Training accuracy: 1.0000\n",
            "Validation loss: 1.7018, Validation accuracy: 0.5795\n",
            "\n",
            "Epoch 59:\n",
            "Training loss: 0.0081, Training accuracy: 1.0000\n",
            "Validation loss: 1.7244, Validation accuracy: 0.5725\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 60:\n",
            "Training loss: 0.0121, Training accuracy: 1.0000\n",
            "Validation loss: 1.7098, Validation accuracy: 0.5751\n",
            "\n",
            "Epoch 61:\n",
            "Training loss: 0.0076, Training accuracy: 1.0000\n",
            "Validation loss: 1.7195, Validation accuracy: 0.5757\n",
            "\n",
            "Epoch 62:\n",
            "Training loss: 0.0113, Training accuracy: 1.0000\n",
            "Validation loss: 1.7151, Validation accuracy: 0.5754\n",
            "\n",
            "Epoch 63:\n",
            "Training loss: 0.0082, Training accuracy: 1.0000\n",
            "Validation loss: 1.7208, Validation accuracy: 0.5789\n",
            "\n",
            "Epoch 64:\n",
            "Training loss: 0.0075, Training accuracy: 1.0000\n",
            "Validation loss: 1.7163, Validation accuracy: 0.5774\n",
            "\n",
            "Epoch 65:\n",
            "Training loss: 0.0109, Training accuracy: 1.0000\n",
            "Validation loss: 1.7234, Validation accuracy: 0.5719\n",
            "\n",
            "Epoch 66:\n",
            "Training loss: 0.0097, Training accuracy: 1.0000\n",
            "Validation loss: 1.7165, Validation accuracy: 0.5792\n",
            "\n",
            "Epoch 67:\n",
            "Training loss: 0.0058, Training accuracy: 1.0000\n",
            "Validation loss: 1.7303, Validation accuracy: 0.5771\n",
            "\n",
            "Epoch 68:\n",
            "Training loss: 0.0069, Training accuracy: 1.0000\n",
            "Validation loss: 1.6908, Validation accuracy: 0.5806\n",
            "\n",
            "Epoch 69:\n",
            "Training loss: 0.0110, Training accuracy: 1.0000\n",
            "Validation loss: 1.7342, Validation accuracy: 0.5755\n",
            "\n",
            "Epoch 70:\n",
            "Training loss: 0.0060, Training accuracy: 1.0000\n",
            "Validation loss: 1.7140, Validation accuracy: 0.5803\n",
            "\n",
            "Epoch 71:\n",
            "Training loss: 0.0074, Training accuracy: 1.0000\n",
            "Validation loss: 1.7278, Validation accuracy: 0.5744\n",
            "\n",
            "Epoch 72:\n",
            "Training loss: 0.0134, Training accuracy: 1.0000\n",
            "Validation loss: 1.7209, Validation accuracy: 0.5757\n",
            "\n",
            "Epoch 73:\n",
            "Training loss: 0.0082, Training accuracy: 1.0000\n",
            "Validation loss: 1.7276, Validation accuracy: 0.5746\n",
            "\n",
            "Epoch 74:\n",
            "Training loss: 0.0071, Training accuracy: 1.0000\n",
            "Validation loss: 1.7142, Validation accuracy: 0.5812\n",
            "\n",
            "Epoch 75:\n",
            "Training loss: 0.0098, Training accuracy: 1.0000\n",
            "Validation loss: 1.7180, Validation accuracy: 0.5764\n",
            "\n",
            "Epoch 76:\n",
            "Training loss: 0.0081, Training accuracy: 1.0000\n",
            "Validation loss: 1.7279, Validation accuracy: 0.5784\n",
            "\n",
            "Epoch 77:\n",
            "Training loss: 0.0172, Training accuracy: 0.9950\n",
            "Validation loss: 1.7140, Validation accuracy: 0.5789\n",
            "\n",
            "Epoch 78:\n",
            "Training loss: 0.0084, Training accuracy: 1.0000\n",
            "Validation loss: 1.7179, Validation accuracy: 0.5743\n",
            "\n",
            "Epoch 79:\n",
            "Training loss: 0.0121, Training accuracy: 1.0000\n",
            "Validation loss: 1.7151, Validation accuracy: 0.5740\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 80:\n",
            "Training loss: 0.0082, Training accuracy: 1.0000\n",
            "Validation loss: 1.7343, Validation accuracy: 0.5710\n",
            "\n",
            "Epoch 81:\n",
            "Training loss: 0.0065, Training accuracy: 1.0000\n",
            "Validation loss: 1.7131, Validation accuracy: 0.5769\n",
            "\n",
            "Epoch 82:\n",
            "Training loss: 0.0052, Training accuracy: 1.0000\n",
            "Validation loss: 1.7456, Validation accuracy: 0.5757\n",
            "\n",
            "Epoch 83:\n",
            "Training loss: 0.0068, Training accuracy: 1.0000\n",
            "Validation loss: 1.7165, Validation accuracy: 0.5788\n",
            "\n",
            "Epoch 84:\n",
            "Training loss: 0.0129, Training accuracy: 1.0000\n",
            "Validation loss: 1.7291, Validation accuracy: 0.5751\n",
            "\n",
            "Epoch 85:\n",
            "Training loss: 0.0056, Training accuracy: 1.0000\n",
            "Validation loss: 1.7191, Validation accuracy: 0.5783\n",
            "\n",
            "Epoch 86:\n",
            "Training loss: 0.0079, Training accuracy: 1.0000\n",
            "Validation loss: 1.7170, Validation accuracy: 0.5788\n",
            "\n",
            "Epoch 87:\n",
            "Training loss: 0.0074, Training accuracy: 1.0000\n",
            "Validation loss: 1.7255, Validation accuracy: 0.5771\n",
            "\n",
            "Epoch 88:\n",
            "Training loss: 0.0074, Training accuracy: 1.0000\n",
            "Validation loss: 1.7395, Validation accuracy: 0.5795\n",
            "\n",
            "Epoch 89:\n",
            "Training loss: 0.0070, Training accuracy: 1.0000\n",
            "Validation loss: 1.7218, Validation accuracy: 0.5728\n",
            "\n",
            "Epoch 90:\n",
            "Training loss: 0.0074, Training accuracy: 1.0000\n",
            "Validation loss: 1.7336, Validation accuracy: 0.5755\n",
            "\n",
            "Epoch 91:\n",
            "Training loss: 0.0129, Training accuracy: 1.0000\n",
            "Validation loss: 1.7217, Validation accuracy: 0.5745\n",
            "\n",
            "Epoch 92:\n",
            "Training loss: 0.0068, Training accuracy: 1.0000\n",
            "Validation loss: 1.7092, Validation accuracy: 0.5761\n",
            "\n",
            "Epoch 93:\n",
            "Training loss: 0.0050, Training accuracy: 1.0000\n",
            "Validation loss: 1.7115, Validation accuracy: 0.5780\n",
            "\n",
            "Epoch 94:\n",
            "Training loss: 0.0132, Training accuracy: 0.9950\n",
            "Validation loss: 1.7377, Validation accuracy: 0.5681\n",
            "\n",
            "Epoch 95:\n",
            "Training loss: 0.0086, Training accuracy: 1.0000\n",
            "Validation loss: 1.7257, Validation accuracy: 0.5741\n",
            "\n",
            "Epoch 96:\n",
            "Training loss: 0.0086, Training accuracy: 1.0000\n",
            "Validation loss: 1.7183, Validation accuracy: 0.5751\n",
            "\n",
            "Epoch 97:\n",
            "Training loss: 0.0076, Training accuracy: 1.0000\n",
            "Validation loss: 1.7174, Validation accuracy: 0.5765\n",
            "\n",
            "Epoch 98:\n",
            "Training loss: 0.0084, Training accuracy: 1.0000\n",
            "Validation loss: 1.7367, Validation accuracy: 0.5770\n",
            "\n",
            "Epoch 99:\n",
            "Training loss: 0.0102, Training accuracy: 1.0000\n",
            "Validation loss: 1.7154, Validation accuracy: 0.5742\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.5812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "categories = ['20', '100', '400', '1000', '5000']\n",
        "accuracy_values_supervised = [35.52, 52.00, 71.73, 81.51, 91.9]\n",
        "accuracy_values_selfsupervised = [58.12, 72.31, 80.52, 83.8, 89.04]\n",
        "\n",
        "# Plotting\n",
        "plt.plot(categories, accuracy_values_supervised,  marker='o', linestyle='-', color='b', label='supervised')\n",
        "plt.plot(categories, accuracy_values_selfsupervised,  marker='o', linestyle='-', color='y', label='semi supervised')\n",
        "# Adding labels and title\n",
        "plt.xlabel('num_label used')\n",
        "plt.ylabel('Validation Accuracy (%)')\n",
        "plt.ylim(0, 100)  # Set y-axis limit to 0-100%\n",
        "\n",
        "\n",
        "# Display the plot\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "OqbEg3mbV1Cv",
        "outputId": "f3be4686-673b-4f55-9349-814daa6a1c21"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**finetune the whole model**\n",
        "\n",
        "feature map + classifier"
      ],
      "metadata": {
        "id": "5b0FZ1BbgmBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Assuming you have NIN_net_4block and classifier models\n",
        "NIN_net_4block = NetworkInNetwork({'num_classes': 4, 'num_stages': 4, 'use_avg_on_conv3': False})\n",
        "# Load the state_dict using torch.load\n",
        "checkpoint_path = '/content/NIN_net_4block.pth'\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# Load the state_dict into the model\n",
        "NIN_net_4block.load_state_dict(checkpoint)\n",
        "\n",
        "classifier = Classifier({'num_classes': 10, 'nChannels': 192, 'cls_type': 'NIN_ConvBlock3'}).to(device)\n",
        "\n",
        "# Extract the first two blocks from NIN_net_4block\n",
        "nin_blocks = nn.Sequential(\n",
        "    NIN_net_4block._feature_blocks[0],\n",
        "    NIN_net_4block._feature_blocks[1]\n",
        ")\n",
        "\n",
        "# Combine the first two blocks from NIN_net_4block and the classifier\n",
        "combined_model = nn.Sequential(\n",
        "    nin_blocks,\n",
        "    classifier\n",
        ")\n",
        "\n",
        "#hyperparameter setting\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "EPOCHS = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(combined_model.parameters(), lr = INITIAL_LR,\n",
        "                      momentum = MOMENTUM,\n",
        "                      weight_decay=REG)\n"
      ],
      "metadata": {
        "id": "CbK0icIpkAsH"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the folder where the trained model is saved\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "\n",
        "# start the training/validation process\n",
        "# the process should take about 5 minutes on a GTX 1070-Ti\n",
        "# if the code is written efficiently.\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "DECAY = 0.2\n",
        "valid_acc_finetune = []\n",
        "losslist = []\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    if i  == 30 or i == 60 or i == 80:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "\n",
        "    combined_model.train()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    # this help you compute the training accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0 # track training loss if you want\n",
        "\n",
        "    # Train the model for 1 epoch.\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        ####################################\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # inputs = inputs.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "        combined_model = combined_model.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # compute the output and loss\n",
        "        outputs = combined_model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss\n",
        "        # zero the gradient\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation\n",
        "\n",
        "        loss.backward()\n",
        "        # apply gradient and update the weights\n",
        "        optimizer.step()\n",
        "        # count the number of correctly predicted samples in the current batch\n",
        "        _, predicted = outputs.max(1)  #make prediction based on the highest value\n",
        "        total_examples += targets.size(0) # in this case,128 for each batch\n",
        "        correct_examples += predicted.eq(targets).sum().item()\n",
        "        ####################################\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "\n",
        "    combined_model.eval()\n",
        "\n",
        "    #######################\n",
        "\n",
        "    # this help you compute the validation accuracy\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    val_loss = 0 # again, track the validation loss if you want\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            ####################################\n",
        "            # your code here\n",
        "            # copy inputs to device\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            # compute the output and loss\n",
        "\n",
        "            outputs = combined_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # count the number of correctly predicted samples in the current batch\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += predicted.eq(targets).sum().item()\n",
        "            ####################################\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    valid_acc_finetune.append(avg_acc)\n",
        "    losslist.append(avg_loss.item())\n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        #if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "        #    os.makedirs(CHECKPOINT_FOLDER)\n",
        "        #print(\"Saving ...\")\n",
        "\n",
        "        torch.save(classifier.state_dict(), 'classifier_finetune.pth')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng2Fw0MOkSF_",
        "outputId": "be0f5df8-b7ec-4d81-af84-e33d8145b2be"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 1.4899, Training accuracy: 0.4654\n",
            "Validation loss: 1.5715, Validation accuracy: 0.4447\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 1.0259, Training accuracy: 0.6376\n",
            "Validation loss: 1.7177, Validation accuracy: 0.4708\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 0.8615, Training accuracy: 0.6979\n",
            "Validation loss: 0.8769, Validation accuracy: 0.6927\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 0.7692, Training accuracy: 0.7314\n",
            "Validation loss: 1.1552, Validation accuracy: 0.6134\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 0.7226, Training accuracy: 0.7503\n",
            "Validation loss: 0.9085, Validation accuracy: 0.6911\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 0.6826, Training accuracy: 0.7625\n",
            "Validation loss: 0.8907, Validation accuracy: 0.7068\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 0.6506, Training accuracy: 0.7760\n",
            "Validation loss: 1.3859, Validation accuracy: 0.5809\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 0.6362, Training accuracy: 0.7799\n",
            "Validation loss: 1.1080, Validation accuracy: 0.6429\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 0.6168, Training accuracy: 0.7874\n",
            "Validation loss: 0.8460, Validation accuracy: 0.7069\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 0.5963, Training accuracy: 0.7949\n",
            "Validation loss: 0.7140, Validation accuracy: 0.7537\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 0.5788, Training accuracy: 0.8013\n",
            "Validation loss: 0.9360, Validation accuracy: 0.6943\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 0.5684, Training accuracy: 0.8061\n",
            "Validation loss: 0.6672, Validation accuracy: 0.7661\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 0.5645, Training accuracy: 0.8044\n",
            "Validation loss: 0.7231, Validation accuracy: 0.7546\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 0.5453, Training accuracy: 0.8119\n",
            "Validation loss: 0.6930, Validation accuracy: 0.7636\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 0.5373, Training accuracy: 0.8162\n",
            "Validation loss: 1.0838, Validation accuracy: 0.6756\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 0.5301, Training accuracy: 0.8183\n",
            "Validation loss: 0.8454, Validation accuracy: 0.7180\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 0.5237, Training accuracy: 0.8202\n",
            "Validation loss: 0.8128, Validation accuracy: 0.7410\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 0.5185, Training accuracy: 0.8230\n",
            "Validation loss: 0.7176, Validation accuracy: 0.7553\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 0.5138, Training accuracy: 0.8249\n",
            "Validation loss: 0.6481, Validation accuracy: 0.7841\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 0.5011, Training accuracy: 0.8285\n",
            "Validation loss: 0.7980, Validation accuracy: 0.7285\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 0.4978, Training accuracy: 0.8291\n",
            "Validation loss: 0.6552, Validation accuracy: 0.7758\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 0.4987, Training accuracy: 0.8299\n",
            "Validation loss: 0.9760, Validation accuracy: 0.6989\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 0.4901, Training accuracy: 0.8318\n",
            "Validation loss: 0.8957, Validation accuracy: 0.7056\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 0.4897, Training accuracy: 0.8328\n",
            "Validation loss: 0.7897, Validation accuracy: 0.7371\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 0.4844, Training accuracy: 0.8331\n",
            "Validation loss: 0.8045, Validation accuracy: 0.7332\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 0.4734, Training accuracy: 0.8357\n",
            "Validation loss: 0.7179, Validation accuracy: 0.7615\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 0.4759, Training accuracy: 0.8354\n",
            "Validation loss: 0.7688, Validation accuracy: 0.7447\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 0.4707, Training accuracy: 0.8390\n",
            "Validation loss: 0.7054, Validation accuracy: 0.7615\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 0.4711, Training accuracy: 0.8379\n",
            "Validation loss: 0.7016, Validation accuracy: 0.7578\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 0.4652, Training accuracy: 0.8387\n",
            "Validation loss: 0.6459, Validation accuracy: 0.7825\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 30:\n",
            "Training loss: 0.2929, Training accuracy: 0.9005\n",
            "Validation loss: 0.3436, Validation accuracy: 0.8844\n",
            "\n",
            "Epoch 31:\n",
            "Training loss: 0.2451, Training accuracy: 0.9177\n",
            "Validation loss: 0.3384, Validation accuracy: 0.8848\n",
            "\n",
            "Epoch 32:\n",
            "Training loss: 0.2277, Training accuracy: 0.9228\n",
            "Validation loss: 0.3423, Validation accuracy: 0.8811\n",
            "\n",
            "Epoch 33:\n",
            "Training loss: 0.2164, Training accuracy: 0.9269\n",
            "Validation loss: 0.3409, Validation accuracy: 0.8806\n",
            "\n",
            "Epoch 34:\n",
            "Training loss: 0.2105, Training accuracy: 0.9283\n",
            "Validation loss: 0.3679, Validation accuracy: 0.8753\n",
            "\n",
            "Epoch 35:\n",
            "Training loss: 0.2030, Training accuracy: 0.9296\n",
            "Validation loss: 0.3587, Validation accuracy: 0.8784\n",
            "\n",
            "Epoch 36:\n",
            "Training loss: 0.1986, Training accuracy: 0.9314\n",
            "Validation loss: 0.3753, Validation accuracy: 0.8736\n",
            "\n",
            "Epoch 37:\n",
            "Training loss: 0.1995, Training accuracy: 0.9312\n",
            "Validation loss: 0.3865, Validation accuracy: 0.8724\n",
            "\n",
            "Epoch 38:\n",
            "Training loss: 0.1984, Training accuracy: 0.9321\n",
            "Validation loss: 0.4922, Validation accuracy: 0.8387\n",
            "\n",
            "Epoch 39:\n",
            "Training loss: 0.1990, Training accuracy: 0.9306\n",
            "Validation loss: 0.3709, Validation accuracy: 0.8732\n",
            "\n",
            "Epoch 40:\n",
            "Training loss: 0.1949, Training accuracy: 0.9324\n",
            "Validation loss: 0.4215, Validation accuracy: 0.8633\n",
            "\n",
            "Epoch 41:\n",
            "Training loss: 0.1941, Training accuracy: 0.9320\n",
            "Validation loss: 0.3809, Validation accuracy: 0.8765\n",
            "\n",
            "Epoch 42:\n",
            "Training loss: 0.2006, Training accuracy: 0.9309\n",
            "Validation loss: 0.3752, Validation accuracy: 0.8775\n",
            "\n",
            "Epoch 43:\n",
            "Training loss: 0.1926, Training accuracy: 0.9346\n",
            "Validation loss: 0.4165, Validation accuracy: 0.8627\n",
            "\n",
            "Epoch 44:\n",
            "Training loss: 0.1932, Training accuracy: 0.9329\n",
            "Validation loss: 0.4087, Validation accuracy: 0.8613\n",
            "\n",
            "Epoch 45:\n",
            "Training loss: 0.1985, Training accuracy: 0.9309\n",
            "Validation loss: 0.3881, Validation accuracy: 0.8716\n",
            "\n",
            "Epoch 46:\n",
            "Training loss: 0.1888, Training accuracy: 0.9360\n",
            "Validation loss: 0.4412, Validation accuracy: 0.8550\n",
            "\n",
            "Epoch 47:\n",
            "Training loss: 0.1885, Training accuracy: 0.9362\n",
            "Validation loss: 0.4111, Validation accuracy: 0.8644\n",
            "\n",
            "Epoch 48:\n",
            "Training loss: 0.1869, Training accuracy: 0.9362\n",
            "Validation loss: 0.4737, Validation accuracy: 0.8494\n",
            "\n",
            "Epoch 49:\n",
            "Training loss: 0.1932, Training accuracy: 0.9336\n",
            "Validation loss: 0.3861, Validation accuracy: 0.8735\n",
            "\n",
            "Epoch 50:\n",
            "Training loss: 0.1913, Training accuracy: 0.9335\n",
            "Validation loss: 0.4320, Validation accuracy: 0.8589\n",
            "\n",
            "Epoch 51:\n",
            "Training loss: 0.1892, Training accuracy: 0.9347\n",
            "Validation loss: 0.4229, Validation accuracy: 0.8608\n",
            "\n",
            "Epoch 52:\n",
            "Training loss: 0.1815, Training accuracy: 0.9377\n",
            "Validation loss: 0.3967, Validation accuracy: 0.8707\n",
            "\n",
            "Epoch 53:\n",
            "Training loss: 0.1866, Training accuracy: 0.9362\n",
            "Validation loss: 0.3728, Validation accuracy: 0.8755\n",
            "\n",
            "Epoch 54:\n",
            "Training loss: 0.1818, Training accuracy: 0.9377\n",
            "Validation loss: 0.4220, Validation accuracy: 0.8624\n",
            "\n",
            "Epoch 55:\n",
            "Training loss: 0.1819, Training accuracy: 0.9371\n",
            "Validation loss: 0.5442, Validation accuracy: 0.8303\n",
            "\n",
            "Epoch 56:\n",
            "Training loss: 0.1808, Training accuracy: 0.9384\n",
            "Validation loss: 0.4531, Validation accuracy: 0.8581\n",
            "\n",
            "Epoch 57:\n",
            "Training loss: 0.1790, Training accuracy: 0.9387\n",
            "Validation loss: 0.3867, Validation accuracy: 0.8705\n",
            "\n",
            "Epoch 58:\n",
            "Training loss: 0.1806, Training accuracy: 0.9371\n",
            "Validation loss: 0.4321, Validation accuracy: 0.8587\n",
            "\n",
            "Epoch 59:\n",
            "Training loss: 0.1794, Training accuracy: 0.9386\n",
            "Validation loss: 0.4459, Validation accuracy: 0.8523\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 60:\n",
            "Training loss: 0.0947, Training accuracy: 0.9715\n",
            "Validation loss: 0.2643, Validation accuracy: 0.9107\n",
            "\n",
            "Epoch 61:\n",
            "Training loss: 0.0687, Training accuracy: 0.9810\n",
            "Validation loss: 0.2574, Validation accuracy: 0.9169\n",
            "\n",
            "Epoch 62:\n",
            "Training loss: 0.0613, Training accuracy: 0.9837\n",
            "Validation loss: 0.2627, Validation accuracy: 0.9151\n",
            "\n",
            "Epoch 63:\n",
            "Training loss: 0.0528, Training accuracy: 0.9868\n",
            "Validation loss: 0.2608, Validation accuracy: 0.9150\n",
            "\n",
            "Epoch 64:\n",
            "Training loss: 0.0507, Training accuracy: 0.9869\n",
            "Validation loss: 0.2721, Validation accuracy: 0.9129\n",
            "\n",
            "Epoch 65:\n",
            "Training loss: 0.0473, Training accuracy: 0.9887\n",
            "Validation loss: 0.2678, Validation accuracy: 0.9118\n",
            "\n",
            "Epoch 66:\n",
            "Training loss: 0.0443, Training accuracy: 0.9893\n",
            "Validation loss: 0.2595, Validation accuracy: 0.9149\n",
            "\n",
            "Epoch 67:\n",
            "Training loss: 0.0414, Training accuracy: 0.9907\n",
            "Validation loss: 0.2618, Validation accuracy: 0.9163\n",
            "\n",
            "Epoch 68:\n",
            "Training loss: 0.0387, Training accuracy: 0.9914\n",
            "Validation loss: 0.2691, Validation accuracy: 0.9131\n",
            "\n",
            "Epoch 69:\n",
            "Training loss: 0.0368, Training accuracy: 0.9919\n",
            "Validation loss: 0.2640, Validation accuracy: 0.9180\n",
            "\n",
            "Epoch 70:\n",
            "Training loss: 0.0356, Training accuracy: 0.9922\n",
            "Validation loss: 0.2677, Validation accuracy: 0.9159\n",
            "\n",
            "Epoch 71:\n",
            "Training loss: 0.0338, Training accuracy: 0.9933\n",
            "Validation loss: 0.2626, Validation accuracy: 0.9167\n",
            "\n",
            "Epoch 72:\n",
            "Training loss: 0.0335, Training accuracy: 0.9929\n",
            "Validation loss: 0.2712, Validation accuracy: 0.9141\n",
            "\n",
            "Epoch 73:\n",
            "Training loss: 0.0318, Training accuracy: 0.9934\n",
            "Validation loss: 0.2775, Validation accuracy: 0.9133\n",
            "\n",
            "Epoch 74:\n",
            "Training loss: 0.0311, Training accuracy: 0.9932\n",
            "Validation loss: 0.2672, Validation accuracy: 0.9171\n",
            "\n",
            "Epoch 75:\n",
            "Training loss: 0.0299, Training accuracy: 0.9936\n",
            "Validation loss: 0.2767, Validation accuracy: 0.9154\n",
            "\n",
            "Epoch 76:\n",
            "Training loss: 0.0286, Training accuracy: 0.9947\n",
            "Validation loss: 0.2714, Validation accuracy: 0.9184\n",
            "\n",
            "Epoch 77:\n",
            "Training loss: 0.0291, Training accuracy: 0.9943\n",
            "Validation loss: 0.2743, Validation accuracy: 0.9139\n",
            "\n",
            "Epoch 78:\n",
            "Training loss: 0.0280, Training accuracy: 0.9945\n",
            "Validation loss: 0.2804, Validation accuracy: 0.9120\n",
            "\n",
            "Epoch 79:\n",
            "Training loss: 0.0263, Training accuracy: 0.9955\n",
            "Validation loss: 0.2881, Validation accuracy: 0.9122\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 80:\n",
            "Training loss: 0.0220, Training accuracy: 0.9967\n",
            "Validation loss: 0.2722, Validation accuracy: 0.9195\n",
            "\n",
            "Epoch 81:\n",
            "Training loss: 0.0205, Training accuracy: 0.9967\n",
            "Validation loss: 0.2673, Validation accuracy: 0.9192\n",
            "\n",
            "Epoch 82:\n",
            "Training loss: 0.0195, Training accuracy: 0.9973\n",
            "Validation loss: 0.2657, Validation accuracy: 0.9173\n",
            "\n",
            "Epoch 83:\n",
            "Training loss: 0.0193, Training accuracy: 0.9974\n",
            "Validation loss: 0.2645, Validation accuracy: 0.9196\n",
            "\n",
            "Epoch 84:\n",
            "Training loss: 0.0187, Training accuracy: 0.9975\n",
            "Validation loss: 0.2741, Validation accuracy: 0.9141\n",
            "\n",
            "Epoch 85:\n",
            "Training loss: 0.0191, Training accuracy: 0.9972\n",
            "Validation loss: 0.2719, Validation accuracy: 0.9166\n",
            "\n",
            "Epoch 86:\n",
            "Training loss: 0.0179, Training accuracy: 0.9978\n",
            "Validation loss: 0.2633, Validation accuracy: 0.9190\n",
            "\n",
            "Epoch 87:\n",
            "Training loss: 0.0177, Training accuracy: 0.9976\n",
            "Validation loss: 0.2699, Validation accuracy: 0.9179\n",
            "\n",
            "Epoch 88:\n",
            "Training loss: 0.0183, Training accuracy: 0.9978\n",
            "Validation loss: 0.2681, Validation accuracy: 0.9162\n",
            "\n",
            "Epoch 89:\n",
            "Training loss: 0.0170, Training accuracy: 0.9983\n",
            "Validation loss: 0.2687, Validation accuracy: 0.9169\n",
            "\n",
            "Epoch 90:\n",
            "Training loss: 0.0178, Training accuracy: 0.9980\n",
            "Validation loss: 0.2696, Validation accuracy: 0.9184\n",
            "\n",
            "Epoch 91:\n",
            "Training loss: 0.0180, Training accuracy: 0.9978\n",
            "Validation loss: 0.2646, Validation accuracy: 0.9173\n",
            "\n",
            "Epoch 92:\n",
            "Training loss: 0.0177, Training accuracy: 0.9978\n",
            "Validation loss: 0.2700, Validation accuracy: 0.9178\n",
            "\n",
            "Epoch 93:\n",
            "Training loss: 0.0168, Training accuracy: 0.9982\n",
            "Validation loss: 0.2641, Validation accuracy: 0.9201\n",
            "\n",
            "Epoch 94:\n",
            "Training loss: 0.0167, Training accuracy: 0.9982\n",
            "Validation loss: 0.2724, Validation accuracy: 0.9174\n",
            "\n",
            "Epoch 95:\n",
            "Training loss: 0.0167, Training accuracy: 0.9979\n",
            "Validation loss: 0.2623, Validation accuracy: 0.9209\n",
            "\n",
            "Epoch 96:\n",
            "Training loss: 0.0165, Training accuracy: 0.9983\n",
            "Validation loss: 0.2715, Validation accuracy: 0.9178\n",
            "\n",
            "Epoch 97:\n",
            "Training loss: 0.0169, Training accuracy: 0.9977\n",
            "Validation loss: 0.2677, Validation accuracy: 0.9190\n",
            "\n",
            "Epoch 98:\n",
            "Training loss: 0.0166, Training accuracy: 0.9983\n",
            "Validation loss: 0.2644, Validation accuracy: 0.9194\n",
            "\n",
            "Epoch 99:\n",
            "Training loss: 0.0163, Training accuracy: 0.9983\n",
            "Validation loss: 0.2651, Validation accuracy: 0.9191\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.9209\n"
          ]
        }
      ]
    }
  ]
}